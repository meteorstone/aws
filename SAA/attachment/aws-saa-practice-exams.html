
<!DOCTYPE html>
<html>

<head>
	<title>aws-saa-exam</title>
	<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.js"></script>
	<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO"
		crossorigin="anonymous">
</head>

<body>
	<nav class="navbar navbar-dark bg-dark">
		<a href="#" class="navbar-brand">The quizzes</a>
		<p class="text-info mb-0 mr-0">选择题的选项由上至下分别对应a,b,c,d,e...以此类推</p>
	</nav>
	<div class="container-fluid">
		<div class="row">
			<nav class="col-3 navbar navbar-light bg-light" style="align-items: start">
				<div class="sidebar-sticky">
					<a class="navbar-brand" href="#">Chapter</a>
					<nav class="nav nav-pills flex-column" style="font-size: 14px">
						<ul class="list-group list-group-flush">

						</ul>
					</nav>

				</div>
			</nav>
			<div class="col-9" style="background-color: #f3ffc4">
				<div class="container">

				</div>
			</div>

		</div>


	</div>

	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
		crossorigin="anonymous"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
		crossorigin="anonymous"></script>
	<script type="text/javascript">
		var None = null;
		var True = true
		$(document).ready(function () {
	
				var json_data = [{'quizzes': [{'quiz_data': {'next': None, 'count': 65, 'previous': None, 'results': [{'section': 'Auto Scaling', 'prompt': {'answers': ['<p>Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the CPU utilization.</p>', '<p>Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the Memory utilization.</p>', '<p>Configure a Scheduled scaling policy for the Auto Scaling group to launch new instances before the start of the day.</p>', '<p>Set up an Application Load Balancer (ALB) to your architecture to ensure that the traffic is properly distributed on the instances.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A tech company has a CRM application hosted on an Auto Scaling group of On-Demand EC2 instances. The application is extensively used during office hours from 9 in the morning till 5 in the afternoon. Their users are complaining that the performance of the application is slow during the start of the day but then works normally after a couple of hours.\xa0 </p><p>Which of the following can be done to ensure that the application works properly at the beginning of the day?</p>', 'explanation': '<p>Scaling based on a schedule allows you to scale your application in response to predictable load changes. For example, every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling activities based on the predictable traffic patterns of your web application.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/as-sample-web-architecture-diagram-with-asgs.png" alt="                                         An illustration of a basic Auto Scaling group.                                 " /></p> <p>&nbsp;</p> <p>To configure your Auto Scaling group to scale based on a schedule, you create a scheduled action. The scheduled action tells Amazon EC2 Auto Scaling to perform a scaling action at specified times. To create a scheduled scaling action, you specify the start time when the scaling action should take effect, and the new minimum, maximum, and desired sizes for the scaling action. At the specified time, Amazon EC2 Auto Scaling updates the group with the values for minimum, maximum, and desired size specified by the scaling action. You can create scheduled actions for scaling one time only or for scaling on a recurring schedule.</p> <p>Option 3 is the correct answer. You need to configure a Scheduled scaling policy. This will ensure that the instances are already scaled up and ready before the start of the day since this is when the application is used the most.</p> <p>Options 1 and 2 are incorrect because although this is a valid solution, it is still better to configure a Scheduled scaling policy as you already know the exact peak hours of your application. By the time either the CPU or Memory hits a peak, the application already has performance issues, so you need to ensure the scaling is done beforehand using a Scheduled scaling policy.</p> <p>Option 4 is incorrect. Although the Application load balancer can also balance the traffic, it cannot increase the instances based on demand.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566848, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'A tech company has a CRM application hosted on an Auto Scaling group of On-Demand EC2 instances. The application is extensively used during office hours from 9 in the morning till 5 in the afternoon. Their users are complaining that the performance of the application is slow during the start of the day but then works normally after a couple of hours.\xa0 Which of the following can be done to ensure that the application works properly at the beginning of the day?', 'id': 10440552, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'answers': ['<p>Enable the IAM DB Authentication.</p>', '<p>Configure SSL in your application to encrypt the database connection to RDS.</p>', '<p>Create an IAM Role and assign it to your EC2 instances which will grant exclusive access to your RDS instance.</p>', '<p>Use a combination of IAM and STS to restrict access to your RDS instance via a temporary token.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are deploying an Interactive Voice Response (IVR) telephony system in your cloud architecture that interacts with callers, gathers information, and routes calls to the appropriate recipients in your company. The system will be composed of an Auto Scaling group of EC2 instances, an Application Load Balancer, and an RDS instance in a Multi-AZ Deployments configuration. To protect the confidential data of your customers, you have to ensure that your RDS database can only be accessed using the profile credentials specific to your EC2 instances via an authentication token.\xa0 \xa0</p><p>As the Solutions Architect of the company, which of the following should you do to meet the above requirement?</p>', 'explanation': '<p>You can authenticate to your DB&nbsp;instance&nbsp;using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don\'t need to use a password when you connect to a DB instance. Instead, you use an authentication token.</p> <p>An&nbsp;<em>authentication token</em>&nbsp;is a unique string of characters that&nbsp;Amazon RDS&nbsp;generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don\'t need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-13_07-04-06-a2157247b0fa129795001208504fcb51.png" width="750" height="830" /></p> <p>&nbsp;</p> <p>IAM database authentication provides the following benefits:</p> <div> <ol type="disc"> <li> <p>Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL).</p> </li> <li> <p>You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB&nbsp;instance.</p> </li> <li> <p>For applications running on Amazon EC2, you can use profile credentials specific to your EC2 instance to access your database instead of a password, for greater security</p> </li> </ol> <p>Hence, Option 1 is the correct answer based on the above reference.</p> <p>Option 2 is incorrect because an SSL connection is not using an authentication token from IAM. Although configuring SSL to your application can improve the security of your data in flight, it is still not a suitable option to use in this scenario.</p> <p>Option 3 is incorrect because although you can create and assign an IAM Role to your EC2 instances, you still need to configure your RDS to use IAM DB Authentication.</p> <p>Option 4 is incorrect because you have to use IAM DB Authentication for this scenario, and not a combination of an IAM and STS. Although STS is used to send temporary tokens for authentication, this is not a compatible use case for RDS.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS cheat sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p> </div>'}, 'correct_response': ['a'], 'original_assessment_id': 2566850, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are deploying an Interactive Voice Response (IVR) telephony system in your cloud architecture that interacts with callers, gathers information, and routes calls to the appropriate recipients in your company. The system will be composed of an Auto Scaling group of EC2 instances, an Application Load Balancer, and an RDS instance in a Multi-AZ Deployments configuration. To protect the confidential data of your customers, you have to ensure that your RDS database can only be accessed using the profile credentials specific to your EC2 instances via an authentication token.\xa0 \xa0As the Solutions Architect of the company, which of the following should you do to meet the above requirement?', 'id': 10440554, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Billing and Cost Management', 'prompt': {'answers': ['Cost Explorer', 'Cost Allocation Tags', 'AWS Budgets', 'Payment History'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You founded a tech startup that provides online training and software development courses to various students across the globe. Your team has developed an online portal in AWS where the students can log into and access the courses they are subscribed to.\xa0 \xa0</p><p>Since you are in the early phases of the startup and the funding is still hard to come by, which service can help you manage the budgets for all your AWS resources? </p>', 'explanation': '<p>AWS Budgets gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount.</p> <p>Budgets can be tracked at the monthly, quarterly, or yearly level, and you can customize the start and end dates. You can further refine your budget to track costs associated with multiple dimensions, such as AWS service, linked account, tag, and others. Budget alerts can be sent via email and/or Amazon Simple Notification Service (SNS) topic.</p> <p>You can also use AWS Budgets to set a custom reservation utilization target and receive alerts when your utilization drops below the threshold you define. RI utilization alerts support Amazon EC2, Amazon RDS, Amazon Redshift, and Amazon ElastiCache reservations.</p> <p>Budgets can be created and tracked from the AWS Budgets dashboard or via the Budgets API.</p> <p>Option 1 is incorrect because the Cost Explorer only helps you visualize and manage your AWS costs and usages over time. It offers a set of reports you can view data with for up to the last 13 months, forecast how much you\'re likely to spend for the next three months, and get recommendations for what Reserved Instances to purchase. You use Cost Explorer to identify areas that need further inquiry and see trends to understand your costs.</p> <p>Option 2 is incorrect because Cost Allocation Tags only eases the organization of your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs.</p> <p>Option 4 is incorrect because the payment history option only provides a location where you can view the monthly invoices you receive from AWS. If your account isn\'t past due, the Payment History page shows only previous invoices and payment status.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/aws-cost-management/aws-budgets/">https://aws.amazon.com/aws-cost-management/aws-budgets/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-billing-and-cost-management/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-billing-and-cost-management/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566852, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You founded a tech startup that provides online training and software development courses to various students across the globe. Your team has developed an online portal in AWS where the students can log into and access the courses they are subscribed to.\xa0 \xa0Since you are in the early phases of the startup and the funding is still hard to come by, which service can help you manage the budgets for all your AWS resources?', 'id': 10440556, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['Verify if your private key (.pem) file has been correctly converted to the format recognized by PuTTY (.ppk).', 'Verify that your IAM user policy has permission to launch Amazon EC2 instances.', 'Verify that you are connecting with the appropriate user name for your AMI such as <code>ec2-user</code> for Linux AMI, <code>centos</code> for Centos AMI or <code>admin</code> for Debian AMI', 'Verify that the Amazon EC2 Instance was launched with the proper IAM role.', '<p>Verify that you have waited at least 1 hour after the EC2 instance was created before connecting via SSH.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are trying to establish an SSH connection to a newly created Amazon EC2 instance using the PuTTY tool. However, you are getting the following error message: <br><br><code>Error: No supported authentication methods available</code><br><br>What steps should you take to fix this issue? (Choose 2)</p>', 'explanation': '<p>If you use PuTTY to connect to your instance via SSH and get either of the following errors,&nbsp;<code>Error: Server refused our key</code>&nbsp;or&nbsp;<code>Error: No supported authentication methods available</code>, verify that you are connecting with the appropriate user name for your AMI. Enter the user name in the&nbsp;<strong>User name</strong>&nbsp;box in the&nbsp;<strong>PuTTY Configuration</strong>&nbsp;window.</p> <p>The appropriate user names are as follows:</p> <div> <ul> <li>-For an Amazon Linux AMI, the user name is&nbsp;<code>ec2-user</code>.</li> <li>-For a RHEL AMI, the user name is&nbsp;<code>ec2-user</code>&nbsp;or&nbsp;<code>root</code>.</li> <li>-For an Ubuntu AMI, the user name is&nbsp;<code>ubuntu</code>&nbsp;or&nbsp;<code>root</code>.</li> <li>-For a Centos AMI, the user name is&nbsp;<code>centos</code>.</li> <li>-For a Debian AMI, the user name is&nbsp;<code>admin</code>&nbsp;or&nbsp;<code>root</code>.</li> <li>-For a Fedora AMI, the user name is&nbsp;<code>ec2-user</code>.</li> <li>-For a SUSE AMI, the user name is&nbsp;<code>ec2-user</code>&nbsp;or&nbsp;<code>root</code>.</li> <li>-Otherwise, if&nbsp;<code>ec2-user</code>&nbsp;and&nbsp;<code>root</code>&nbsp;don\'t work, check with the AMI provider.</li> </ul> </div> <p>&nbsp;</p> <p>You should also verify that your private key (.pem) file has been correctly converted to the format recognized by PuTTY (.ppk).&nbsp;</p> <p>Options 2 and 4 are incorrect because both an IAM user and&nbsp;IAM role policy have nothing to do with this issue.&nbsp;</p> <p>Option 5 is incorrect because you don\'t need to wait an hour in order to connect to a new EC2 instance as you can immediately connect to it once it is created.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectingPuTTY">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectingPuTTY</a></p> <p><code></code></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2566854, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are trying to establish an SSH connection to a newly created Amazon EC2 instance using the PuTTY tool. However, you are getting the following error message: Error: No supported authentication methods availableWhat steps should you take to fix this issue? (Choose 2)', 'id': 10440558, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'CloudWatch', 'prompt': {'answers': ['One second', 'Five seconds', 'One minute', '<p>Five minutes</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A financial application that calculates accruals, interests, and other data is hosted on a fleet of Spot EC2 instances that are configured with Auto Scaling. The application is used by an external reporting application that provides the total calculation for each user account and transaction. You used CloudWatch to automatically monitor the EC2 instance without manually checking the server for high CPU Utilization or crashes.\xa0 \xa0</p><p>What is the time period of data that Amazon CloudWatch receives and aggregates from EC2 by default?\xa0 </p>', 'explanation': '<p>By default, your instance is enabled for basic monitoring. You can optionally enable detailed monitoring. After you enable detailed monitoring, the Amazon EC2 console displays monitoring graphs with a 1-minute period for the instance. The following table describes basic and detailed monitoring for instances.</p> <ol> <li><strong>Basic</strong> -&nbsp;Data is available automatically in 5-minute periods at no charge.</li> <li><strong>Detailed</strong> -&nbsp;Data is available in 1-minute periods for an additional cost. To get this level of data, you must specifically enable it for the instance. For the instances where you\'ve enabled detailed monitoring, you can also get aggregated data across groups of similar instances.</li> </ol> <p><img src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-46-37-faa6d85a690dafca59594a939866f0e3.png" /></p> <p>&nbsp;</p> <p>Options 1 and 2 are incorrect because although you can publish Custom Metrics down to 1-second or 2-second resolution to give you more immediate visibility and greater granularity into the state and performance of your custom applications, these are not the default values in CloudWatch.</p> <p>Option 3 is incorrect because the 1-minute data period is only available for detailed monitoring and it is not enabled by default.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/cloudwatch/faqs/">https://aws.amazon.com/cloudwatch/faqs/</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2566856, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'A financial application that calculates accruals, interests, and other data is hosted on a fleet of Spot EC2 instances that are configured with Auto Scaling. The application is used by an external reporting application that provides the total calculation for each user account and transaction. You used CloudWatch to automatically monitor the EC2 instance without manually checking the server for high CPU Utilization or crashes.\xa0 \xa0What is the time period of data that Amazon CloudWatch receives and aggregates from EC2 by default?', 'id': 10440560, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': '', 'prompt': {'answers': ['There is no additional charge for AWS CloudFormation. You only pay for the AWS resources that are created.', 'The cost is based on the file size of the template.', 'It is charged per hour.', 'The cost is based on the size of the template.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You are a Solutions Architect of a bank, designing various CloudFormation templates for a new online trading platform that your department will build.<br><br>How much does it cost to use CloudFormation templates?', 'explanation': '<p>There is no additional charge for AWS CloudFormation. You only pay for the AWS resources that are created (e.g. Amazon EC2 instances, Elastic Load Balancing load balancers, etc.)</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/cloudformation/faqs/" target="_blank" rel="noopener">https://aws.amazon.com/cloudformation/faqs/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudFormation Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566858, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are a Solutions Architect of a bank, designing various CloudFormation templates for a new online trading platform that your department will build.How much does it cost to use CloudFormation templates?', 'id': 10440562, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB', 'prompt': {'answers': ['<p>An Amazon RDS instance in Multi-AZ Deployments configuration</p>', '<p>Amazon DynamoDB</p>', '<p>An Amazon Aurora database with Read Replicas</p>', '<p>Redshift</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a technology company which is in the process of migrating their applications to AWS. One of their systems requires a database that can scale globally and can handle frequent schema changes. It should also provide low-latency response to high-traffic queries. </p><p>Which is the most suitable database solution to use to achieve this requirement?</p>', 'explanation': '<p>Before we proceed in answering this question, we must first be clear with the actual definition of a "<strong>schema</strong>". Basically, the english definition of a schema is:&nbsp;<em>a representation of a plan or theory in the form of an outline or model</em>.</p> <p>Just think of a schema as the "structure" or a "model" of your data in your database. Since the scenario requires that the schema, or the structure of your data, changes frequently, then you have to pick a database which provides a non-rigid and flexible way of adding or removing new types of data. This is a classic example of choosing between a relational database and non-relational (NoSQL) database.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-03-08_21-57-30-7860e883ac634eb2e771a5f4694cbafa.png" width="700" height="337" /></p> <p>&nbsp;</p> <p>A relational database is known for having a rigid schema, with a lot of constraints and limits as to which (and what type of ) data can be inserted or not. It is primarily used for scenarios where you have to support complex queries which fetch data across a number of tables. It is best for scenarios where you have complex table relationships but for use cases where you need to have a flexible schema, this is not a suitable database to use.</p> <p>For NoSQL, it is not as rigid as a relational database because you can easily add or remove rows or elements in your table/collection entry. It also has a more flexible schema because&nbsp;it can store complex hierarchical data within a single item which, unlike a relational database, does not entail changing multiple related tables. Hence, the best answer to be used here is a NoSQL&nbsp;database, like DynamoDB. When your business requires a low-latency response to high-traffic queries, taking advantage of a NoSQL system generally makes technical and economic sense.</p> <p>Amazon DynamoDB helps solve the problems that limit the relational system scalability by avoiding them.&nbsp;In DynamoDB, you design your schema specifically to make the most common and important queries as fast and as inexpensive as possible. Your data structures are tailored to the specific requirements of your business use cases.</p> <p>Remember that a relational database system&nbsp;<strong>does not scale</strong>&nbsp;well for the following reasons:</p> <ul> <li>-It normalizes data and stores it on multiple tables that require multiple queries to write to disk.</li> <li>-It generally incurs the performance costs of an ACID-compliant transaction system.</li> <li>-It uses expensive joins to reassemble required views of query results.</li> </ul> <p>&nbsp;</p> <p>For DynamoDB, it scales well due to these reasons:</p> <ul> <li>-Its<strong> schema flexibility</strong>&nbsp;lets DynamoDB store complex hierarchical data within a single item. DynamoDB is not a totally <em>schemaless</em> database since the very definition of a schema is just the model or structure of your data.</li> <li>-Composite key design lets it store related items close together on the same table.</li> </ul> <p>&nbsp;</p> <p>Options 1 and 3 are incorrect because both of them are a type of relational database.</p> <p>Option 4 is incorrect because Redshift is primarily used for OLAP systems.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-general-nosql-design.html</a></p> <p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-relational-modeling.html" target="_blank" rel="noopener noreferrer">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-relational-modeling.html</a></p> <p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html</a></p> <p>&nbsp;</p> <p>Also check the <em><strong>AWS Certified Solutions Architect Official Study Guide: Associate Exam</strong></em> 1st Edition and turn to page 161 which talks about NoSQL Databases.</p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon DynamoDB Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb</a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566862, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are working as a Solutions Architect for a technology company which is in the process of migrating their applications to AWS. One of their systems requires a database that can scale globally and can handle frequent schema changes. It should also provide low-latency response to high-traffic queries. Which is the most suitable database solution to use to achieve this requirement?', 'id': 10440564, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Security', 'prompt': {'answers': ['Immediately to all 12 instances in the security group.', 'Immediately to the new instances only.', 'Immediately to the new instances, but not for the old ones which must be restarted before the changes take effect.', 'The changes will apply to all 12 instances after an hour when the propagation is complete.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You have one security group associated with 10 On-Demand EC2 instances. You then modified the security group to allow all inbound SSH traffic and then right after that, you created two new EC2 instances in the same security group.\xa0 \xa0</p><p>When will the changes be applied to the EC2 instances? </p>', 'explanation': '<p>A&nbsp;<em>security group</em>&nbsp;acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC could be assigned to a different set of security groups. If you don\'t specify a particular group at launch time, the instance is automatically assigned to the default security group for the VPC.&nbsp;</p> <p>Option 1 is correct. When you add or remove rules, those changes are automatically applied to all instances to which you\'ve assigned the security group. Since the first 10 instances are already assigned to the security group, you can SSH into them immediately after the change. After adding the two new instances to the security group, you should be able to SSH into them as well since the change was made beforehand.</p> <p>Options 2 and 3 are incorrect because the changes will be applied to all EC2 instances and not just to the new or old set of instances.</p> <p>Option 4 is incorrect because you don\'t have to wait for an hour for the changes to be applied to your security group since the changes will be immediately&nbsp;reflected.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html</a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566864, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You have one security group associated with 10 On-Demand EC2 instances. You then modified the security group to allow all inbound SSH traffic and then right after that, you created two new EC2 instances in the same security group.\xa0 \xa0When will the changes be applied to the EC2 instances?', 'id': 10440566, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudWatch', 'prompt': {'answers': ['Memory Utilization of an EC2 instance', 'CPU Utilization of an EC2 instance', 'Disk Reads activity of an EC2 instance', '<p>Network packets out of an EC2 instance</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>The company that you are working for has a highly available architecture consisting of an elastic load balancer and several EC2 instances configured with auto-scaling in three Availability Zones. You want to monitor your EC2 instances based on a particular metric, which is not readily available in CloudWatch.\xa0 \xa0</p><p>Which of the following is a custom metric in CloudWatch which you have to manually set up?</p>', 'explanation': '<p>CloudWatch has available Amazon EC2 Metrics for you to use for monitoring. CPU Utilization identifies the processing power required to run an application upon a selected instance. Network Utilization identifies the volume of incoming and outgoing network traffic to a single instance. Disk Reads metric is used to determine the volume of the data the application reads from the hard disk of the instance. This can be used to determine the speed of the application. However, there are certain metrics that are not readily available in CloudWatch such as memory utilization, disk space utilization, and many others which can be collected by setting up a custom metric.</p> <p>You need to prepare a custom metric using CloudWatch Monitoring Scripts which is written in Perl.&nbsp;You can also install CloudWatch Agent to collect more system-level metrics from Amazon EC2 instances. Here\'s the list of custom metrics that you can set up:</p> <p style="padding-left: 30px;">- Memory utilization<br /> - Disk swap utilization<br /> - Disk space utilization<br /> - Page file utilization<br /> - Log collection</p> <p style="padding-left: 30px;">&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-48-03-503799cd694a657201456b3add758b53.png" /></p> <p>&nbsp;</p> <p>Options 2, 3, and 4 are incorrect because these metrics are readily available in CloudWatch by default.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/products/databases/">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html#using_put_script">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html#using_put_script</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></strong></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566866, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'The company that you are working for has a highly available architecture consisting of an elastic load balancer and several EC2 instances configured with auto-scaling in three Availability Zones. You want to monitor your EC2 instances based on a particular metric, which is not readily available in CloudWatch.\xa0 \xa0Which of the following is a custom metric in CloudWatch which you have to manually set up?', 'id': 10440568, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Lambda', 'prompt': {'answers': ['<p>There is no need to do anything because, by default, AWS Lambda already encrypts the environment variables using the AWS Key Management Service.</p>', '<p>Enable SSL encryption that leverages on AWS CloudHSM to store and encrypt the sensitive information.</p>', '<p>AWS Lambda does not provide encryption for the environment variables. Deploy your code to an EC2 instance instead.</p>', '<p>Create a new KMS key and use it to enable encryption helpers that leverage on AWS Key Management Service to store and encrypt the sensitive information.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are leading a software development team which uses serverless computing with AWS Lambda to build and run applications without having to set up or manage servers. You have a Lambda function that connects to a MongoDB Atlas, which is a popular Database as a Service (DBaaS) platform and also uses a third party API to fetch certain data for your application. You instructed one of your junior developers to create the environment variables for the MongoDB database hostname, username, and password as well as the API credentials that will be used by the Lambda function for DEV, SIT, UAT and PROD environments.\xa0 </p><p>Considering that the Lambda function is storing sensitive database and API credentials, how can you secure these information to prevent other developers in your team, or anyone, from seeing these credentials in plain text? Select the best option that provides the maximum security.</p>', 'explanation': '<p>When you create or update Lambda functions that use environment variables, AWS Lambda encrypts them using the AWS Key Management Service. When your Lambda function is invoked, those values are decrypted and made available to the Lambda code.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/lambda/latest/dg/images/env_variables1.png" alt="" width="750" height="491" /></p> <p>&nbsp;</p> <p>The first time you create or update Lambda functions that use environment variables in a region, a default service key is created for you automatically within AWS KMS. This key is used to encrypt environment variables. However, if you wish to use encryption helpers and use KMS to encrypt environment variables after your Lambda function is created, you must create your own AWS KMS key and choose it instead of the default key. The default key will give errors when chosen. Creating your own key gives you more flexibility, including the ability to create, rotate, disable, and define access controls, and to audit the encryption keys used to protect your data.</p> <p>Option 1 is incorrect since Lambda does not encrypt environment variables by default during the deployment process. When you deploy your Lambda function, all the environment variables you\'ve specified are encrypted by default after, but not during, the deployment process.</p> <p>Option 2 is also incorrect since enabling SSL would encrypt data only when in-transit. Your other teams would still be able to view the plaintext at-rest. Use AWS KMS instead.</p> <p>Option 3 is incorrect since, as mentioned, Lambda does provide encryption functionality of environment variables.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/lambda/latest/dg/env_variables.html#env_encrypt">https://docs.aws.amazon.com/lambda/latest/dg/env_variables.html#env_encrypt</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Lambda Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2566868, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are leading a software development team which uses serverless computing with AWS Lambda to build and run applications without having to set up or manage servers. You have a Lambda function that connects to a MongoDB Atlas, which is a popular Database as a Service (DBaaS) platform and also uses a third party API to fetch certain data for your application. You instructed one of your junior developers to create the environment variables for the MongoDB database hostname, username, and password as well as the API credentials that will be used by the Lambda function for DEV, SIT, UAT and PROD environments.\xa0 Considering that the Lambda function is storing sensitive database and API credentials, how can you secure these information to prevent other developers in your team, or anyone, from seeing these credentials in plain text? Select the best option that provides the maximum security.', 'id': 10440570, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'answers': ['<p>172.0.0.0/27</p>', '<p>172.0.0.0/28</p>', '<p>172.0.0.0/29</p>', '<p>172.0.0.0/30</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>In your AWS VPC, you need to add a new subnet that will allow you to host a total of 20 EC2 instances. <br><br>Which of the following IPv4 CIDR block can you use for this scenario?</p>', 'explanation': '<p>To calculate the total number of IP addresses of a given CIDR Block, you simply need to follow the 2 easy steps below. Let\'s say you have a CIDR block <strong>/27</strong>:&nbsp;</p> <p style="padding-left: 30px;">1. Subtract <strong>32</strong> with the mask number :&nbsp;</p> <p style="padding-left: 60px;">(32 - 27) = <strong>5</strong></p> <p style="padding-left: 30px;">2. Raise&nbsp;the number&nbsp;<strong>2</strong> to the power of the answer in Step #1 :&nbsp;</p> <p style="padding-left: 60px;">2^ <strong>5</strong> = (2 * 2 * 2 * 2 * 2)</p> <p style="padding-left: 90px;">&nbsp; = <strong>32</strong></p> <p>The answer to Step #2 is the total number of IP addresses available in the given CIDR netmask. Don\'t forget that in AWS, the first 4 IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance.&nbsp;In addition, you can always associate a netmask of /27 which also has the same number of usable IP addresses (27) to help you with your exam.</p> <p>Option 1 is the correct answer because the CIDR block of 172.0.0.0/27, with a netmask of /27, has an equivalent of 27 <em>usable</em> IP addresses. Take note that a netmask of /27 originally provides you with 32 IP addresses but in AWS, there are 5 IP addresses that are reserved which you cannot use. The first 4 IP addresses and the last IP address in each subnet CIDR block are not available in your VPC which means that you have to <strong>always</strong> subtract 5 IP addresses, hence 32 - 5 = 27.&nbsp;</p> <p>Option 2 is incorrect as&nbsp;a netmask of /28 only supports 16 IP Addresses.</p> <p>Options 3 and 4 are incorrect as the only allowed block size is between&nbsp;a /28 netmask and /16 netmask.&nbsp;</p> <p>To add a CIDR block to your VPC, the following rules apply:</p> <div> <ul> <li> <p>-The allowed block size is between a&nbsp;<code>/28</code>&nbsp;netmask and&nbsp;<code>/16</code>&nbsp;netmask.</p> </li> <li> <p>-The CIDR block must not overlap with any existing CIDR block that\'s associated with the VPC.</p> </li> <li> <p>-You cannot increase or decrease the size of an existing CIDR block.</p> </li> <li> <p>-You have a limit on the number of CIDR blocks you can associate with a VPC and the number of routes you can add to a route table. You cannot associate a CIDR block if this results in you exceeding your limits.</p> </li> <li> <p>-The CIDR block must not be the same or larger than the CIDR range of a route in any of the VPC route tables. For example, if you have a route with a destination of&nbsp;<code>10.0.0.0/24</code>&nbsp;to a virtual private gateway, you cannot associate a CIDR block of the same range or larger. However, you can associate a CIDR block of&nbsp;<code>10.0.0.0/25</code>&nbsp;or smaller.</p> </li> <li>-The first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance.</li> </ul> </div> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566870, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'In your AWS VPC, you need to add a new subnet that will allow you to host a total of 20 EC2 instances. Which of the following IPv4 CIDR block can you use for this scenario?', 'id': 10440572, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Kinesis', 'prompt': {'answers': ['<p>Amazon S3 Select</p>', '<p>Amazon Redshift Spectrum</p>', 'Amazon Elasticsearch Service', '<p>Amazon Athena</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A traffic monitoring and reporting application uses Kinesis to accept real-time data. In order to process and store the data, they used Amazon Kinesis Data Firehose to load the streaming data to various AWS resources.\xa0 \xa0</p><p>Which of the following services can you load streaming data into?</p>', 'explanation': '<div> <p>Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you&rsquo;re already using today.</p> <p>It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-es.png" alt="" width="554" height="284" /></p> <p>&nbsp;</p> <p>&nbsp;</p> <p>Options 1 and 2 are incorrect because Amazon S3&nbsp;<strong><em>Select</em></strong>&nbsp;is just a feature of Amazon S3. Likewise, Redshift&nbsp;<strong><em>Spectrum</em></strong>&nbsp;is also just a feature of Amazon Redshift. Although Amazon Kinesis Data Firehose can load streaming data to both Amazon S3 and Amazon Redshift, it does not directly load the data to S3 Select and Redshift Spectrum.&nbsp;</p> <p>S3 Select is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object. Amazon Redshift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3 with no loading or ETL required.</p> <p>Option 4 is incorrect because Amazon Kinesis Data Firehose cannot load streaming data to Athena.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> </div> <p><a href="https://aws.amazon.com/kinesis/data-firehose/">https://aws.amazon.com/kinesis/data-firehose/</a></p> <p>&nbsp;</p> <p><strong>Check out these Amazon Kinesis and Elasticsearch Cheat Sheets:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</a></span></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticsearch-amazon-es/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticsearch-amazon-es/</a></span></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566872, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'A traffic monitoring and reporting application uses Kinesis to accept real-time data. In order to process and store the data, they used Amazon Kinesis Data Firehose to load the streaming data to various AWS resources.\xa0 \xa0Which of the following services can you load streaming data into?', 'id': 10440574, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Networking', 'prompt': {'answers': ['<p>Set up a list of Elastic IP addresses to map the whitelisted IP address range in your on-premises network.</p>', '<p>Set up an IP match condition using a CloudFront web distribution and AWS WAF to whitelist a specific IP address range in your VPC.</p>', '<p>Create a Route Origin Authorization (ROA) then once done, provision and advertise your whitelisted IP address range to your AWS account.</p>', '<p>Submit an AWS Request Form to migrate the IP address range that you own to your AWS Account.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are managing a suite of applications in your on-premises network which are using trusted IP addresses that your partners and customers have whitelisted in their firewalls. There is a requirement to migrate these applications to AWS without requiring your partners and customers to change their IP address whitelists.\xa0 \xa0</p><p>Which of the following is the most suitable solution to properly migrate your applications?</p>', 'explanation': '<p>You can bring part or all of your public IPv4 address range from your on-premises network to your AWS account. You continue to own the address range, but AWS advertises it on the Internet. After you bring the address range to AWS, it appears in your account as an address pool. You can create an Elastic IP address from your address pool and use it with your AWS resources, such as EC2 instances, NAT gateways, and Network Load Balancers. This is also called "Bring Your Own IP Addresses (BYOIP)".</p> <p>To ensure that only you can bring your address range to your AWS account, you must authorize Amazon to advertise the address range and provide proof that you own the address range.</p> <p>A <strong>Route Origin Authorization (ROA)</strong> is a document that you can create through your Regional internet registry (RIR), such as the American Registry for Internet Numbers (ARIN) or R&eacute;seaux IP Europ&eacute;ens Network Coordination Centre (RIPE). It contains the address range, the ASNs that are allowed to advertise the address range, and an expiration date. Hence, Option 3 is the correct answer.</p> <p>The ROA authorizes Amazon to advertise an address range under a specific AS number. However, it does not authorize your AWS account to bring the address range to AWS. To authorize your AWS account to bring an address range to AWS, you must publish a self-signed X509 certificate in the RDAP remarks for the address range. The certificate contains a public key, which AWS uses to verify the authorization-context signature that you provide. You should keep your private key secure and use it to sign the authorization-context message.</p> <p>Option 1 is incorrect because&nbsp;you cannot map the IP address of your on-premises network, which you are migrating to AWS,&nbsp;to an EIP address of your VPC. To satisfy the requirement, you must authorize Amazon to advertise the address range that you own.</p> <p>Option 2 is incorrect because the IP match condition in CloudFront is primarily used in allowing or blocking the incoming web requests based on the IP addresses that the requests originate from. This is the opposite of what is being asked in the scenario, where you have to migrate your&nbsp;suite of applications from your on-premises network and advertise the address range that you own in your VPC.</p> <p>Option 4 is incorrect because you don\'t need to submit an AWS request in order to do this.&nbsp;You can simply create a Route Origin Authorization (ROA) then once done, provision and advertise your whitelisted IP address range to your AWS account.</p> <div> <div> <div><strong>Reference:</strong></div> </div> </div> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-byoip.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-byoip.html</a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566874, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are managing a suite of applications in your on-premises network which are using trusted IP addresses that your partners and customers have whitelisted in their firewalls. There is a requirement to migrate these applications to AWS without requiring your partners and customers to change their IP address whitelists.\xa0 \xa0Which of the following is the most suitable solution to properly migrate your applications?', 'id': 10440576, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['Enable Versioning', '<p>Provide access to S3 data strictly through pre-signed URL only</p>', 'Disallow S3 Delete using an IAM bucket policy', '<p>Enable Amazon S3 Intelligent-Tiering</p>', 'Enable Multi-Factor Authentication Delete'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>There was an incident in your production environment where the user data stored in the S3 bucket has been accidentally deleted by one of the Junior DevOps Engineers. The issue was escalated to your manager and after a few days, you were instructed to improve the security and protection of your AWS resources.\xa0 \xa0 </p><p>What combination of the following options will protect the S3 objects in your bucket from both accidental deletion and overwriting? (Choose 2)</p>', 'explanation': '<p>By using Versioning and enabling MFA (Multi-Factor Authentication) Delete, you can secure and recover your S3 objects from accidental deletion or overwrite.&nbsp;</p> <p>Versioning is a means of keeping multiple variants of an object in the same bucket. Versioning-enabled buckets enable you to recover objects from accidental deletion or overwrite. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.</p> <p>You can also optionally add another layer of security by configuring a bucket to enable MFA (Multi-Factor Authentication) Delete, which requires additional authentication for either of the following operations:</p> <p style="padding-left: 30px;"> - Change the versioning state of your bucket</p> <p style="padding-left: 30px;"> - Permanently delete an object version</p> <p>&nbsp;</p> <p>MFA Delete requires two forms of authentication together:</p> <p style="padding-left: 30px;"> - Your security credentials</p> <p style="padding-left: 30px;"> - The concatenation of a valid serial number, a space, and the six-digit code displayed on an approved authentication device</p> <p>&nbsp;</p> <p>Option 2 is incorrect since a pre-signed URL gives access to the object identified in the URL. Pre-signed URLs are useful when customers perform an object upload to your S3&nbsp;bucket, but does not help in preventing accidental deletes.</p> <p>Option 3 is incorrect since you still want users to be able to delete objects in the bucket, and you just want to prevent accidental deletions.&nbsp;Disallowing S3 Delete using an IAM bucket policy will restrict all delete operations to your bucket.</p> <p>Option 4 is incorrect since S3 intelligent tiering does not help in this situation.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['a', 'e'], 'original_assessment_id': 2566876, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'There was an incident in your production environment where the user data stored in the S3 bucket has been accidentally deleted by one of the Junior DevOps Engineers. The issue was escalated to your manager and after a few days, you were instructed to improve the security and protection of your AWS resources.\xa0 \xa0 What combination of the following options will protect the S3 objects in your bucket from both accidental deletion and overwriting? (Choose 2)', 'id': 10440578, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'AMI', 'prompt': {'answers': ['In the AMI dashboard, add the us-west-2 region to the Network Access Control List which contains the regions  that are allowed to use the AMI.', '<p>Copy the AMI from the eu-central-1 region to the us-west-2 region. Afterwards, create a new Auto Scaling group in the us-west-2 region to use this new AMI ID.</p>', 'Share the AMI to the us-west-2 region.', 'None. AMIs can be used in any region hence, there is no problem using it in the us-west-2 region.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You have a web application deployed in AWS which is currently running in the eu-central-1 region. You have an Auto Scaling group of On-Demand EC2 instances which are using pre-built AMIs. Your manager instructed you to implement disaster recovery for your system so in the event that the application goes down in the eu-central-1 region, a new instance can be started in the us-west-2 region.\xa0 </p><p>As part of your disaster recovery plan, which of the following should you take into consideration?</p>', 'explanation': '<p>In this scenario, the EC2 instances you are currently using depends on a pre-built AMI. This AMI is not accessible to another region hence,&nbsp; you have to copy it to the&nbsp;us-west-2 region to properly establish your disaster recovery instance.</p> <p>You can copy an Amazon Machine Image (AMI) within or across an AWS region using the AWS Management Console, the AWS command line tools or SDKs, or the Amazon EC2 API, all of which support the&nbsp;<code class="code">CopyImage </code>action. You can copy both Amazon EBS-backed AMIs and instance store-backed AMIs. You can copy encrypted AMIs and AMIs with encrypted snapshots.</p> <p>Options 1 and 3 are incorrect because the AMI&nbsp;does not have a Network Access Control nor a Share functionality.</p> <p>Option 4 is incorrect as you can use a unique or pre-built AMI to a specific region only.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a></p> <p>&nbsp;</p> <p><strong>Here is a quick tutorial on how to create an AMI from EBS-backed EC2 instance:</strong></p> <p><iframe src="https://www.youtube.com/embed/vSKWBBrEbNQ" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566878, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You have a web application deployed in AWS which is currently running in the eu-central-1 region. You have an Auto Scaling group of On-Demand EC2 instances which are using pre-built AMIs. Your manager instructed you to implement disaster recovery for your system so in the event that the application goes down in the eu-central-1 region, a new instance can be started in the us-west-2 region.\xa0 As part of your disaster recovery plan, which of the following should you take into consideration?', 'id': 10440580, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB', 'prompt': {'answers': ['<p>Amazon ElastiCache</p>', '<p>AWS Device Farm</p>', '<p>DynamoDB Auto Scaling</p>', '<p>Amazon DynamoDB Accelerator (DAX)</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': "<p>A popular mobile game uses CloudFront, Lambda, and DynamoDB for its backend services. The player data is persisted on a DynamoDB table and the static assets are distributed by CloudFront. However, there are a lot of complaints that saving and retrieving player information is taking a lot of time.\xa0 \xa0</p><p>To improve the game's performance, which AWS service can you use to reduce DynamoDB response times from milliseconds to microseconds?</p>", 'explanation': '<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache that can reduce Amazon DynamoDB response times from milliseconds to microseconds, even at millions of requests per second.</p> <p><img src="https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2017/06/28/ReadThroughCache-1.png" alt="" width="650" height="108" /></p> <p>Option 1 is incorrect because although you may use ElastiCache as your database cache, it will not reduce the DynamoDB response time from milliseconds to microseconds as compared with DynamoDB DAX.</p> <p>Option 2 is incorrect because AWS Device Farm is an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real time.</p> <p>Option 3 is incorrect because DynamoDB Auto Scaling is primarily used&nbsp;to automate capacity management for your tables and global secondary indexes.</p> <div>&nbsp;</div> <div><strong>References:</strong></div> <p><a href="https://aws.amazon.com/dynamodb/dax">https://aws.amazon.com/dynamodb/dax</a></p> <p><a href="https://aws.amazon.com/device-farm">https://aws.amazon.com/device-farm</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2566880, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': "A popular mobile game uses CloudFront, Lambda, and DynamoDB for its backend services. The player data is persisted on a DynamoDB table and the static assets are distributed by CloudFront. However, there are a lot of complaints that saving and retrieving player information is taking a lot of time.\xa0 \xa0To improve the game's performance, which AWS service can you use to reduce DynamoDB response times from milliseconds to microseconds?", 'id': 10440582, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['<p>Set up a standard RAID 0 configuration with two EBS Volumes</p>', '<p>Re-launch the instance with a Paravirtual (PV) AMI and enable Enhanced Networking</p>', '<p>Use a standard RAID 1 configuration with two EBS Volumes</p>', '<p>Set up the EC2 instance in a placement group</p>', 'Increase the size of the EC2 Instance'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>You have launched a new enterprise application with a web server and a database. You are using a large EC2 Instance with one 500 GB EBS volume to host a relational database. Upon checking the performance, it shows that write throughput to the database needs to be improved.\xa0 \xa0</p><p>Which of the following is the most suitable configuration to help you achieve this requirement? (Choose 2)</p>', 'explanation': '<p>The goal here is to increase the write performance of the database hosted in an EC2 instance. You can achieve this by either setting up a standard RAID 0 configuration or simply by increasing the size of the EC2 instance.</p> <p>Some EC2 instance types can drive more I/O throughput than what you can provision for a single EBS volume. You can join multiple&nbsp;<code class="code">gp2</code>,&nbsp;<code class="code">io1</code>,&nbsp;<code class="code">st1</code>, or&nbsp;<code class="code">sc1</code>&nbsp;volumes together in a RAID 0 configuration to use the available bandwidth for these instances.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/ebs_backed_instance.png" alt="" width="510" height="358" /></p> <p>&nbsp;</p> <p>With Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as that particular RAID configuration is supported by the operating system for your instance. This is because all RAID is accomplished at the software level. For greater I/O performance than you can achieve with a single volume, RAID 0 can stripe multiple volumes together; for on-instance redundancy, RAID 1 can mirror two volumes together.</p> <p>Take note that HVM AMIs are required to take advantage of enhanced networking and GPU processing. In order to pass through instructions to specialized network and GPU devices, the OS needs to be able to have access to the native hardware platform which the HVM virtualization provides.&nbsp;</p> <p>Option 2 is incorrect because although the Enhanced Networking feature can provide higher I/O performance and lower CPU utilization to your EC2 instance, you have to use an HVM AMI instead of PV AMI.</p> <p>Option 3 is incorrect because&nbsp;the main use case for RAID 1 is to provide mirroring, redundancy, and fault-tolerance. RAID 0 is a more suitable option for providing faster read and write operations, compared with RAID 1.</p> <p>Option 4 is incorrect because the placement groups feature&nbsp;is primarily used for inter-instance communication.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html">https://aws.amazon.com/ec2/features/#enhanced-networking</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a', 'e'], 'original_assessment_id': 2566882, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You have launched a new enterprise application with a web server and a database. You are using a large EC2 Instance with one 500 GB EBS volume to host a relational database. Upon checking the performance, it shows that write throughput to the database needs to be improved.\xa0 \xa0Which of the following is the most suitable configuration to help you achieve this requirement? (Choose 2)', 'id': 10440584, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Security', 'prompt': {'answers': ['<p>EBS On-Premises Data Encryption</p>', 'S3 Server-Side Encryption', 'S3 Client-Side Encryption', 'Public Data Set Volume Encryption', '<p>S3 On-Premises Data Encryption</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>In a government agency that you are working for, you have been assigned to put confidential tax documents on AWS cloud. However, there is a concern from a security perspective on what can be put on AWS.\xa0 </p><p>What are the features in AWS that can ensure data security for your confidential documents? (Choose 2)</p>', 'explanation': '<p>You can secure the privacy of your data in AWS, both at rest and in-transit, through encryption. If your data is stored in EBS Volumes, you can enable EBS Encryption and if it is stored on Amazon S3, you can enable client-side and server-side encryption.</p> <p>Option 4 is incorrect as public data sets are designed to be publicly accessible.&nbsp;</p> <p>Options 1 and 5 are incorrect as there is no such thing as On-Premises Data Encryption for S3 and EBS as these services are in the AWS cloud and not on your on-premises network.&nbsp;</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b', 'c'], 'original_assessment_id': 2566884, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'In a government agency that you are working for, you have been assigned to put confidential tax documents on AWS cloud. However, there is a concern from a security perspective on what can be put on AWS.\xa0 What are the features in AWS that can ensure data security for your confidential documents? (Choose 2)', 'id': 10440586, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'IAM', 'prompt': {'answers': ['Use Web Identity Federation', 'Use SAML Federation', 'Use IAM users', 'Use AWS VPC'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a large pharmaceutical company that has resources hosted on both their on-premises network and in AWS cloud. They want all of their Software Architects to access resources on both environments using their on-premises credentials, which is stored in Active Directory. <br><br>In this scenario, which of the following can be used to fulfill this requirement?</p>', 'explanation': '<p>Since the company is using&nbsp;Microsoft Active Directory which implements&nbsp;Security Assertion Markup Language (SAML), you can set up a&nbsp;SAML-Based Federation for API Access to your AWS cloud. In this way, you can easily connect to AWS using the login credentials of your on-premises network.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/IAM/latest/UserGuide/images/saml-based-sso-to-console.diagram.png" alt="" width="650" height="375" /></p> <p>&nbsp;</p> <p>AWS supports identity federation with SAML 2.0, an open standard that many identity providers (IdPs) use. This feature enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS APIs without you having to create an IAM user for everyone in your organization. By using SAML, you can simplify the process of configuring federation with AWS, because you can use the IdP\'s service instead of writing custom identity proxy code.</p> <p>Option 1 is incorrect because web identity federation is primarily used to let users sign in via a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google. It does not utilize Active Directory.</p> <p>Option 3 is incorrect because&nbsp;the situation requires you to use the existing credentials stored in their Active Directory, and not user accounts that will be generated by IAM.</p> <p>Option 4 is incorrect because the AWS VPC lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. This has nothing to do with user authentication or Active Directory.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566886, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are working for a large pharmaceutical company that has resources hosted on both their on-premises network and in AWS cloud. They want all of their Software Architects to access resources on both environments using their on-premises credentials, which is stored in Active Directory. In this scenario, which of the following can be used to fulfill this requirement?', 'id': 10440588, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['<p>You are only using an On-Demand EC2 instance which is exactly the same price as Spot EC2 instance, launched by a persistent Spot request.</p>', 'Transferring data from an EC2 instance to an S3 bucket in the same region has no cost at all.', 'Transferring data from an EC2 instance to an S3 bucket in the same region has a 50% discount based on the AWS Pricing.', 'You are only using an On-Demand EC2 instance so the cost will be lower than a Spot instance.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You are a Solutions Architect working for a major telecommunications company in Europe. You deployed an On-Demand EC2 instance that is transferring large amounts of data to an Amazon S3 bucket in the same region.  Your manager is worried about infrastructure cost considering the vast amounts of data being transferred to the bucket. <br><br>What will you say to justify this architecture?', 'explanation': '<p>Transferring data from an EC2 instance to Amazon S3, Amazon Glacier, Amazon DynamoDB, Amazon SES, Amazon SQS, or Amazon SimpleDB in the same AWS Region has no cost at all. Refer to the Amazon EC2 Pricing on the link below for reference.&nbsp;</p> <p>Options 1 and 4 are incorrect since an On-Demand instance costs more than a Spot instance.</p> <p>Option 3 is incorrect as there is no such thing as 50% discount when transferring data from an EC2 instance to an S3 bucket in the same region.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer">https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566888, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are a Solutions Architect working for a major telecommunications company in Europe. You deployed an On-Demand EC2 instance that is transferring large amounts of data to an Amazon S3 bucket in the same region.  Your manager is worried about infrastructure cost considering the vast amounts of data being transferred to the bucket. What will you say to justify this architecture?', 'id': 10440590, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Directory Service', 'prompt': {'answers': ['AWS Directory Service AD Connector', 'AWS Directory Service Simple AD', 'IAM Groups', 'IAM Roles', '<p>Lambda</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': 'A telecommunications company is planning to give AWS Console access to developers. Company policy mandates the use of identity federation and role-based access control. Currently, the roles are already assigned using groups in the corporate Active Directory. <br><br>In this scenario, what combination of the following services can provide developers access to the AWS console? (Choose 2)', 'explanation': '<p>Considering that the company is using a corporate Active Directory, it is best to use&nbsp;AWS Directory Service AD Connector for easier integration. In addition, since the roles are already assigned using groups in the corporate Active Directory, it would be better to&nbsp;also use IAM Roles. Take note that you can assign an IAM Role to the users or groups from your Active Directory once it is integrated with your VPC via the AWS Directory Service AD Connector.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/Products/product-name/diagrams/directory_service_howitworks.80bfccbf2f5d1d63558ec3c086aff247147258f1.png" alt="" width="650" height="207" /></p> <p>&nbsp;</p> <p>AWS Directory Service provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services. Directories store information about users, groups, and devices, and administrators use them to manage access to information and resources. AWS Directory Service provides multiple directory choices for customers who want to use existing Microsoft AD or Lightweight Directory Access Protocol (LDAP)&ndash;aware applications in the cloud. It also offers those same choices to developers who need a directory to manage users, groups, devices, and access.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/blogs/security/how-to-connect-your-on-premises-active-directory-to-aws-using-ad-connector/">https://aws.amazon.com/blogs/security/how-to-connect-your-on-premises-active-directory-to-aws-using-ad-connector/</a></p> <p>&nbsp;</p> <p><strong>Check out these AWS IAM and Directory Service Cheat Sheets:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</a></span></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-directory-service/">https://tutorialsdojo.com/aws-cheat-sheet-aws-directory-service/</a></span></p> <p>&nbsp;</p> <p><strong>Here is a video tutorial on AWS Directory Service:</strong></p> <p><iframe src="https://www.youtube.com/embed/4XeqotTYBtY" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['a', 'd'], 'original_assessment_id': 2566892, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'A telecommunications company is planning to give AWS Console access to developers. Company policy mandates the use of identity federation and role-based access control. Currently, the roles are already assigned using groups in the corporate Active Directory. In this scenario, what combination of the following services can provide developers access to the AWS console? (Choose 2)', 'id': 10440592, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'VPC', 'prompt': {'answers': ['The public subnet', 'The private subnet', 'Either public or private subnet', 'Ideally be launched outside the Amazon VPC'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are tasked to host a web application in a new VPC with private and public subnets. In order to do this, you will need to deploy a new MySQL database server and a fleet of EC2 instances to host the application. In which subnet should you launch the new database server into?</p>', 'explanation': '<p>In an ideal and secure VPC architecture, you launch the web servers or elastic load balancers in the public subnet and the database servers in the private subnet.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-53-23-254fea6a886ca730a6b094cc1f7746f4.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because if you launch your database server in the public subnet, it will be publicly accessible all over the Internet which has a higher security risk.</p> <p>Option 2 is correct because it is more secure to launch your database in the private subnet to prevent other external and unauthorized users to access or attack your system.</p> <p>Option 3 is incorrect since only the private subnet is the correct answer if you want to secure your database from external traffic.</p> <p>Option 4 is incorrect as there is no need to launch it outside the VPC. Having it run in a private subnet should address the security and networking concerns of your database.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566894, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are tasked to host a web application in a new VPC with private and public subnets. In order to do this, you will need to deploy a new MySQL database server and a fleet of EC2 instances to host the application. In which subnet should you launch the new database server into?', 'id': 10440594, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'answers': ['RDS DB instance running as a Multi-AZ deployment', '<p>RDS Read Replica</p>', 'DynamoDB Read Replica', '<p>CloudFront running as a Multi-AZ deployment</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>An application that records weather data every minute is deployed in a fleet of Spot EC2 instances and uses a MySQL RDS database instance. Currently, there is only one RDS instance running in one Availability Zone. You plan to improve the database to ensure high availability and scalability by synchronous data replication to another RDS instance.\xa0 \xa0</p><p>Which of the following performs synchronous data replication in RDS?</p>', 'explanation': '<p>When you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous <strong>standby&nbsp;</strong>replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-06-07_10-00-40-e7c750751ea701ec7b91cbeeb464f364.png" alt="" width="700" height="226" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect as a Read Replica provides an asynchronous replication instead of synchronous.&nbsp;</p> <p>Options 3 and 4 are incorrect as both DynamoDB and CloudFront do not have a Read Replica feature.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/rds/details/multi-az/" target="_blank" rel="noopener">https://aws.amazon.com/rds/details/multi-az/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p> <p>&nbsp;</p> <p><strong>Here is a quick introduction to Amazon RDS:</strong></p> <p><iframe src="https://www.youtube.com/embed/eMzCI7S1P9M" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566896, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'An application that records weather data every minute is deployed in a fleet of Spot EC2 instances and uses a MySQL RDS database instance. Currently, there is only one RDS instance running in one Availability Zone. You plan to improve the database to ensure high availability and scalability by synchronous data replication to another RDS instance.\xa0 \xa0Which of the following performs synchronous data replication in RDS?', 'id': 10440596, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['Data is automatically saved in an EBS volume.', 'Data is unavailable until the instance is restarted.', 'Data will be deleted.', 'Data is automatically saved as an EBS snapshot.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are a newly-hired Solutions Architect in a leading utilities provider, which is in the process of migrating their applications to AWS. You created an EBS-Backed EC2 instance with <code>ephemeral0</code> and <code>ephemeral1</code> instance store volumes attached to host a web application that fetches and stores data from a web API service.\xa0 \xa0</p><p>If this instance is stopped, what will happen to the data on the ephemeral store volumes? </p>', 'explanation': '<p>The word&nbsp;<strong><em>ephemeral</em></strong>&nbsp;means <em>"short-lived"</em> or <em>"temporary"</em> in the English dictionary. Hence, when you see this word in AWS, always consider this as just a temporary memory or a short-lived storage.&nbsp;</p> <p>The virtual devices for instance store volumes are named as&nbsp;<code>ephemeral[0-23]</code>. Instance types that support one instance store volume have&nbsp;<code>ephemeral0</code>. Instance types that support two instance store volumes have&nbsp;<code>ephemeral0</code>&nbsp;and&nbsp;<code>ephemeral1</code>, and so on until&nbsp;<code>ephemeral23</code>.&nbsp;</p> <p>The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists. However, data in the instance store is lost under the following circumstances:</p> <p style="padding-left: 30px;">&nbsp;- The underlying disk drive fails</p> <p style="padding-left: 30px;">&nbsp;- The instance stops</p> <p style="padding-left: 30px;">&nbsp;- The instance terminates</p> <p>&nbsp;</p> <p>Hence, Option 3 is the correct answer.</p> <p>Option 1 is incorrect since instance store volumes and EBS volumes are two different storage types. An Amazon EBS volume is a durable, block-level storage device that you can attach to a single EC2 instance. An instance store provides temporary block-level storage and is located on disks that are physically attached to the host computer. No automatic backup will be performed.</p> <p>Option 2 is incorrect because once you stop an instance, the data in the ephemeral instance store volumes will be gone.</p> <p>Option 4 is incorrect because like Option 2, instance store volumes and EBS volumes are two different storage devices. There is no automated snapshot that will be created.&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html?shortFooter=true#instance-store-lifetime">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html?shortFooter=true#instance-store-lifetime</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566898, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are a newly-hired Solutions Architect in a leading utilities provider, which is in the process of migrating their applications to AWS. You created an EBS-Backed EC2 instance with ephemeral0 and ephemeral1 instance store volumes attached to host a web application that fetches and stores data from a web API service.\xa0 \xa0If this instance is stopped, what will happen to the data on the ephemeral store volumes?', 'id': 10440598, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudWatch', 'prompt': {'answers': ['<p>Use Amazon CloudWatch to monitor the CPU Utilization of your database.</p>', '<p>Create a script that collects and publishes custom metrics to CloudWatch, which tracks the real-time CPU Utilization of the RDS instance, and then set up a custom CloudWatch dashboard to view the metrics.</p>', '<p>Enable Enhanced Monitoring in RDS.</p>', '<p>Check the <code>CPU%</code> and <code>MEM%</code> metrics which are readily available in the Amazon RDS console that shows the percentage of the CPU bandwidth and total memory consumed by each database process of your RDS instance.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>An online cryptocurrency exchange platform is hosted in AWS which uses ECS Cluster and RDS in Multi-AZ Deployments configuration. The application is heavily using the RDS instance to process complex read and write database operations. To maintain the reliability, availability, and performance of your systems, you have to closely monitor how the different processes or threads on a DB instance use the CPU, including the percentage of the CPU bandwidth and total memory consumed by each process.\xa0 \xa0</p><p>Which of the following is the most suitable solution to properly monitor your database?</p>', 'explanation': '<p>Amazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console, or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By default, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the CloudWatch Logs, change the retention for the&nbsp;<code class="code">RDSOSMetrics</code>&nbsp;log group in the CloudWatch console.&nbsp;&nbsp;</p> <p>Take note that&nbsp;there are certain differences between CloudWatch and Enhanced Monitoring Metrics.&nbsp; CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. As a result, you might find differences between the measurements, because the hypervisor layer performs a small amount of work. Hence, Option 3 is the correct answer in this specific scenario.</p> <p>The differences can be greater if your DB instances use smaller instance classes, because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance. Enhanced Monitoring metrics are useful when you want to see how different processes or threads on a DB instance use the CPU.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/metrics2.png" alt="" width="722" height="389" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because although you can use Amazon CloudWatch to monitor the CPU Utilization of your database instance, it does not provide the percentage of the CPU bandwidth and total memory consumed by each database process in your RDS instance. Take note that CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance while RDS Enhanced Monitoring gathers its metrics from an agent on the instance.&nbsp;</p> <p>Option 2 is incorrect because although you can use Amazon CloudWatch Logs and CloudWatch dashboard to monitor the CPU Utilization of the database instance, using CloudWatch alone is still not enough to get the specific percentage of the CPU bandwidth and total memory consumed by each database processes. The data provided by CloudWatch is not as detailed as compared with the Enhanced Monitoring feature in RDS. Take note as well that you do not have direct access to the instances/servers of your RDS database instance, unlike with your EC2 instances where you can install a CloudWatch agent or a custom script to get CPU and memory utilization of your instance.</p> <p>Option 4 is incorrect because&nbsp;the CPU% and MEM% metrics are not readily available in the Amazon RDS console, which is contrary to what is being stated in this option.</p> <p style="padding-left: 30px;">&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs</a></p> <p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html#monitoring-cloudwatch">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html#monitoring-cloudwatch</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</a></span></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></strong></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566900, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'An online cryptocurrency exchange platform is hosted in AWS which uses ECS Cluster and RDS in Multi-AZ Deployments configuration. The application is heavily using the RDS instance to process complex read and write database operations. To maintain the reliability, availability, and performance of your systems, you have to closely monitor how the different processes or threads on a DB instance use the CPU, including the percentage of the CPU bandwidth and total memory consumed by each process.\xa0 \xa0Which of the following is the most suitable solution to properly monitor your database?', 'id': 10440600, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['<p>Amazon Athena</p>', 'Amazon EMR', 'Amazon EC2', 'DynamoDB', '<p>Amazon Neptune</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are a Solutions Architect for a leading Enterprise Resource Planning (ERP) solutions provider and you are instructed to design and set up the architecture of your ERP application in AWS. Your manager instructed you to avoid using fully-managed AWS services and instead, only use specific services which allows you to access the underlying operating system for the resource. This is to allow the company to have a much better control of the underlying resources that their systems are using in the AWS cloud.\xa0 \xa0</p><p>Which of the following services should you choose to satisfy this requirement? (Choose 2)</p>', 'explanation': '<p>Amazon EC2 provides you access to the operating system of the instance that you created.&nbsp;</p> <p>Amazon EMR provides you a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. You can access the operating system of these EC2 instances that were created by Amazon EMR.</p> <p>Options 1, 4 and 5 are incorrect as these are managed services, which means that AWS manages the underlying operating system and other server configurations that these databases use.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/ec2/">https://aws.amazon.com/ec2/</a></p> <p><a href="https://aws.amazon.com/emr/">https://aws.amazon.com/emr/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['b', 'c'], 'original_assessment_id': 2566904, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are a Solutions Architect for a leading Enterprise Resource Planning (ERP) solutions provider and you are instructed to design and set up the architecture of your ERP application in AWS. Your manager instructed you to avoid using fully-managed AWS services and instead, only use specific services which allows you to access the underlying operating system for the resource. This is to allow the company to have a much better control of the underlying resources that their systems are using in the AWS cloud.\xa0 \xa0Which of the following services should you choose to satisfy this requirement? (Choose 2)', 'id': 10440602, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EC2', 'prompt': {'answers': ['<p>The EC2 instance was using an instance store hence, data will be erased when the instance is stopped or terminated.</p>', '<p>The EC2 instance was using EBS-backed root volumes hence, the data will be erased when the instance is terminated.</p>', 'AWS automatically erased the data due to a virus found on the EC2 instance.', 'The EC2 instance has been hacked.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>In the VPC that you are managing, it has one EC2 instance that has its data stored in an instance store. The instance was shut down by a 2nd level support staff over the weekend to save costs. When you arrived in the office the next Monday, you noticed that all data are lost and are no longer available on the EC2 instance.\xa0 \xa0</p><p>What might be the cause of this?</p>', 'explanation': '<p>Since you are using an EC2 instance with an Instance store, its data is ephemeral which means that it will be erased once the instance is stopped or terminated. You may argue that the instance was only shut down but remember that the Operating system shutdown commands always terminate an instance store-backed instance. That is why the right answer is Option 1.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_storage.png" alt="" width="613" height="359" /></p> <p>&nbsp;</p> <p>The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists. However, data in the instance store is lost under any of the following circumstances:</p> <div class="itemizedlist"> <p style="padding-left: 30px;">&nbsp;- The underlying disk drive fails</p> <p style="padding-left: 30px;">&nbsp;- The instance stops</p> <p style="padding-left: 30px;">&nbsp;- The instance terminates</p> <p style="padding-left: 30px;">&nbsp;</p> </div> <p>Therefore, do not rely on instance store for valuable, long-term data. Instead, use more durable data storage, such as Amazon S3, Amazon EBS, or Amazon EFS. When you stop or terminate an instance, every block of storage in the instance store is reset. Hence, your data cannot be accessed through the instance store of another instance.</p> <p>If you create an AMI from an instance, the data on its instance store volumes aren\'t preserved and aren\'t present on the instance store volumes of the instances that you launch from the AMI. You can specify instance store volumes for an instance only when you launch it. You can\'t detach an instance store volume from one instance and attach it to a different instance.</p> <p>Option 2 is incorrect because the data will persist if you use an EBS-backed root volume.&nbsp;&nbsp;</p> <p>Option 3 is incorrect because based on the AWS Shared Responsibility model, AWS will only manage the underlying resources that the services are using and not your actual data. Hence, it is highly unlikely that AWS will automatically erase your data due to a virus.</p> <p>Option 4 is incorrect because although it is remotely possible that someone got hold of your AWS security credentials and deletes your data, this reason is still far fetched and quite unlikely to happen. Based on the given scenario, the stopping of the instance&nbsp;is one key attribute which we can link to its use of Instance Store volumes.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Storage.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Storage.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566906, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'In the VPC that you are managing, it has one EC2 instance that has its data stored in an instance store. The instance was shut down by a 2nd level support staff over the weekend to save costs. When you arrived in the office the next Monday, you noticed that all data are lost and are no longer available on the EC2 instance.\xa0 \xa0What might be the cause of this?', 'id': 10440604, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Security', 'prompt': {'answers': ['Security Group Inbound Rule: Protocol – TCP. Port Range – 22, Source 110.238.98.71/32', 'Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/32', 'Security Group Inbound Rule: Protocol – TCP. Port Range – 22, Source 110.238.98.71/0', 'Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/0'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You have a requirement to make sure that an On-Demand EC2 instance can only be accessed from this IP address (110.238.98.71) via an SSH connection. Which configuration below will satisfy this requirement?', 'explanation': '<div>The SSH protocol uses TCP and port 22. Hence, Options 2 and 4 are incorrect as they are using UDP. Options 1 and 3 have one major difference and that is their CIDR block</div> <div>&nbsp;</div> <div>The requirement is to only allow&nbsp;the individual IP of the client and not the entire network. Therefore,&nbsp;the proper CIDR notation should be used. The <strong>/32</strong> denotes one IP address and the <strong>/0</strong> refers to the entire network. That is why Option 3 is incorrect as it allowed the entire network instead of a single IP.</div> <p>&nbsp;</p> <p>References:</p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html#security-group-rules">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html#security-group-rules</a></p> <p>&nbsp;</p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566908, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You have a requirement to make sure that an On-Demand EC2 instance can only be accessed from this IP address (110.238.98.71) via an SSH connection. Which configuration below will satisfy this requirement?', 'id': 10440606, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'answers': ['The EC2 instance gets terminated automatically by the ELB.', 'The EC2 instance gets quarantined by the ELB for root cause analysis.', 'The EC2 instance is replaced automatically by the ELB.', 'The ELB stops sending traffic to the EC2 instance'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>There are many clients complaining that the online trading application of an investment bank is always down. Your manager instructed you to re-design the architecture of the application to prevent the unnecessary service interruptions. To ensure high availability, you set up the application to use an ELB to distribute the incoming requests across an auto-scaled group of EC2 instances in two single Availability Zones.\xa0 \xa0</p><p>In this scenario, what happens when an EC2 instance behind an ELB fails a health check?</p>', 'explanation': '<p>In this scenario, the load balancer will route&nbsp;the incoming requests only to the healthy instances. When the load balancer determines that an instance is unhealthy, it stops routing requests to that instance. The load balancer resumes routing requests to the instance when it has been restored to a healthy state.</p> <p data-pm-slice="1 1 []">There are two ways of checking the status of your EC2 instances:</p> <p style="padding-left: 30px;">1. Via the Auto Scaling group</p> <p style="padding-left: 30px;">2. Via the ELB health checks</p> <p style="padding-left: 30px;">&nbsp;</p> <p>The default health checks for an Auto Scaling group are <strong>EC2 status checks</strong> only. If an instance fails these status checks, the Auto Scaling group considers the instance unhealthy and replaces. If you attached one or more load balancers or target groups to your Auto Scaling group, the group does not, by default, consider an instance unhealthy and replace it if it fails the load balancer health checks.</p> <p>However, you can optionally configure the Auto Scaling group to use Elastic Load Balancing health checks. This ensures that the group can determine an instance\'s health based on additional tests provided by the load balancer. The load balancer periodically sends pings, attempts connections, or sends requests to test the EC2 instances. These tests are called <em><strong>health checks. </strong></em></p> <p>If you configure the Auto Scaling group to use Elastic Load Balancing health checks, it considers the instance unhealthy if it fails either the EC2 status checks or the load balancer health checks. If you attach multiple load balancers to an Auto Scaling group, all of them must report that the instance is healthy in order for it to consider the instance healthy. If one load balancer reports an instance as unhealthy, the Auto Scaling group replaces the instance, even if other load balancers report it as healthy.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/as-sample-web-architecture-diagram-with-asgs-and-azs.png" alt="" width="495" height="403" /></p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html</a></p> <p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-elb-healthcheck.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</a></span></p> <p>&nbsp;</p> <p><strong>EC2 Instance Health Check vs ELB Health Check vs Auto Scaling and Custom Health Check:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-ec2-instance-health-check-vs-elb-health-check-vs-auto-scaling-and-custom-health-check-2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-ec2-instance-health-check-vs-elb-health-check-vs-auto-scaling-and-custom-health-check-2/</span></a></p> <p>&nbsp;</p> <p><strong>Here is an additional training material on why an Amazon EC2 Auto Scaling group terminates a healthy instance:</strong></p> <p><iframe src="https://www.youtube.com/embed/_ew-J3DQKZg" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2566912, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'There are many clients complaining that the online trading application of an investment bank is always down. Your manager instructed you to re-design the architecture of the application to prevent the unnecessary service interruptions. To ensure high availability, you set up the application to use an ELB to distribute the incoming requests across an auto-scaled group of EC2 instances in two single Availability Zones.\xa0 \xa0In this scenario, what happens when an EC2 instance behind an ELB fails a health check?', 'id': 10440608, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'STS', 'prompt': {'answers': ['<p>Use 3rd party Single Sign-On solutions such as Atlassian Crowd, OKTA, OneLogin and many others.</p>', '<p>Set up a Federation proxy or an Identity provider, and use AWS Security Token Service to generate temporary tokens.</p>', 'Use a resource tag on each folder in the S3 bucket.', '<p>Configure an IAM role and an IAM Policy to access the bucket.</p>', 'Setup up a matching IAM user for each 1200 users in your corporate directory that needs access to a folder in the S3 bucket.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>A tech company that you are working for has undertaken a Total Cost Of Ownership (TCO) analysis evaluating the use of Amazon S3 versus acquiring more storage hardware. The result was that all 1200 employees would be granted access to use Amazon S3 for storage of their personal documents.\xa0 \xa0</p><p>Which of the following will you need to consider so you can set up a solution that incorporates single sign-on feature from your corporate AD or LDAP directory and also restricts access for each individual user to a designated user folder in an S3 bucket? (Choose 2)</p>', 'explanation': '<p>The question refers to one of the common scenarios for temporary credentials in AWS. Temporary credentials are useful in scenarios that involve identity federation, delegation, cross-account access, and IAM roles. In this example, it is called <strong>enterprise identity federation</strong> considering that you also need to set up a&nbsp;single sign-on (SSO) capability.</p> <p>The correct answers are:</p> <p style="padding-left: 30px;">- Setup a Federation proxy or an Identity provider</p> <p style="padding-left: 30px;">- Setup an AWS Security Token Service to generate temporary tokens (Option 2)</p> <p style="padding-left: 30px;">- Configure an IAM role (Option 4)&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/IAM/latest/UserGuide/images/saml-based-federation.diagram.png" width="700" height="393" /></p> <p>&nbsp;</p> <p>In an enterprise identity federation, you can authenticate users in your organization\'s network, and then provide those users access to AWS without creating new AWS identities for them and requiring them to sign in with a separate user name and password. This is known as the&nbsp;<em>single sign-on</em>&nbsp;(SSO) approach to temporary access. AWS STS supports open standards like Security Assertion Markup Language (SAML) 2.0, with which you can use Microsoft AD FS to leverage your Microsoft Active Directory. You can also use SAML 2.0 to manage your own solution for federating user identities.</p> <p>Option 1 is incorrect since you don\'t have to use 3rd party solutions to provide the access. AWS already provides the necessary tools that you can use in this situation.</p> <p>Option 3 is incorrect since placing resource tags on each folder won\'t help restrict access to a specific user.</p> <p>Option 5 is incorrect since creating that many IAM users would be unnecessary. Also, you want the account to integrate with your AD or LDAP directory, hence, IAM Users does not fit these criteria.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:&nbsp;</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></strong></p>'}, 'correct_response': ['b', 'd'], 'original_assessment_id': 2566916, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'A tech company that you are working for has undertaken a Total Cost Of Ownership (TCO) analysis evaluating the use of Amazon S3 versus acquiring more storage hardware. The result was that all 1200 employees would be granted access to use Amazon S3 for storage of their personal documents.\xa0 \xa0Which of the following will you need to consider so you can set up a solution that incorporates single sign-on feature from your corporate AD or LDAP directory and also restricts access for each individual user to a designated user folder in an S3 bucket? (Choose 2)', 'id': 10440610, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'SQS', 'prompt': {'answers': ['The web application is set for long polling so the messages are being sent twice.', 'The web application is not deleting the messages in the SQS queue after it has processed them.', 'The web application is set to short polling so some messages are not being picked up', 'The web application does not have permission to consume messages in the SQS queue. '], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You have a web application hosted in EC2 that consumes messages from an SQS queue and is integrated with SNS to send out an email to you once the process is complete. You received 5 orders but after a few hours, you saw more than 20 email notifications in your inbox. <br><br>Which of the following could be the possible culprit for this issue?</p>', 'explanation': '<p>Always remember that the messages in the SQS queue will continue to exist even after the EC2 instance has processed it, until you delete that message. You have to ensure that you&nbsp;delete the message after processing to prevent the message from being received and processed again once the visibility timeout expires.</p> <p>There are three main parts in a distributed messaging system:</p> <p style="padding-left: 30px;">1. The components of your distributed system (EC2 instances)</p> <p style="padding-left: 30px;">2. Your queue (distributed on Amazon SQS servers)</p> <p style="padding-left: 30px;">3. Messages in the queue.</p> <p>&nbsp;</p> <p>You can set up a system which has several components that send messages to the queue and receive messages from the queue. The queue redundantly stores the messages across multiple Amazon SQS servers.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/images/sqs-message-lifecycle-diagram.png" /></p> <p>&nbsp;</p> <p>Refer to the third step of the SQS Message Lifecycle:</p> <ol> <li>Component 1 sends Message A to a queue, and the message is distributed across the Amazon SQS servers redundantly.</li> <li>When Component 2 is ready to process a message, it consumes messages from the queue, and Message A is returned. While Message A is being processed, it remains in the queue and isn\'t returned to subsequent receive requests for the duration of the visibility timeout.&nbsp;</li> <li>Component 2 <strong>deletes</strong> Message A from the queue to prevent the message from being received and processed again once the visibility timeout expires.</li> </ol> <p>Option 1 is incorrect because long polling helps reduce the cost of using SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren\'t included in a response). Messages being sent twice in&nbsp;an SQS queue configured with long polling is quite unlikely.</p> <p>Option 3 is incorrect since you are receiving emails from SNS where messages are certainly being processed. Following the scenario, messages not being picked up won\'t result into 20 messages being sent to your inbox.</p> <p>Option 4 is incorrect because not having the correct permissions would have resulted in a different response. The scenario says that messages were properly processed but there were over 20 messages that were sent, hence, there is no problem with the accessing the queue.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-message-lifecycle.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-message-lifecycle.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566918, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You have a web application hosted in EC2 that consumes messages from an SQS queue and is integrated with SNS to send out an email to you once the process is complete. You received 5 orders but after a few hours, you saw more than 20 email notifications in your inbox. Which of the following could be the possible culprit for this issue?', 'id': 10440612, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Redshift', 'prompt': {'answers': ['<p>This is not possible with Redshift because it is not intended for OLAP application but rather, for OLTP. Use RDS database instead.</p>', '<p>Create a Lambda function that can accept the number of query queues and use this value to control Redshift.</p>', '<p>Use the workload management (WLM) in the parameter group configuration.</p>', '<p>This is not possible with Redshift because it is not intended for OLAP application but rather, for OLTP. Use a NoSQL DynamoDB database instead.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A company is using Redshift for its online analytical processing (OLAP) application which processes complex queries against large datasets. There is a requirement in which you have to define the number of query queues that are available and how queries are routed to those queues for processing.\xa0 \xa0 </p><p>Which of the following will you use to meet this requirement?</p>', 'explanation': '<p>When you create a parameter group, the default WLM configuration contains one queue that can run up to five queries concurrently. You can add additional queues and configure WLM properties in each of them if you want more control over query processing. Each queue that you add has the same default WLM configuration until you configure its properties. When you add additional queues, the last queue in the configuration is the <em>default queue</em>. Unless a query is routed to another queue based on criteria in the WLM configuration, it is processed by the default queue. You cannot specify user groups or query groups for the default queue.</p> <p>As with other parameters, you cannot modify the WLM configuration in the default parameter group. Clusters associated with the default parameter group always use the default WLM configuration. If you want to modify the WLM configuration, you must create a parameter group and then associate that parameter group with any clusters that require your custom WLM configuration.</p> <p>Option 3 is correct. In Amazon Redshift, you use workload management (WLM) to define the number of query queues that are available, and how queries are routed to those queues for processing. WLM is part of parameter group configuration. A cluster uses the WLM configuration that is specified in its associated parameter group.</p> <p>Options 1 and 4 are incorrect. Redshift is a good choice if you want to perform OLAP transactions in the cloud. On the contrary, RDS and DynamoDB are more suitable for OLTP applications.</p> <p>Option 2 is incorrect since it will be too costly and inefficient to use Lambda. Workload management (WLM) is a feature of Redshift that addresses the problem aptly.</p> <p>&nbsp;</p> <p><strong>Reference</strong>:</p> <p><a href="https://docs.aws.amazon.com/redshift/latest/mgmt/workload-mgmt-config.html">https://docs.aws.amazon.com/redshift/latest/mgmt/workload-mgmt-config.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566920, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'A company is using Redshift for its online analytical processing (OLAP) application which processes complex queries against large datasets. There is a requirement in which you have to define the number of query queues that are available and how queries are routed to those queues for processing.\xa0 \xa0 Which of the following will you use to meet this requirement?', 'id': 10440614, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'answers': ['<p>Create a new snapshot of the current Amazon EBS volume. Restore the snapshot to a new, encrypted Amazon EBS volume. Mount the Amazon EBS volume.</p>', 'Create and mount a new, encrypted Amazon EBS volume. Move the data to the new volume and finally, delete the old Amazon EBS volume.', 'Unmount the EBS volume and then set the encryption attribute to true. Afterwards, re-mount the Amazon EBS volume to the instance.', 'Associate the Amazon EBS volume with your AWS CloudHSM and then remount the Amazon EBS volume.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'A web application that you developed stores sensitive information on a non-boot, unencrypted Amazon EBS data volume attached to an Amazon EC2 instance. Which of the following ways could provide protection to the sensitive data of your Amazon EBS volume?', 'explanation': '<p>Amazon EBS encryption offers a simple encryption solution for your EBS volumes without the need to build, maintain, and secure your own key management infrastructure. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:</p> <ul> <li>-Data at rest inside the volume</li> <li>-All data moving between the volume and the instance</li> <li>-All snapshots created from the volume</li> <li>-All volumes created from those snapshots</li> </ul> <p>&nbsp;</p> <p>In this scenario, the EBS volume attached to the instance is already unencrypted. The best way to encrypt the data is to create and mount a new, encrypted Amazon EBS volume. Then move the data to the new volume and finally, delete the old, unencrypted Amazon EBS volume. Hence, Option 2 is the correct answer.</p> <p>Option 1 is incorrect because a step is missing for this option to be a valid answer. You need to copy the snapshot first while applying encryption parameters, in order for the resulting target snapshot to be encrypted before restoring it to a new encrypted EBS volume.</p> <p>Option 3 is incorrect because you cannot encrypt the volume even if you unmount the volume. Remember that encryption has to be done during volume creation.</p> <p>Option 4 is incorrect because you cannot create an encrypted snapshot of an unencrypted volume or change existing volume from unencrypted to encrypted. You have to create new encrypted volume and transfer data to the new volume.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566922, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'A web application that you developed stores sensitive information on a non-boot, unencrypted Amazon EBS data volume attached to an Amazon EC2 instance. Which of the following ways could provide protection to the sensitive data of your Amazon EBS volume?', 'id': 10440616, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Redshift', 'prompt': {'answers': ['<p>Enable Audit Logging in your Amazon Redshift cluster.</p>', '<p>Enable Enhanced VPC routing on your Amazon Redshift cluster.</p>', '<p>Use the Amazon Redshift Spectrum feature.</p>', '<p>Create a new flow log that tracks the traffic of your Amazon Redshift cluster.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a major telecommunications company where you are assigned to improve the security of your database tier by tightly managing the data flow of your Amazon Redshift cluster. One of the requirements is to use VPC flow logs to monitor all the COPY and UNLOAD traffic of your Redshift cluster that moves in and out of your VPC.\xa0 </p><p>Which of the following is the most suitable solution to implement in this scenario?</p>', 'explanation': '<p>When you use Amazon Redshift Enhanced VPC Routing, Amazon Redshift forces all COPY and UNLOAD traffic between your cluster and your data repositories through your Amazon VPC. By using Enhanced VPC Routing, you can use standard VPC features, such as VPC security groups, network access control lists (ACLs), VPC endpoints, VPC endpoint policies, internet gateways, and Domain Name System (DNS) servers. Hence, Option 2 is the correct answer.</p> <p>You use these features to tightly manage the flow of data between your Amazon Redshift cluster and other resources. When you use Enhanced VPC Routing to route traffic through your VPC, you can also use VPC flow logs to monitor COPY and UNLOAD traffic. If Enhanced VPC Routing is not enabled, Amazon Redshift routes traffic through the Internet, including traffic to other services within the AWS network.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/redshift/latest/mgmt/images/enhanced-routing-create.png" alt="" width="750" height="329" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because the Audit Logging feature is primarily used to get the information about the connection, queries, and user activities in your Redshift cluster.</p> <p>Option 3 is incorrect because the Redshift Spectrum is primarily used to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required.</p> <p>Option 4 is incorrect because,&nbsp;by default, you cannot create a flow log for your Amazon Redshift cluster. You have to enable Enhanced VPC Routing and set up the required VPC configuration.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/redshift/latest/mgmt/enhanced-vpc-routing.html">https://docs.aws.amazon.com/redshift/latest/mgmt/enhanced-vpc-routing.html</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566924, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are working as a Solutions Architect for a major telecommunications company where you are assigned to improve the security of your database tier by tightly managing the data flow of your Amazon Redshift cluster. One of the requirements is to use VPC flow logs to monitor all the COPY and UNLOAD traffic of your Redshift cluster that moves in and out of your VPC.\xa0 Which of the following is the most suitable solution to implement in this scenario?', 'id': 10440618, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EFS', 'prompt': {'answers': ['<p>Create an S3 bucket and use this as the storage for the CMS</p>', '<p>Use EFS</p>', '<p>Upgrade your existing EBS volumes to Provisioned IOPS SSD Volumes</p>', '<p>Use ElastiCache</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand EC2 instances which use Amazon Aurora as its database. Currently, the system stores the file documents that the users uploaded in one of the attached EBS Volumes. Your manager noticed that the system performance is quite slow and he has instructed you to improve the architecture of the system. </p><p>In this scenario, what will you do to implement a scalable, high throughput POSIX-compliant file system?</p>', 'explanation': '<p>Amazon Elastic File System (Amazon EFS) provides simple, scalable, elastic file storage for use with AWS Cloud services and on-premises resources. When mounted on Amazon EC2 instances, an Amazon EFS file system provides a standard file system interface and file system access semantics, allowing you to seamlessly integrate Amazon EFS with your existing applications and tools. Multiple Amazon EC2 instances can access an Amazon EFS file system at the same time, allowing Amazon EFS to provide a common data source for workloads and applications running on more than one Amazon EC2 instance.</p> <p>This particular scenario tests your understanding of EBS, EFS, and S3. In this scenario, there is a fleet of On-Demand EC2 instances that stores file documents from the users to one of the attached EBS Volumes. The system performance is quite slow because the architecture doesn\'t provide the EC2 instances a parallel shared access to the file documents.</p> <p>Remember that an EBS Volume can be attached to one EC2 instance at a time, hence, no other EC2 instance can connect to that EBS Provisioned IOPS Volume. Take note as well that the type of storage needed here is a "file storage" which means that S3 (Option 1) is not the best service to use because it is mainly used for "object storage", and S3 does not provide the notion of "folders" too. This is why Option 2 is the correct answer.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-55-39-826ea8db5a1179c5f79d02b3b6cf1f2c.png" /></p> <p>&nbsp;</p> <p>Option 3 is incorrect because the scenario requires you to set up a scalable, high throughput storage system that will allow concurrent access from multiple EC2 instances. This is clearly not possible in EBS, even with Provisioned IOPS SSD Volumes. You have to use EFS instead.</p> <p>Option 4 is incorrect because&nbsp;ElastiCache is an in-memory data store that improves the performance of your applications, which is not what you need since it is not a file storage.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/efs/">https://aws.amazon.com/efs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EFS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-efs/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-efs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 vs EBS vs EFS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3-vs-ebs-vs-efs/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3-vs-ebs-vs-efs/</a></p> <p>&nbsp;</p> <p><strong>Here\'s a short video tutorial on Amazon EFS:</strong></p> <p><iframe src="https://www.youtube.com/embed/AvgAozsfCrY" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566926, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand EC2 instances which use Amazon Aurora as its database. Currently, the system stores the file documents that the users uploaded in one of the attached EBS Volumes. Your manager noticed that the system performance is quite slow and he has instructed you to improve the architecture of the system. In this scenario, what will you do to implement a scalable, high throughput POSIX-compliant file system?', 'id': 10440620, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudTrail', 'prompt': {'answers': ['CloudTrail for security logs', '<p>CloudWatch</p>', '<p>AWS X-Ray</p>', '<p>Redshift Spectrum</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are an AWS Solutions Architect designing an online analytics application that uses Redshift Cluster for its data warehouse. Which service will allow you to monitor all API calls to your Redshift instance and can also provide secured data for auditing and compliance purposes?</p>', 'explanation': '<p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.</p> <p>CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools,&nbsp;API calls,&nbsp;and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://media.amazonwebservices.com/blog/2014/cloudtrail_revised_flow_2.png" alt="" width="750" height="545" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect because although CloudWatch is also a monitoring service, it cannot track the API calls to your AWS resources.</p> <p>Option 3 is incorrect because AWS X-Ray is not a suitable&nbsp;service to use to track each API call to your AWS resources. It&nbsp;just helps you debug and analyze your microservices applications with request tracing so you can find the root cause of issues and performance.</p> <p>Option 4 is incorrect because Redshift Spectrum is not a monitoring service but rather a feature of Amazon Redshift that enables you to query and analyze all of your data in Amazon S3 using the open data formats you already use, with no data loading or transformations needed.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudTrail Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566928, '_class': 'assessment', 'updated': '2019-06-23T00:32:10Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'You are an AWS Solutions Architect designing an online analytics application that uses Redshift Cluster for its data warehouse. Which service will allow you to monitor all API calls to your Redshift instance and can also provide secured data for auditing and compliance purposes?', 'id': 10440622, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB', 'prompt': {'answers': ['<p>Using the Kinesis Client Library (KCL), write an application that leverages on DynamoDB Streams Kinesis Adapter that will fetch data from the DynamoDB Streams endpoint. When there are updates made by a particular user, notify the subscribers via email using SNS.</p>', '<p>Create a Lambda function that uses DynamoDB Streams Kinesis Adapter which will fetch data from the DynamoDB Streams endpoint. Set up an SNS Topic that will notify the subscribers via email when there is an update made by a particular user.</p>', '<p>Set up a DAX cluster to access the source DynamoDB table. Create a new DynamoDB trigger and a Lambda function. For every update made in the user data, the trigger will send data to the Lambda function which will then notify the subscribers via email using SNS.</p>', '<p>Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to SNS Topic that will notify the subscribers via email.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': "<p>A popular social network is hosted in AWS and is using a DynamoDB table as its database. There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email. Which of the following is the most suitable solution that you should implement to meet the requirement?</p>", 'explanation': '<p>A&nbsp;<em>DynamoDB stream</em>&nbsp;is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.</p> <p>Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attribute(s) of the items that were modified. A&nbsp;<em>stream record&nbsp;</em>contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the "before" and "after" images of modified items.</p> <p>Amazon DynamoDB is integrated with AWS Lambda so that you can create&nbsp;<em>triggers</em>&mdash;pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.</p> <p>If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table\'s stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. The Lambda function can perform any actions you specify, such as sending a notification or initiating a workflow. Hence, the correct answer in this scenario is Option 4.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/StreamsAndTriggers.png" alt="" width="684" height="370" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because although this is a valid solution, it is missing a vital step which is to enable DynamoDB Streams.&nbsp;With the DynamoDB Streams Kinesis Adapter in place, you can begin developing applications via the KCL interface, with the API calls seamlessly directed at the DynamoDB Streams endpoint. Remember that the DynamoDB Stream feature is not enabled by default.</p> <p>Option 2 is incorrect because just like Option 1, you have to manually enable DynamoDB Streams first before you can use its endpoint.</p> <p>Option 3 is incorrect because the DynamoDB Accelerator (DAX) feature is primarily used to significantly improve the in-memory read performance of your database, and not to capture the time-ordered sequence of item-level modifications. You should use&nbsp;DynamoDB Streams in this scenario instead.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p> <p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.Tutorial.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.Tutorial.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon DynamoDB Cheat Sheet:&nbsp;</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</span></a></strong></p>'}, 'correct_response': ['d'], 'original_assessment_id': 7031976, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': "A popular social network is hosted in AWS and is using a DynamoDB table as its database. There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email. Which of the following is the most suitable solution that you should implement to meet the requirement?", 'id': 10440678, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'AWS Systems Manager', 'prompt': {'answers': ['<p>In the ECS task definition file of the ECS Cluster, store the database credentials using Docker Secrets to centrally manage these sensitive data and securely transmit it to only those containers that need access to it. Secrets are encrypted during transit and at rest. A given secret is only accessible to those services which have been granted explicit access to it via IAM Role, and only while those service tasks are running.</p>', '<p>Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt it with KMS. Store the task definition JSON file in a private S3 bucket and ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Create an IAM role to the ECS task definition script that allows access to the specific S3 bucket and then pass the <code>--cli-input-json</code> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials.</p>', '<p>Use the AWS Secrets Manager to store the database credentials and then encrypt them using AWS KMS. Create a resource-based policy for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container.</p>', '<p>Use the AWS Systems Manager Parameter Store to keep the database credentials and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A media company has an Amazon ECS Cluster, which uses the Fargate launch type, to host its news website. The database credentials should be supplied using environment variables, to comply with strict security compliance. As the Solutions Architect, you have to ensure that the credentials are secure and that they cannot be viewed in plaintext on the cluster itself. </p><p>Which of the following is the most suitable solution in this scenario that you can implement with minimal effort?</p>', 'explanation': '<p>Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.</p> <p>Secrets can be exposed to a container in the following ways:</p> <div class="itemizedlist"> <p style="padding-left: 30px;">- To inject sensitive data into your containers as environment variables, use the&nbsp;<code class="code">secrets</code>&nbsp;container definition parameter.</p> <p style="padding-left: 30px;">- To reference sensitive information in the log configuration of a container, use the&nbsp;<code class="code">secretOptions</code>&nbsp;container definition parameter.</p> <p style="padding-left: 30px;">&nbsp;</p> </div> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/08/25/diagram3-1.png" alt="" width="350" height="466" /></p> <div class="aws-note">&nbsp;</div> <p>Within your container definition, specify&nbsp;<code class="code">secrets</code>&nbsp;with the name of the environment variable to set in the container and the full ARN of either the Secrets Manager secret or Systems Manager Parameter Store parameter containing the sensitive data to present to the container. The parameter that you reference can be from a different Region than the container using it, but must be from within the same account. Hence, Option 4 is the correct answer.</p> <p>Option 1 is incorrect because although you can use Docker Secrets to secure the sensitive database credentials, this feature is only applicable in Docker Swarm. In AWS, the recommended way&nbsp;to secure sensitive data is either through the use of&nbsp;Secrets Manager or Systems Manager Parameter Store.</p> <p>Option 2 is incorrect because although the solution may work, it is not recommended to store sensitive credentials in S3. This entails a lot of overhead and manual configuration steps which can be simplified by simply using the Secrets Manager or Systems Manager Parameter Store.</p> <p>Option 3 is incorrect because although the use of&nbsp;Secrets Manager in securing sensitive data in ECS is valid, using an IAM Role is a more suitable choice over a resource-based policy for the Amazon ECS task execution role.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html</a></p> <p><a href="https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/">https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/</a></p> <p>&nbsp;</p> <p><strong>Check out these Amazon ECS and AWS Systems Manager Cheat Sheets:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-container-service-amazon-ecs/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-container-service-amazon-ecs/</a></span></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-systems-manager/">https://tutorialsdojo.com/aws-cheat-sheet-aws-systems-manager/</a></span></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2566932, '_class': 'assessment', 'updated': '2019-06-23T00:32:13Z', 'created': '2019-06-23T00:32:10Z', 'question_plain': 'A media company has an Amazon ECS Cluster, which uses the Fargate launch type, to host its news website. The database credentials should be supplied using environment variables, to comply with strict security compliance. As the Solutions Architect, you have to ensure that the credentials are secure and that they cannot be viewed in plaintext on the cluster itself. Which of the following is the most suitable solution in this scenario that you can implement with minimal effort?', 'id': 10440624, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'API Gateway', 'prompt': {'answers': ['<p>Switch from using AWS Lambda and API Gateway to a more scalable and highly available architecture using EC2 instances, ELB, and Auto Scaling.</p>', '<p>Enable throttling limits and result caching in API Gateway.</p>', '<p>Use CloudFront in front of the API Gateway to act as a cache.</p>', '<p>Move the Lambda function in a VPC.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A cryptocurrency trading platform is using an API built in AWS Lambda and API Gateway. Due to the recent news and rumors about the upcoming price surge of Bitcoin, Ethereum and other cryptocurrencies, it is expected that the trading platform would have a significant increase in site visitors and new users in the coming days ahead. </p>\n\n<p>In this scenario, how can you protect the backend systems of the platform from traffic spikes?</p>', 'explanation': '<p>Amazon API Gateway provides throttling at multiple levels including global and by service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds. Amazon API Gateway tracks the number of requests per second. Any request over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response. Hence,&nbsp;Option 2 is the correct answer.</p> <p>You can add caching to API calls by provisioning an Amazon API Gateway cache and specifying its size in gigabytes. The cache is provisioned for a specific stage of your APIs. This improves performance and reduces the traffic sent to your back end. Cache settings allow you to control the way the cache key is built and the time-to-live (TTL) of the data stored for each method. Amazon API Gateway also exposes management APIs that help you invalidate the cache for each stage.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/09/26/api-gw-settings.png" alt="" width="750-" height="462" /></p> <p>&nbsp;&nbsp;</p> <p>Option 1 is incorrect since there is no need to transfer your applications to other services.</p> <p>Option 3 is incorrect because CloudFront only speeds up content delivery which provides a better latency experience for your users. It does not help much for the backend.</p> <p>Option 4 is incorrect because this answer is irrelevant to what is being asked. A VPC is your own virtual private cloud where you can launch AWS services.</p> <p>&nbsp;</p> <p><strong>Reference</strong>:</p> <p><a href="https://aws.amazon.com/api-gateway/faqs/">https://aws.amazon.com/api-gateway/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon API Gateway Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-api-gateway/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-api-gateway/</span></a></p> <p>&nbsp;</p> <p><strong>Here is an in-depth tutorial on Amazon API Gateway:</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/XwfpPEFHKtQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'}, 'correct_response': ['b'], 'original_assessment_id': 2566934, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'A cryptocurrency trading platform is using an API built in AWS Lambda and API Gateway. Due to the recent news and rumors about the upcoming price surge of Bitcoin, Ethereum and other cryptocurrencies, it is expected that the trading platform would have a significant increase in site visitors and new users in the coming days ahead. \n\nIn this scenario, how can you protect the backend systems of the platform from traffic spikes?', 'id': 10440626, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'answers': ['Create a policy in IAM to deny access from the IP Address block.', 'Modify the Network Access Control List associated with all public subnets in the VPC to deny access from the IP Address block.', 'Add a rule in the Security Group of the EC2 instances to deny access from the IP Address block.', 'Configure the firewall in the operating system of the EC2 instances to deny access from the IP address block.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You are working as a Solutions Architect in a top software development company in Silicon Valley. The company has multiple applications hosted in their VPC. While you are monitoring the system, you noticed that multiple port scans are coming in from a specific IP address block which are trying to connect to several AWS resources inside your VPC. The internal security team has requested that all offending IP addresses be denied for the next 24 hours for security purposes. <br><br>Which of the following is the best method to quickly and temporarily deny access from the specified IP addresses?', 'explanation': '<p>To control the traffic coming in and out of your VPC network, you can use the&nbsp;<em>network access control list (ACL).</em>&nbsp;It is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. This is the best solution among other options as you can easily add and remove the restriction in a matter of minutes.</p> <p>Option 1 is incorrect as an IAM policy does not control the inbound and outbound traffic of your VPC.</p> <p>Option 3 is incorrect as although a Security Group acts as a firewall, it will only control both inbound and outbound traffic at the instance level and not on the whole VPC.&nbsp;</p> <p>Option 4 is incorrect because adding a firewall in the underlying operating system of the EC2 instance is not enough; the attacker can just connect to other AWS resources since the network access control list still allows them to do so.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566936, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You are working as a Solutions Architect in a top software development company in Silicon Valley. The company has multiple applications hosted in their VPC. While you are monitoring the system, you noticed that multiple port scans are coming in from a specific IP address block which are trying to connect to several AWS resources inside your VPC. The internal security team has requested that all offending IP addresses be denied for the next 24 hours for security purposes. Which of the following is the best method to quickly and temporarily deny access from the specified IP addresses?', 'id': 10440628, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudWatch', 'prompt': {'answers': ['<p>Use the default CloudWatch configuration to your EC2 instances where the memory and disk utilization metrics are already available. Install the AWS Systems Manager (SSM) Agent to all of your EC2 instances.</p>', '<p>Install the CloudWatch agent to all of your EC2 instances which gathers the memory and disk utilization data. View the custom metrics in the Amazon CloudWatch console.</p>', '<p>Enable the Enhanced Monitoring option in EC2 and install CloudWatch agent to all of your EC2 instances to be able to view the memory and disk utilization in the CloudWatch dashboard.</p>', '<p>Use Amazon Inspector and install the Inspector agent to all of your EC2 instances.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>Your cloud architecture is composed of Linux and Windows EC2 instances which process high volumes of financial data 24 hours a day, 7 days a week. To ensure high availability of your systems, you are required to monitor the memory and disk utilization of all of your instances.\xa0 \xa0</p><p>Which of the following is the most suitable monitoring solution to implement?</p>', 'explanation': '<p>CloudWatch has available Amazon EC2 Metrics for you to use for monitoring CPU utilization, Network utilization, Disk performance, and Disk Reads/Writes. In case that you need to monitor the below items,&nbsp;you need to prepare a custom metric using a Perl or other shell script, as there are no ready to use metrics for these:</p> <ol> <li>Memory utilization</li> <li>disk swap utilization</li> <li>disk space utilization</li> <li>page file utilization</li> <li>log collection</li> </ol> <p>Take note that there is a multi-platform CloudWatch agent which can be installed on both Linux and Windows-based instances. You can use a single agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. This agent supports both Windows Server and Linux and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU core. It is recommended that you use the new agent instead of the older monitoring scripts to collect metrics and logs.&nbsp;&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/aeb-container-cw.png" alt="" width="650" height="397" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because, by default, CloudWatch does not automatically provide memory and disk utilization metrics of your instances. You have to set up custom CloudWatch metrics to monitor the memory, disk swap, disk space and page file utilization of your instances.</p> <p>Option 3 is incorrect because Enhanced Monitoring is a feature of RDS and not of CloudWatch.</p> <p>Option 4 is incorrect because Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances. It does not provide a custom metric to track the memory and disk utilization of each and every EC2 instance in your VPC.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring_ec2.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html#using_put_script">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html#using_put_script</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</a></span></p> <p>&nbsp;</p> <p><strong>CloudWatch Agent vs &nbsp;SSM Agent vs Custom Daemon Scripts:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566938, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'Your cloud architecture is composed of Linux and Windows EC2 instances which process high volumes of financial data 24 hours a day, 7 days a week. To ensure high availability of your systems, you are required to monitor the memory and disk utilization of all of your instances.\xa0 \xa0Which of the following is the most suitable monitoring solution to implement?', 'id': 10440630, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Highly Available Network Design', 'prompt': {'answers': ['Write a script that checks the health of the EC2 instance. If the instance stops responding, the script will switch the elastic IP address to a standby EC2 instance.', 'Assign an Elastic IP address to the instance.', 'Postpone the deployment until you have fully converted the application to work with the ELB and Auto Scaling.', 'Launch the instance using Auto Scaling which will deploy the instance again if it becomes unhealthy.', '<p>Use Cloudfront with a custom origin pointed to your on-premises network where the web application is deployed.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working for a software company that has moved a legacy application from an on-premises data center to the cloud. The legacy application requires a static IP address hard-coded into the backend, which blocks you from using an Application Load Balancer. <br><br>Which steps would you take to apply high availability and fault tolerance to this application without ELB? (Choose 2)</p>', 'explanation': '<p>For this scenario, it is best to set up a self-monitoring EC2 instance with a virtual IP Address. You can use an Elastic IP and then write a custom script that checks the health of the EC2 instance and if the instance stops responding, the script will switch the Elastic IP address to a standby EC2 instance.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://s3.amazonaws.com/apnblog/2016+Blog+Images/VPC+Pt+2/VIP.png" /></p> <p>&nbsp;</p> <p>A custom script&nbsp;enables one Amazon Elastic Compute Cloud (EC2) instance to monitor another Amazon EC2 instance and take over a private&nbsp;"virtual"&nbsp;IP address on instance failure. When used with two instances, the script enables a High Availability scenario where instances monitor each other and take over a shared virtual IP address if the other instance fails. It could easily be modified to run on a third-party monitoring or witness server to perform the VIP swapping on behalf of the two monitored nodes.</p> <p>Option 3 is incorrect because you don\'t have to postpone your deployment as you have the option to set up a self-monitoring EC2 instance with an EIP address.</p> <p>Option 4 is incorrect as even though the Auto Scaling group provides high availability and scalability, it still depends on ELB which is not available in this scenario.&nbsp;Take note that you need to have a static IP&nbsp;address which can be in the form of an Elastic IP. Although an Auto Scaling group can scale out if one of the EC2 instances became unhealthy, you still cannot directly assign an EIP&nbsp;to an Auto Scaling group. In addition, you are only limited to use EC2 instance status checks for your&nbsp;Auto Scaling group if you do not have an ELB which can provide you the actual health check of your application (<em>using its port</em>), and not just the health of the EC2 instance.</p> <p>Option 5 is incorrect because although this option is feasible, the goal of the company is to move the application to the cloud and not to continue using its on-premises resources.&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/articles/leveraging-multiple-ip-addresses-for-virtual-ip-address-fail-over-in-6-simple-steps" target="_blank" rel="noopener">https://aws.amazon.com/articles/leveraging-multiple-ip-addresses-for-virtual-ip-address-fail-over-in-6-simple-steps</a></p> <p><a href="https://aws.amazon.com/blogs/apn/amazon-vpc-for-on-premises-network-engineers-part-two/">https://aws.amazon.com/blogs/apn/amazon-vpc-for-on-premises-network-engineers-part-two/</a></p> <p>&nbsp;</p>'}, 'correct_response': ['a', 'b'], 'original_assessment_id': 2566940, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You are working for a software company that has moved a legacy application from an on-premises data center to the cloud. The legacy application requires a static IP address hard-coded into the backend, which blocks you from using an Application Load Balancer. Which steps would you take to apply high availability and fault tolerance to this application without ELB? (Choose 2)', 'id': 10440632, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'CloudFront', 'prompt': {'answers': ['<p>Configure your CloudFront distribution to use Match Viewer as its Origin Protocol Policy which will automatically match the user request. This will allow access to the private content if the request is a paying member and deny it if it is not a member.</p>', '<p>Create a Signed URL with a custom policy which only allows the members to see the private files.</p>', '<p>Configure your CloudFront distribution to use Field-Level Encryption to protect your private data and only allow access to members.</p>', '<p>Use Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to determine whether a user should have access to your content. For members, send the required <code>Set-Cookie</code> headers to the viewer which will unlock the content only to them.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A web application is using CloudFront to distribute their images, videos, and other static contents stored in their S3 bucket to its users around the world. The company has recently introduced a new member-only access to some of its high quality media files. There is a requirement to provide access to multiple private media files only to their paying subscribers without having to change their current URLs.\xa0 \xa0</p><p>Which of the following is the most suitable solution that you should implement to satisfy this requirement?</p>', 'explanation': '<p>CloudFront signed URLs and signed cookies provide the same basic functionality: they allow you to control who can access your content. If you want to serve private content through CloudFront and you\'re trying to decide whether to use signed URLs or signed cookies, consider the following:</p> <p>Use <strong>signed URLs</strong>&nbsp;for the following cases:</p> <div> <ul> <li>-You want to use an RTMP distribution. Signed cookies aren\'t supported for RTMP distributions.</li> <li>-You want to restrict access to individual files, for example, an installation download for your application.</li> <li>-Your users are using a client (for example, a custom HTTP client) that doesn\'t support cookies.</li> </ul> </div> <p>Use <strong>signed cookies</strong>&nbsp;for the following cases:</p> <div> <ul> <li>-You want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers\' area of a website.</li> <li>-You don\'t want to change your current URLs.</li> </ul> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/PrivateContent_TwoParts.png" alt="" width="422" height="319" /></p> </div> <p>&nbsp;</p> <p>Hence, the correct answer to this scenario is Option 4.</p> <p>Option 1 is incorrect because a Match Viewer is an Origin Protocol Policy which configures CloudFront to communicate with your origin using HTTP or HTTPS, depending on the protocol of the viewer request. CloudFront caches the object only once even if viewers make requests using both HTTP and HTTPS protocols.&nbsp;</p> <p>Option 2 is incorrect because&nbsp;Signed URLs are primarily used for providing access to individual files, as shown on the above explanation. In addition, the scenario explicitly says that they don\'t want to change their current URLs which is why implementing Signed Cookies is more suitable than Signed URL.</p> <p>Option 3 is incorrect because&nbsp;Field-Level Encryption only allows you to securely upload user-submitted sensitive information to your web servers. It does not provide access to download multiple private files.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2566942, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'A web application is using CloudFront to distribute their images, videos, and other static contents stored in their S3 bucket to its users around the world. The company has recently introduced a new member-only access to some of its high quality media files. There is a requirement to provide access to multiple private media files only to their paying subscribers without having to change their current URLs.\xa0 \xa0Which of the following is the most suitable solution that you should implement to satisfy this requirement?', 'id': 10440634, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'answers': ['The volume can be used as normal while the snapshot is in progress.', 'The volume can be used in write-only mode while the snapshot is in progress.', 'The volume can be used in read-only mode while the snapshot is in progress.', 'The volume cannot be used until the snapshot completes.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You have triggered the creation of a snapshot of your EBS volume and is currently on-going. At this point, what are the things that the EBS volume can or cannot do?', 'explanation': '<p>EBS snapshots occur asynchronously which makes option 1 the correct answer. This means that the point-in-time snapshot is created immediately, but the status of the snapshot is&nbsp;<code>pending</code>&nbsp;until the snapshot is complete (when all of the modified blocks have been transferred to Amazon S3), which can take several hours for large initial snapshots or subsequent snapshots where many blocks have changed. While it is completing, an in-progress snapshot is not affected by ongoing reads and writes to the volume hence, you can still use the volume.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/snapshot_1a.png" width="700" height="691" /></p> <p>&nbsp;</p> <p>Option 2, 3 and 4 are incorrect because you will still be able to perform normal read and write operations on your EBS volume even while a snapshot is ongoing. Although you can take a snapshot of a volume while a previous snapshot of that volume is in the pending status, having multiple pending snapshots of a volume may result in reduced volume performance until the snapshots complete.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566944, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You have triggered the creation of a snapshot of your EBS volume and is currently on-going. At this point, what are the things that the EBS volume can or cannot do?', 'id': 10440636, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ElastiCache', 'prompt': {'answers': ['<p>DynamoDB</p>', '<p>Amazon RDS</p>', 'Amazon ElastiCache', '<p>Amazon Redshift</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A startup based in Australia is deploying a new two-tier web application in AWS. The Australian company wants to store their most frequently used data in an in-memory data store to improve the retrieval and response time of their web application.\xa0 \xa0</p><p>Which of the following is the most suitable service to be used for this requirement?</p>', 'explanation': '<p>Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/elasticache/Databases@2x.2ccaf2321ff1e9c857d9405d62e04de7e087c6ad.png" width="750" height="285" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because DynamoDB is primarily used as a NoSQL database which supports both document and key-value store models. ElastiCache is a more suitable service to use than DynamoDB, if you need an in-memory data store.</p> <p>Option 2 is incorrect because RDS is mainly used as a relational database and not as a data storage for frequently used data.</p> <p>Option 4 is incorrect&nbsp;because Redshift is a data warehouse service and is not suitable to be used as an in-memory data store.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/elasticache/">https://aws.amazon.com/elasticache/</a></p> <p><a href="https://aws.amazon.com/products/databases/">https://aws.amazon.com/products/databases/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Elasticache Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/</span></a>&nbsp;</p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566946, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'A startup based in Australia is deploying a new two-tier web application in AWS. The Australian company wants to store their most frequently used data in an in-memory data store to improve the retrieval and response time of their web application.\xa0 \xa0Which of the following is the most suitable service to be used for this requirement?', 'id': 10440638, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'answers': ['Amazon RDS', 'Amazon RDS with Multi-AZ deployments', 'Amazon EC2 instances with data replication in one Availability Zone', 'Amazon EC2 instances with data replication between two different Availability Zones'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are designing a multi-tier web application architecture that consists of a fleet of EC2 instances and an Oracle relational database server. It is required that the database is highly available and that you have full control over its underlying operating system. <br><br>Which AWS service will you use for your database tier?</p>', 'explanation': '<p>To achieve this requirement, you can deploy your Oracle database to Amazon EC2 instances with data replication between two different Availability Zones. Hence, option 4 is the correct answer. The deployment of this architecture can easily be achieved by using Cloudformation and Quick Start. Please refer to the reference link for information.</p> <p>The Quick Start deploys the Oracle primary database (using the preconfigured, general-purpose starter database from Oracle) on an Amazon EC2 instance in the first Availability Zone. It then sets up a second EC2 instance in a second Availability Zone, copies the primary database to the second instance by using the&nbsp;<code class="code">DUPLICATE</code>&nbsp;command, and configures Oracle Data Guard.</p> <p>Options 1 and 2 are incorrect because the scenario requires you to have access to the underlying operating system of the database server. Remember that Amazon RDS is a managed database service, which means that Amazon is the one that manages the underlying operating system of the database instance and not you.</p> <p>Option 3 is incorrect since deploying to just one Availability Zone (AZ) will not make the database tier highly available. If that AZ went down, your database will be unavailable.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/quickstart/">https://aws.amazon.com/quickstart/</a></p> <p><a href="https://docs.aws.amazon.com/quickstart/latest/oracle-database/architecture.html">https://docs.aws.amazon.com/quickstart/latest/oracle-database/architecture.html</a></p> <p><a href="http://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.ReplicationInstance.html">http://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.ReplicationInstance.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2566948, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You are designing a multi-tier web application architecture that consists of a fleet of EC2 instances and an Oracle relational database server. It is required that the database is highly available and that you have full control over its underlying operating system. Which AWS service will you use for your database tier?', 'id': 10440640, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'answers': ['The EC2 instance gets terminated automatically by the Application Load Balancer.', 'The EC2 instance gets quarantined by the Application Load Balancer for root cause analysis.', 'The EC2 instance is replaced automatically by the Application Load Balancer.', 'The Application Load Balancer stops sending traffic to the instance that failed its health check.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You have a new e-commerce web application written in Angular framework which is deployed to a fleet of EC2 instances behind an Application Load Balancer. You configured the load balancer to perform health checks on these EC2 instances. <br><br>What will happen if one of these EC2 instances failed the health checks?', 'explanation': '<p>In case that one of the EC2 instances failed a health check, the Application Load Balancer stops sending traffic to that instance.</p> <p>Your Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called&nbsp;<em>health checks</em>. Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target group with which the target is registered. After your target is registered, it must pass one health check to be considered healthy. After each health check is completed, the load balancer node closes the connection that was established for the health check.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</a></span></p> <p>&nbsp;</p> <p><strong>EC2 Instance Health Check vs ELB Health Check vs Auto Scaling and Custom Health Check:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-ec2-instance-health-check-vs-elb-health-check-vs-auto-scaling-and-custom-health-check-2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-ec2-instance-health-check-vs-elb-health-check-vs-auto-scaling-and-custom-health-check-2/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2566950, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You have a new e-commerce web application written in Angular framework which is deployed to a fleet of EC2 instances behind an Application Load Balancer. You configured the load balancer to perform health checks on these EC2 instances. What will happen if one of these EC2 instances failed the health checks?', 'id': 10440642, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Auto Scaling', 'prompt': {'answers': ['<p>The EC2 instance which has the least number of user sessions</p>', '<p>The EC2 instance which has been running for the longest time</p>', '<p>The EC2 instance which belongs to an Auto Scaling group with the oldest launch configuration</p>', '<p>The instance will be randomly selected by the Auto Scaling group</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A suite of web applications is composed of several different Auto Scaling group of EC2 instances which is configured with default settings and then deployed across three Availability Zones. There is an Application Load Balancer that forwards the request to the respective target group on the URL path. The scale-in policy has been triggered due to the low number of incoming traffic to the application.\xa0 </p><p>Which EC2 instance will be the first one to be terminated by your Auto Scaling group?</p>', 'explanation': '<p>The default termination policy is designed to help ensure that your network architecture spans Availability Zones evenly. With the default termination policy, the behavior of the Auto Scaling group is as follows:</p> <p style="padding-left: 30px;">1. If there are instances in multiple Availability Zones, choose the Availability Zone with the most instances and at least one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, choose the Availability Zone with the instances that use the oldest launch configuration.</p> <p style="padding-left: 30px;">2. Determine which unprotected instances in the selected Availability Zone use the oldest launch configuration. If there is one such instance, terminate it.</p> <p style="padding-left: 30px;">3. If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) If there is one such instance, terminate it.</p> <p style="padding-left: 30px;">4. If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random.</p> <p>The following flow diagram illustrates how the default termination policy works:</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/termination-policy-default-flowchart-diagram.png" /> <br /><br /></p> <p><strong>Reference:</strong><br /><br /><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566954, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'A suite of web applications is composed of several different Auto Scaling group of EC2 instances which is configured with default settings and then deployed across three Availability Zones. There is an Application Load Balancer that forwards the request to the respective target group on the URL path. The scale-in policy has been triggered due to the low number of incoming traffic to the application.\xa0 Which EC2 instance will be the first one to be terminated by your Auto Scaling group?', 'id': 10440644, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'answers': ['<p>General Purpose SSD</p>', '<p>Provisioned IOPS SSD</p>', '<p>Throughput Optimized HDD</p>', '<p>Cold HDD</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are building a new data analytics application in AWS which will be deployed in an Auto Scaling group of On-Demand EC2 instances and a MongoDB database. It is expected that the database will have high-throughput workloads performing small, random I/O operations. As the Solutions Architect, you are required to properly set up and launch the required resources in AWS.\xa0 </p><p>Which of the following is the most suitable EBS type to use for your database?</p>', 'explanation': '<p>On a given volume configuration, certain I/O characteristics drive the performance behavior for your EBS volumes. SSD-backed volumes, such as General Purpose SSD (<code class="code">gp2</code>) and Provisioned IOPS SSD (<code class="code">io1</code>), deliver consistent performance whether an I/O operation is random or sequential. HDD-backed volumes like Throughput Optimized HDD (<code class="code">st1</code>) and Cold HDD (<code class="code">sc1</code>) deliver optimal performance only when I/O operations are large and sequential.</p> <p>In the exam, always consider the difference between SSD and HDD as shown on the table below. This will allow you to easily eliminate specific EBS-types in the options which are not SSD or not HDD, depending on whether the question asks for a storage type which has <strong><em>small, random</em></strong> I/O operations or <strong><em>large, sequential</em></strong> I/O operations.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_22-34-15-d1fd30e8eaa8701ddd964e5878e78242.png" width="750" /></p> <p>&nbsp;</p> <p>Provisioned IOPS SSD (<code class="code">io1</code>) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike&nbsp;<code class="code">gp2</code>, which uses a bucket and credit model to calculate performance, an&nbsp;<code class="code">io1</code>&nbsp;volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers within 10 percent of the provisioned IOPS performance 99.9 percent of the time over a given year.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_05-52-53-2f6fe79f1600d1f1c4eaab87872423d7.png" width="750" height="346" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because although General Purpose is a type of SSD that can handle small, random I/O operations, the Provisioned IOPS SSD volumes are much more suitable to meet the needs of I/O-intensive database workloads such as MongoDB, Oracle, MySQL, and many others.&nbsp;</p> <p>Options 3 and 4 are incorrect because HDD volumes (such as Throughput Optimized HDD and Cold HDD volumes) are more suitable for workloads with large,&nbsp;sequential I/O operations instead of small, random I/O operations.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_piops">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_piops</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566956, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You are building a new data analytics application in AWS which will be deployed in an Auto Scaling group of On-Demand EC2 instances and a MongoDB database. It is expected that the database will have high-throughput workloads performing small, random I/O operations. As the Solutions Architect, you are required to properly set up and launch the required resources in AWS.\xa0 Which of the following is the most suitable EBS type to use for your database?', 'id': 10440646, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFront', 'prompt': {'answers': ['Configure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.', '<p>Use CloudFront distributions for your photos.</p>', 'Block the IP addresses of the offending websites using NACL.', 'Store photos on an Amazon EBS volume of the web server.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You have launched a travel photo sharing website using Amazon S3 to serve high-quality photos to visitors of your website. After a few days, you found out that there are other travel websites linking and using your photos. This resulted in financial losses for your business.\xa0 \xa0</p><p>What is an effective method to mitigate this issue?</p>', 'explanation': '<p>In Amazon S3, all objects are private by default. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.</p> <p>When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration.</p> <p>Anyone who receives the pre-signed URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_00-58-48-19fb174d579477b46d422a9b264f6455.jpg" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect. CloudFront is a content delivery network service that speeds up delivery of content to your customers.</p> <p>Option 3 is also incorrect. Blocking IP address using NACLs is not a very efficient method because a quick change in IP address would easily bypass this configuration.</p> <p>Option 4 is also incorrect. You cannot serve objects directly from an EBS volume, which needs to be attached to an EC2 instance. EBS volumes also do not provide the same durability as compared to S3.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</a></span></p> <p>&nbsp;</p> <p><strong>S3 Pre-signed URLs vs CloudFront Signed URLs vs Origin Access Identity (OAI)</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566958, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You have launched a travel photo sharing website using Amazon S3 to serve high-quality photos to visitors of your website. After a few days, you found out that there are other travel websites linking and using your photos. This resulted in financial losses for your business.\xa0 \xa0What is an effective method to mitigate this issue?', 'id': 10440648, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'answers': ['<p>Launch an Oracle database instance in RDS with Recovery Manager (RMAN) enabled.</p>', '<p>Launch an Oracle Real Application Clusters (RAC) in RDS.</p>', '<p>Create an Oracle database in RDS with Multi-AZ deployments.</p>', '<p>Migrate your Oracle data to Amazon Aurora by converting the database schema using AWS Schema Conversion Tool and AWS Database Migration Service. </p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A Forex trading platform, which frequently processes and stores global financial data every minute, is hosted in your on-premises data center and uses an Oracle database. Due to a recent cooling problem in their data center, the company urgently needs to migrate their infrastructure to AWS to improve the performance of their applications. As the Solutions Architect, you are responsible in ensuring that the database is properly migrated and should remain available in case of database server failure in the future.\xa0 </p><p>Which of the following is the most suitable solution to meet the requirement? </p>', 'explanation': '<p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/con-multi-AZ.png" alt="" width="484" height="344" /></p> <p>&nbsp;</p> <p>In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.</p> <p>In this scenario, the best RDS configuration to use is an Oracle database in RDS with Multi-AZ deployments to ensure high availability even if the primary database instance goes down. Hence, Option 3 is the correct answer.</p> <p>Options 1 and 2 are incorrect because Oracle RMAN and RAC are not supported in RDS.</p> <p>Option 4 is incorrect because although this solution is feasible, it takes time to migrate your Oracle database to&nbsp;Aurora which is not acceptable. Based on this option, the&nbsp;Aurora database does not have a Read Replica and is not configured as an Amazon Aurora DB cluster, which could have improved the availability of the database.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://aws.amazon.com/rds/details/multi-az/">https://aws.amazon.com/rds/details/multi-az/</a></p> <p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 7031948, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'A Forex trading platform, which frequently processes and stores global financial data every minute, is hosted in your on-premises data center and uses an Oracle database. Due to a recent cooling problem in their data center, the company urgently needs to migrate their infrastructure to AWS to improve the performance of their applications. As the Solutions Architect, you are responsible in ensuring that the database is properly migrated and should remain available in case of database server failure in the future.\xa0 Which of the following is the most suitable solution to meet the requirement?', 'id': 10440676, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Amazon MQ', 'prompt': {'answers': ['<p>Amazon MQ</p>', '<p>Amazon SQS</p>', '<p>Amazon SNS</p>', '<p>Amazon SWF</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A multi-tiered application hosted in your on-premises data center is scheduled to be migrated to AWS. The application has a message broker service which uses industry standard messaging APIs and protocols that must be migrated as well, without rewriting the messaging code in your application.\xa0 </p><p>Which of the following is the most suitable service that you should use to move your messaging service to AWS?</p>', 'explanation': '<p>Amazon MQ, Amazon SQS, and Amazon SNS are messaging services that are suitable for anyone from startups to enterprises. If you\'re using messaging with existing applications and want to move your messaging&nbsp;service to the cloud quickly and easily, it is&nbsp;recommended that you consider Amazon MQ. It supports industry-standard APIs and protocols so you can switch from any standards-based message broker to Amazon MQ without rewriting the messaging code in your applications. Hence, Option 1 is the correct answer.</p> <p>If you are building brand new applications in the cloud, then it is highly recommended that you consider Amazon SQS and Amazon SNS. Amazon SQS and SNS are lightweight, fully managed message queue and topic services that scale almost infinitely and provide simple, easy-to-use APIs. You can use Amazon SQS and SNS to decouple and scale microservices, distributed systems, and serverless applications, and improve reliability.</p> <p>&nbsp;</p> <p><strong><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/images/amazon-mq-architecture-active-standby-deployment.png" /></strong></p> <p>&nbsp;</p> <p>Option 2 is incorrect because although Amazon SQS is a&nbsp;fully managed message queuing service, it does not support an extensive&nbsp;list of industry-standard messaging APIs and protocol, unlike Amazon MQ. Moreover, using Amazon SQS requires you to do additional changes in the messaging code of applications to make it compatible.</p> <p>Option 3 is incorrect because SNS is more suitable as a&nbsp;pub/sub messaging service instead of&nbsp;a message broker service.&nbsp;</p> <p>Option 4 is incorrect because SWF is a fully-managed state tracker and task coordinator service and not a messaging service, unlike Amazon MQ, AmazonSQS and Amazon SNS.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/amazon-mq/faqs/">https://aws.amazon.com/amazon-mq/faqs/</a></p> <p><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html#sqs-difference-from-amazon-mq-sns">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html#sqs-difference-from-amazon-mq-sns</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566960, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'A multi-tiered application hosted in your on-premises data center is scheduled to be migrated to AWS. The application has a message broker service which uses industry standard messaging APIs and protocols that must be migrated as well, without rewriting the messaging code in your application.\xa0 Which of the following is the most suitable service that you should use to move your messaging service to AWS?', 'id': 10440650, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'API Gateway', 'prompt': {'answers': ['<p>Use throttling limits in API Gateway</p>', '<p>API Gateway will automatically scale and handle massive\xa0traffic spikes so you do not have to do anything.</p>', '<p>Manually upgrade the EC2 instances being used by\xa0API Gateway</p>', '<p>Deploy\xa0Multi-AZ in API Gateway with Read Replica</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are using a combination of API Gateway and Lambda for the web services of your online web portal that is being accessed by hundreds of thousands of clients each day. Your company will be announcing a new revolutionary product and it is expected that your web portal will receive a massive number of visitors all around the globe. How can you protect your backend systems and applications from traffic spikes?</p>', 'explanation': '<p>Amazon API Gateway provides throttling at multiple levels including global and by a service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds.</p> <p>Amazon API Gateway tracks the number of requests per second. Any requests over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response.</p> <p>Option 2 is incorrect because although it can scale using AWS Edge locations, you still need to configure the throttling to further manage the bursts of your APIs.</p> <p>Option 3 is incorrect because API Gateway is a fully managed service and hence, you do not have access to its underlying resources.</p> <p>Option 4 is incorrect because RDS has Multi-AZ and Read Replica capabilities, and not API Gateway.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/api-gateway/faqs/#Throttling_and_Caching">https://aws.amazon.com/api-gateway/faqs/#Throttling_and_Caching</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon API Gateway Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-api-gateway/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-api-gateway/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566962, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You are using a combination of API Gateway and Lambda for the web services of your online web portal that is being accessed by hundreds of thousands of clients each day. Your company will be announcing a new revolutionary product and it is expected that your web portal will receive a massive number of visitors all around the globe. How can you protect your backend systems and applications from traffic spikes?', 'id': 10440652, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Aurora', 'prompt': {'answers': ['<p>Configure your application to use the reader endpoint for both production traffic and reporting queries, which will enable your Aurora database to automatically perform load-balancing among all the Aurora Replicas.</p>', '<p>In your application, use the cluster endpoint of your Aurora database to handle the incoming production traffic and use the instance endpoint to handle reporting queries.</p>', '<p>Create a new custom endpoint in Aurora which will load-balance database connections based on the specified criteria. Configure your application to use the custom endpoint for both production traffic and reporting queries.</p>', '<p>In your application, use the writer endpoint of your Aurora database to handle the production traffic. Create a new custom endpoint to handle reporting queries.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>An online shopping platform is hosted on an Auto Scaling group of Spot EC2 instances and uses Amazon Aurora PostgreSQL as its database. There is a requirement to optimize your database workloads in your cluster where you have to direct production traffic to your high-capacity instances and point the reporting queries sent by your internal staff to the low-capacity instances. </p><p>Which is the most suitable configuration for your application as well as your Aurora database cluster to achieve this requirement?</p>', 'explanation': '<p>Amazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an&nbsp;<em>endpoint</em>. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don\'t have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren\'t available.</p> <p>For certain Aurora tasks, different instances or groups of instances perform different roles. For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-04-03_00-40-18-850e58ca7802358579be53729bfc0cbb.gif" alt="" width="880" height="423" /></p> <p>&nbsp;</p> <p>Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For example, to perform DDL statements you can connect to whichever instance is the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance.</p> <p>The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read-write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. For example, you might direct internal users to low-capacity instances for report generation or ad hoc (one-time) querying, and direct production traffic to high-capacity instances. Hence, Option 3 is the correct answer.</p> <p>Option 1 is incorrect because although it is true that a reader endpoint enables your Aurora database to automatically perform load-balancing among all the Aurora Replicas, it is quite limited to doing read operations only. You still need to use a custom endpoint to load-balance the database connections based on the specified criteria.&nbsp;</p> <p>Option 2 is incorrect because&nbsp;a cluster endpoint (also known as a writer endpoint) for an Aurora DB cluster simply connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations in the database such as DDL statements, which is perfect for handling production traffic but not suitable for handling queries for reporting. This kind of endpoint does not have the functionality to automatically perform load-balancing among all the Aurora Replicas of your cluster.</p> <p>Option 4 is incorrect because although this configuration may work, it is not the most suitable option since you can just use one custom endpoint instead of using a separate writer/cluster endpoint to handle the production traffic.</p> <p>&nbsp;</p> <p><strong>Reference</strong>:</p> <p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Aurora Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/amazon-aurora/">https://tutorialsdojo.com/amazon-aurora/</a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566964, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'An online shopping platform is hosted on an Auto Scaling group of Spot EC2 instances and uses Amazon Aurora PostgreSQL as its database. There is a requirement to optimize your database workloads in your cluster where you have to direct production traffic to your high-capacity instances and point the reporting queries sent by your internal staff to the low-capacity instances. Which is the most suitable configuration for your application as well as your Aurora database cluster to achieve this requirement?', 'id': 10440654, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['<p>Use S3 Infrequently Accessed storage to store the data.</p>', 'Enable S3 Versioning and Multi-Factor Authentication Delete on the bucket.', '<p>Set up a signed URL for all users.</p>', 'Create an IAM bucket policy that disables delete operation.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You are a Solutions Architect in your company working with 3 DevOps Engineers under you. One of the engineers accidentally deleted a file hosted in Amazon S3 which has caused disruption of service. <br><br>What can you do to prevent this from happening again?', 'explanation': '<p>To avoid accidental deletion in Amazon S3 bucket, you can:</p> <p>&nbsp; &nbsp; - Enable Versioning</p> <p>&nbsp; &nbsp; - Enable MFA (Multi-Factor Authentication) Delete</p> <p>&nbsp;</p> <p>Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.</p> <p>If the MFA (Multi-Factor Authentication) Delete is enabled, it requires additional authentication for either of the following operations:</p> <p>&nbsp; &nbsp; - Change the versioning state of your bucket</p> <p>&nbsp; &nbsp; - Permanently delete an object version</p> <p>&nbsp;</p> <p>Option 1 is incorrect. Switching your storage class to S3 Infrequent Access won\'t help mitigate accidental deletions.</p> <p>Option 3 is incorrect. Signed URLs give you more control over access to your content, so this feature deals more on accessing rather than deletion.</p> <p>Option 4 is incorrect. If you create a bucket policy preventing deletion, other users won\'t be able to delete objects that should be deleted. You only want to prevent accidental deletion, not disable the action itself.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566966, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You are a Solutions Architect in your company working with 3 DevOps Engineers under you. One of the engineers accidentally deleted a file hosted in Amazon S3 which has caused disruption of service. What can you do to prevent this from happening again?', 'id': 10440656, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'answers': ['Make a snapshot of the database', 'Enabled Multi-AZ failover ', 'Increase the database instance size', 'Create a read replica'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'There are a lot of outages in the Availability Zone of your RDS database instance to the point that you have lost access to the database. What could you do to prevent losing access to your database in case that this event happens again?', 'explanation': '<p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. For this scenario, option 2 is correct. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/con-multi-AZ.png" alt="" width="484" height="344" /></p> <p>&nbsp;</p> <p>In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete.&nbsp;</p> <p>In option 1, creating a snapshot allows you to have a backup of your database, but it does not provide immediate availability in case of AZ failure. So this is incorrect.</p> <p>For option 3, increasing database instance size is not a solution for this problem. Doing this action addresses the need to upgrade your compute capacity but does not solve the requirement of providing access to your database even in the event of a loss of one of the Availability Zones.</p> <p>Option 4 is incorrect because read replicas provide enhanced performance for read-heavy database workloads. Although you can promote a read replica, its asynchronous replication might not provide you the latest version of your database.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/rds/details/multi-az/">https://aws.amazon.com/rds/details/multi-az/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566968, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'There are a lot of outages in the Availability Zone of your RDS database instance to the point that you have lost access to the database. What could you do to prevent losing access to your database in case that this event happens again?', 'id': 10440658, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB', 'prompt': {'answers': ['<p>Redshift Spectrum</p>', 'DynamoDB', '<p>RDS</p>', 'ElastiCache', 'Glacier'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': 'You are working for a large financial company as an IT consultant. Your role is to help their development team to build a highly available web application using stateless web servers. In this scenario, which AWS services are suitable for storing session state data? (Choose 2)', 'explanation': '<p>Options 2 and 4 are the correct answers. You can store session state data on both DynamoDB and ElastiCache. These AWS services provide high-performance storage of key-value pairs which can be used to build a highly available web application.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_01-00-29-3a01a791766edb97563653a3c08ac334.PNG" alt="" width="844" height="401" />Option 1 is incorrect since Redshift Spectrum is a data warehousing solution where you can directly query data from your&nbsp;data warehouse. Redshift is not suitable for storing session state, but more on analytics and OLAP processes.</p> <p>Option 3 is incorrect as well since RDS is a relational database solution of AWS. This relational storage type might not be the best fit for session states, and it might not provide the performance you need compared to DynamoDB for the same cost.</p> <p>Option 5 is incorrect as well since Glacier is a low-cost cloud storage service for data archiving and long-term backup. The archival and retrieval speeds of Glacier is too slow for handling session states.</p> <div>&nbsp;</div> <p><strong>References:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/caching/database-caching/">https://aws.amazon.com/caching/database-caching/</a></p> <p><a href="https://aws.amazon.com/caching/session-management/">https://aws.amazon.com/caching/session-management/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Elasticache Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/</span></a>&nbsp;</p>'}, 'correct_response': ['b', 'd'], 'original_assessment_id': 2566970, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You are working for a large financial company as an IT consultant. Your role is to help their development team to build a highly available web application using stateless web servers. In this scenario, which AWS services are suitable for storing session state data? (Choose 2)', 'id': 10440660, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Lambda', 'prompt': {'answers': ['<p>Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.</p>', '<p>Use multiple and geographically disperse VPCs to various AWS regions then create a transit VPC to connect all of your resources. In order to handle the requests faster, set up Lambda functions in each region using the AWS Serverless Application Model (SAM) service.</p>', '<p>Configure your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code> to increase the cache hit ratio of your CloudFront distribution.</p>', '<p>Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user.</p>', '<p>Set up an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': "<p>A popular social media website uses a CloudFront web distribution to serve their static contents to their millions of users around the globe. They are receiving a number of complaints recently that their users take a lot of time to log into their website. There are also occasions when their users are getting HTTP 504 errors. You are instructed by your manager to significantly reduce the user's login time to further optimize the system. </p><p>Which of the following options should you use together to set up a cost-effective solution that can improve your application's performance? (Choose 2)</p>", 'explanation': '<p>Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:</p> <div> <ul> <li>-After CloudFront receives a request from a viewer (viewer request)</li> <li>-Before CloudFront forwards the request to the origin (origin request)</li> <li>-After CloudFront receives the response from the origin (origin response)</li> <li>-Before CloudFront forwards the response to the viewer (viewer response)</li> </ul> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/lambda/latest/dg/images/cloudfront-events-that-trigger-lambda-functions.png" alt="" width="545" height="194" /></p> </div> <p>&nbsp;</p> <p>In the given scenario, you can use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users. In addition, you can set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin fails. This will alleviate the occasional HTTP 504 errors that users are experiencing. Therefore, the correct answers are Options 1 and 5.</p> <p>Option 2 is incorrect because of the same reason provided in Option 1 above. Although setting up multiple VPCs across various regions which&nbsp;are connected with a transit VPC is valid, this solution still entails higher setup and maintenance costs. A more cost-effective option would be to use Lambda@Edge instead.</p> <p>Option 3 is incorrect because&nbsp;improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the sluggish authentication process of your global users and not just the caching of the static objects.</p> <p>Option 4 is incorrect because&nbsp;although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will&nbsp;improve the performance of the application with <strong>minimal cost</strong>.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p> <p><a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html">https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html</a></p> <p>&nbsp;</p> <p><strong>Check out these Amazon CloudFront and AWS Lambda Cheat Sheets:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</span></a></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/</span></a></p>'}, 'correct_response': ['a', 'e'], 'original_assessment_id': 2566972, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': "A popular social media website uses a CloudFront web distribution to serve their static contents to their millions of users around the globe. They are receiving a number of complaints recently that their users take a lot of time to log into their website. There are also occasions when their users are getting HTTP 504 errors. You are instructed by your manager to significantly reduce the user's login time to further optimize the system. Which of the following options should you use together to set up a cost-effective solution that can improve your application's performance? (Choose 2)", 'id': 10440662, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'IAM', 'prompt': {'answers': ['IAM users are created by default with partial permissions', 'IAM users are created by default with full permissions', 'IAM users are created by default with no permissions', 'You need to wait for 24 hours for the new IAM user to have access.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You have a new joiner in your organization. You have provisioned an IAM user for the new employee in AWS however, the user is not able to perform any actions. What could be the reason for this?</p>', 'explanation': '<p>The reason for this issue is that IAM users are created with no permissions&nbsp;by default. That means that when you created the new IAM user, you might not provisioned any permissions to the user. Hence, option 3 is correct and conversely, options 1 and 2 are wrong.</p> <p>Option 4 is incorrect because provisions are applied immediately, and not after 24 hours.</p> <p>The IAM user might need to make API calls or use the AWS CLI or the Tools for Windows PowerShell. In that case, create an access key (an access key ID and a secret access key) for that user. This is called Programmatic access.</p> <p>If the user needs to access AWS resources from the AWS Management Console, create a password and provide it to the user.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/iam/details/manage-users/">https://aws.amazon.com/iam/details/manage-users/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2566974, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You have a new joiner in your organization. You have provisioned an IAM user for the new employee in AWS however, the user is not able to perform any actions. What could be the reason for this?', 'id': 10440664, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['<p>Standard Reserved Instances can be later exchanged for other Convertible Reserved Instances</p>', '<p>It can enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration.</p>', "<p>Reserved Instances doesn't get interrupted unlike Spot instances in the event that there are not enough unused EC2 instances to meet the demand.</p>", "<p>It runs in a VPC on hardware that's dedicated to a single customer.</p>", '<p>You can have capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term through Scheduled Reserved Instances</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>Your company announced that there would be a surprise IT audit on all of the AWS resources being used in the production environment. During the audit activities, it was noted that you are using a combination of Standard and Scheduled Reserved EC2 instances in your applications. They argued that you should have used Spot EC2 instances instead as it is cheaper than the Reserved Instance. </p><p>Which of the following are the characteristics and benefits of using these two types of Reserved EC2 instances, which you can use as justification? (Choose 2)</p>', 'explanation': '<p>Reserved Instances (RIs) provide you with a significant discount (up to 75%) compared to On-Demand instance pricing. You have the flexibility to change families, OS types, and tenancies while benefiting from RI pricing when you use Convertible RIs. One important thing to remember here is that Reserved Instances are <span style="text-decoration: underline;">not</span> physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account.&nbsp;</p> <p>When your computing needs change, you can modify your Standard or Convertible Reserved Instances and continue to take advantage of the billing benefit. You can modify the Availability Zone, scope, network platform, or instance size (within the same instance type) of your Reserved Instance. You can also&nbsp;sell your unused instance on the Reserved Instance Marketplace.</p> <p>Option 1 is incorrect because only&nbsp;<span data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;Convertible Reserved Instances can be later exchanged for other Reserved Instances&quot;}" data-sheets-userformat="{&quot;2&quot;:769,&quot;3&quot;:{&quot;1&quot;:0},&quot;11&quot;:0,&quot;12&quot;:0}">Convertible Reserved Instances can be exchanged for other Convertible Reserved Instances.</span></p> <p>Option 2 is incorrect because you can indeed modify the Availability Zone, scope, network platform, or instance size of your Reserved Instance as long as it is within the same instance type.&nbsp;</p> <p>Option 3 is correct because you can definitely use Auto Scaling on Reserved Instances. Remember that it is basically just a billing concept hence, you can use features like Auto Scaling with your Reserved Instances, same as with your Spot and On-Demand instances. Keep in mind that Reserved Instances are not physical servers/instances, but rather a billing discount applied to the use of On-Demand Instances in your account.&nbsp;</p> <p>Option 4 is wrong because that is the description of a Dedicated instance and not a Reserved Instance. A Dedicated instance runs in a VPC on hardware that\'s dedicated to a single customer.</p> <p>Option 5 is correct because reserved instances can be used to lower costs. Reserved Instances provide you with a discount on usage of EC2 instances, and a capacity reservation when they are applied to a specific Availability Zone, giving you additional confidence that you will be able to launch the instances you have reserved when you need them.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-modifying.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-modifying.html</a></p> <p><a href="https://aws.amazon.com/ec2/pricing/reserved-instances/">https://aws.amazon.com/ec2/pricing/reserved-instances/</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html</a>&nbsp;</p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c', 'e'], 'original_assessment_id': 2566976, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'Your company announced that there would be a surprise IT audit on all of the AWS resources being used in the production environment. During the audit activities, it was noted that you are using a combination of Standard and Scheduled Reserved EC2 instances in your applications. They argued that you should have used Spot EC2 instances instead as it is cheaper than the Reserved Instance. Which of the following are the characteristics and benefits of using these two types of Reserved EC2 instances, which you can use as justification? (Choose 2)', 'id': 10440666, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'KMS', 'prompt': {'answers': ['<p>Security Token Service</p>', '<p>EBS encryption</p>', '<p>Elastic File System (EFS)</p>', '<p>AWS KMS API</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a government project in which they are building an online portal to allow people to pay their taxes and claim their tax refunds online. Due to the confidentiality of data, the security policy requires that the application hosted in EC2 encrypts the data first before writing it to the disk for storage.\xa0 \xa0</p><p>In this scenario, which service would you use to meet this requirement?</p>', 'explanation': '<p>AWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. The master keys that you create in AWS KMS are protected by FIPS 140-2 validated cryptographic modules. AWS KMS is integrated with most other AWS services that encrypt your data with encryption keys that you manage. AWS KMS is also integrated with AWS CloudTrail to provide encryption key usage logs to help meet your auditing, regulatory and compliance needs.</p> <p>In this scenario, you can configure your application to use the KMS API to encrypt all data before saving it to disk. Hence, Option 4 is the correct answer.</p> <p>Option 1 is incorrect because AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). It is not used for encrypting data unlike KMS.</p> <p>Option 2 is incorrect because although EBS encryption provides additional security for the EBS volumes, the application could not use this service to encrypt or decrypt each individual data that it writes on the disk. It is better to use KMS API instead to automatically encrypt the data before saving it to disk.</p> <p>Option 3 is incorrect because EFS is a storage service and does not provide encryption services unlike KMS API.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://docs.aws.amazon.com/kms/latest/developerguide/programming-top.html">https://docs.aws.amazon.com/kms/latest/developerguide/programming-top.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;AWS Key Management Service (KMS) Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-key-management-service-aws-kms/">https://tutorialsdojo.com/aws-cheat-sheet-aws-key-management-service-aws-kms/</a></span></p>'}, 'correct_response': ['d'], 'original_assessment_id': 4371674, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You are working as a Solutions Architect for a government project in which they are building an online portal to allow people to pay their taxes and claim their tax refunds online. Due to the confidentiality of data, the security policy requires that the application hosted in EC2 encrypts the data first before writing it to the disk for storage.\xa0 \xa0In this scenario, which service would you use to meet this requirement?', 'id': 10440668, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['<p>Unlike other Amazon object storage classes, which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ.</p>', '<p>Storing data in S3 One Zone-IA costs less than storing it in S3 Standard-IA.</p>', '<p>Storing data in S3 One Zone-IA costs more than storing it in S3 Standard-IA but provides more durability.</p>', '<p>Unlike other Amazon object storage classes, which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in two AZs only. Hence the name, One Zone-IA since the data replication is skipped in one Availability Zone.</p>', '<p>S3 One Zone-IA offers lower durability and low throughput compared with Amazon S3 Standard and S3 Standard-IA which is why it has a low per GB storage price and per GB retrieval fee.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working as a Solutions Architect in a new startup that provides storage for high-quality photos which are infrequently accessed by the users. To make the architecture cost-effective, you designed the cloud service to use an S3 One Zone-Infrequent Access (S3 One Zone-IA) storage type for free users and an S3 Standard-Infrequent Access (S3 Standard-IA) storage type for premium users. When your manager found out about this, he asked you about the trade-offs of using S3 One Zone-IA instead of the S3 Standard-IA.\xa0 </p><p>What will you say to your manager? (Choose 2)</p>', 'explanation': '<div> <p>Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) is an Amazon S3 storage class for data that is accessed less frequently but requires rapid access when needed. Unlike other Amazon object storage classes, which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ. Because of this, storing data in S3 One Zone-IA costs 20% less than storing it in S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA storage. It&rsquo;s a good choice, for example, for storing secondary backup copies of on-premises data or easily re-creatable data, or for storage used as an S3 Cross-Region Replication target from another AWS Region.</p> <p>S3 One Zone-IA offers the same high durability, high throughput, and low latency of Amazon S3 Standard and S3 Standard-IA, with a low per GB storage price and per GB retrieval fee. The S3 One Zone-IA storage class is set at the object level and can exist in the same bucket as S3 Standard and S3 Standard-IA, allowing you to use S3 Lifecycle Policies to automatically transition objects between storage classes without any application changes.</p> <p><strong>Key Features:</strong></p> <ul> <li>-Same low latency and high throughput performance of S3 Standard and S3 Standard-IA</li> <li>-Designed for durability of 99.999999999% of objects in a single Availability Zone, but data will be lost in the event of Availability Zone destruction</li> <li>-Designed for 99.5% availability over a given year</li> <li>-Backed with the&nbsp;Amazon S3 Service Level Agreement for availability</li> <li>-Supports SSL for data in transit and encryption of data at rest</li> <li>-Lifecycle management for automatic migration of objects</li> </ul> </div> <div>&nbsp;</div> <div>Remember that since the S3 One Zone-IA stores data in a single AWS Availability Zone, data stored in this storage class will be lost in the event of Availability Zone destruction.</div> <div>&nbsp;</div> <div>&nbsp;</div> <div><strong>Reference</strong>:&nbsp;</div> <div>&nbsp;</div> <div><a href="https://aws.amazon.com/s3/storage-classes/#Amazon_S3_One_Zone-Infrequent_Access">https://aws.amazon.com/s3/storage-classes/#Amazon_S3_One_Zone-Infrequent_Access</a></div> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['a', 'b'], 'original_assessment_id': 4814324, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You are working as a Solutions Architect in a new startup that provides storage for high-quality photos which are infrequently accessed by the users. To make the architecture cost-effective, you designed the cloud service to use an S3 One Zone-Infrequent Access (S3 One Zone-IA) storage type for free users and an S3 Standard-Infrequent Access (S3 Standard-IA) storage type for premium users. When your manager found out about this, he asked you about the trade-offs of using S3 One Zone-IA instead of the S3 Standard-IA.\xa0 What will you say to your manager? (Choose 2)', 'id': 10440670, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'DynamoDB', 'prompt': {'answers': ['<p>Reduce the number of partition keys in the DynamoDB table.</p>', '<p>Use partition keys with high-cardinality attributes, which have a large number of distinct values for each item.</p>', '<p>Use partition keys with low-cardinality attributes, which have a few number of distinct values for each item.</p>', '<p>Avoid using a composite primary key, which is composed of a partition key and a sort key.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using DynamoDB. You are instructed to improve the database performance by distributing the workload evenly and using the provisioned throughput efficiently.\xa0 \xa0 </p><p>Which of the following would you consider to implement for your DynamoDB table?</p>', 'explanation': '<p>The partition key portion of a table\'s primary key determines the logical partitions in which a table\'s data is stored. This in turn affects the underlying physical partitions. Provisioned I/O capacity for the table is divided evenly among these physical partitions. Therefore a partition key design that doesn\'t distribute I/O requests evenly can create "hot" partitions that result in throttling and use your provisioned I/O capacity inefficiently.</p> <p>The optimal usage of a table\'s provisioned throughput depends not only on the workload patterns of individual items, but also on the partition-key design. This doesn\'t mean that you must access all partition key values to achieve an efficient throughput level, or even that the percentage of accessed partition key values must be high. It does mean that the more distinct partition key values that your workload accesses, the more those requests will be spread across the partitioned space. In general, you will use your provisioned throughput more efficiently as the ratio of partition key values accessed to the total number of partition key values increases.</p> <p>One example for this is the use of partition keys with high-cardinality attributes, which have a large number of distinct values for each item. Hence, Option 2 is the correct answer.</p> <p>Option 1 is incorrect because instead of reducing the number of partition keys in your DynamoDB table, you should actually add more to improve its performance to distribute the I/O requests evenly and not avoid "hot" partitions.</p> <p>Option 3 is incorrect because this is the exact opposite of the correct answer. Remember that the more distinct partition key values your workload accesses, the more those requests will be spread across the partitioned space. Conversely, the less distinct partition key values, the less evenly spread it would be across the partitioned space, which effectively slows the performance.</p> <p>Option 4 is incorrect because, just like Option 2, a composite primary key will provide more partition for the table and in turn, improves the performance. Hence, it should be used and not avoided.</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html</a></p> <p><a href="https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/"> https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 5802010, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using DynamoDB. You are instructed to improve the database performance by distributing the workload evenly and using the provisioned throughput efficiently.\xa0 \xa0 Which of the following would you consider to implement for your DynamoDB table?', 'id': 10440672, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ElastiCache', 'prompt': {'answers': ['<p>Set up an IAM Policy and MFA which requires the Cloud Engineers to enter their IAM credentials and token before they can access the ElastiCache cluster.</p>', '<p>Set up a Redis replication group and enable the <code>AtRestEncryptionEnabled</code> parameter.</p>', '<p>Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the <code>--transit-encryption-enabled</code> and <code>--auth-token</code> parameters enabled.</p>', '<p>Enable the in-transit encryption for Redis replication groups.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are designing a banking portal which uses Amazon ElastiCache for Redis as its distributed session management component. Since the other Cloud Engineers in your department have access to your ElastiCache cluster, you have to secure the session data in the portal by requiring them to enter a password before they are granted permission to execute Redis commands.\xa0 \xa0</p><p>As the Solutions Architect, which of the following should you do to meet the above requirement?</p>', 'explanation': '<p>Using Redis&nbsp;<code class="code">AUTH</code>&nbsp;command can improve data security by requiring the user to enter a password before they are granted permission to execute Redis commands on a password-protected Redis server. Hence, Option 3 is the correct answer.</p> <p>To require that users enter a password on a password-protected Redis server, include the parameter&nbsp;<code class="code"><strong>--auth-token</strong></code>&nbsp;with the correct password when you create your replication group or cluster and on all subsequent commands to the replication group or cluster.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/images/ElastiCache-Redis-Secure-Compliant.png" alt="" width="600" height="381" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because this is not possible in IAM. You have to use the Redis AUTH option instead.</p> <p>Option 2 is incorrect because&nbsp;the Redis At-Rest Encryption feature only secures&nbsp;the data inside&nbsp;the in-memory data store. You have to use Redis AUTH option instead.</p> <p>Option 4 is incorrect because although&nbsp;in-transit encryption is part of the solution, it is missing the most important thing which is the Redis AUTH option.</p> <p>&nbsp;</p> <p><strong>References</strong>:</p> <p><a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Elasticache cheat sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/</span></a></p> <p>&nbsp;</p> <p><strong>Redis (cluster mode enabled vs disabled) vs Memcached:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-redis-cluster-mode-enabled-vs-disabled-vs-memcached/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-redis-cluster-mode-enabled-vs-disabled-vs-memcached/</span></a></p> <p>&nbsp;</p>'}, 'correct_response': ['c'], 'original_assessment_id': 7011948, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You are designing a banking portal which uses Amazon ElastiCache for Redis as its distributed session management component. Since the other Cloud Engineers in your department have access to your ElastiCache cluster, you have to secure the session data in the portal by requiring them to enter a password before they are granted permission to execute Redis commands.\xa0 \xa0As the Solutions Architect, which of the following should you do to meet the above requirement?', 'id': 10440674, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'AWS Shield', 'prompt': {'answers': ['<p>Use AWS Shield to detect and mitigate DDoS attacks.</p>', '<p>Using the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks and other DDoS attacks.</p>', '<p>Set up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic.</p>', '<p>A combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You have identified a series of DDoS attacks while monitoring your VPC. As the Solutions Architect, you are responsible in fortifying your current cloud infrastructure to protect the data of your clients.\xa0 </p><p>Which of the following is the most suitable solution to mitigate these kinds of attacks?</p>', 'explanation': '<p>For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/aws-answers/answers-images/web-app-ddos-mitigation.4fc1b1d10a6e8517dcd80547544c1ee4f423da1b.png" width="550" height="199" /></p> <p>&nbsp;</p> <p>AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing(ELB), Amazon CloudFront, and Amazon Route 53 charges.</p> <p>Option 2 is incorrect because the AWS Firewall Manager is mainly used to simplify your AWS WAF administration and maintenance tasks across multiple accounts and resources. It does not protect your VPC against DDoS attacks.</p> <p>Option 3 is incorrect because even though AWS WAF can help you block common attack patterns to your VPC such as SQL injection or cross-site scripting, this is still not enough to withstand DDoS attacks. It is better to use AWS Shield in this scenario.</p> <p>Option 4 is incorrect because although using a combination of Security Groups and NACLs are valid to provide security to your VPC, this is not enough to mitigate a DDoS attack. You should use AWS Shield for better security protection.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf">https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a></p> <p><a href="https://aws.amazon.com/shield/">https://aws.amazon.com/shield/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Shield Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-shield/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-shield/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 7110478, '_class': 'assessment', 'updated': '2019-06-23T00:32:11Z', 'created': '2019-06-23T00:32:11Z', 'question_plain': 'You have identified a series of DDoS attacks while monitoring your VPC. As the Solutions Architect, you are responsible in fortifying your current cloud infrastructure to protect the data of your clients.\xa0 Which of the following is the most suitable solution to mitigate these kinds of attacks?', 'id': 10440680, 'related_lectures': [], 'assessment_type': 'multiple-choice'}]}, 'type': 'practice-test', 'title': 'AWS Certified Solutions Architect Associate Practice Test 1'}, {'quiz_data': {'next': None, 'count': 65, 'previous': None, 'results': [{'section': 'OpsWorks', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. OpsWorks has three offerings - AWS Opsworks for Chef Automate, AWS OpsWorks for Puppet Enterprise, and AWS OpsWorks Stacks.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://media.amazonwebservices.com/blog/2016/opsworks_chef_auto_welcome_1.png" alt="" width="700" height="521" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because AWS SWF is a fully-managed state tracker and task coordinator in the Cloud. It does not let you leverage Chef recipes.</p> <p>Option 2 is incorrect because Elastic Beanstalk handles an application\'s deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. It does not let you leverage Chef recipes just like Option 1.</p> <p>Option 3 is incorrect because CloudFormation is a service that lets you create a collection of related AWS resources and provision them in a predictable fashion using infrastructure as code. It does not let you leverage Chef recipes just like Options 1 and 2.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/opsworks/">https://aws.amazon.com/opsworks/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this&nbsp;AWS OpsWorks Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-opsworks/">https://tutorialsdojo.com/aws-cheat-sheet-aws-opsworks/</a></span></p> <p>&nbsp;</p> <p><strong>Elastic Beanstalk vs &nbsp;CloudFormation vs OpsWorks vs CodeDeploy:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</span></a></p>', 'relatedLectureIds': '', 'question': 'You are a Solutions Architect working with a company that uses Chef Configuration management in their datacenter. Which service is designed to let the customer leverage existing Chef recipes in AWS?', 'answers': ['Amazon Simple Workflow Service', 'AWS Elastic Beanstalk', 'AWS CloudFormation', 'AWS OpsWorks']}, 'correct_response': ['d'], 'original_assessment_id': 2566810, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are a Solutions Architect working with a company that uses Chef Configuration management in their datacenter. Which service is designed to let the customer leverage existing Chef recipes in AWS?', 'id': 10442412, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Amazon EC2 uses public&ndash;key cryptography to encrypt and decrypt login information. Public&ndash;key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a&nbsp;<em>key pair</em>.</p> <p>To log in to your instance, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance.&nbsp;On a Linux instance, the public key content is placed in an entry within&nbsp;<code>~/.ssh/authorized_keys</code>. This is done at boot time and enables you to securely access your instance using the private key instead of a password.</p> <p>Options 1 and 2 are incorrect as both&nbsp;Custom EC2 password and EC2 Connection Strings do not exist.</p> <p>Option 4 is incorrect as Access Keys are used for API calls and not for logging in to EC2.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You work for a leading university as an AWS Infrastructure Engineer and also as a professor to aspiring AWS architects. As a way to familiarize your students with AWS, you gave them a project to host their applications to an EC2 instance. One of your students created an instance to host their online enrollment system project but is having a hard time connecting to their newly created EC2 instance. Your students have explored all of the troubleshooting guides by AWS and narrowed it down to login issues.\xa0 \xa0</p><p>Which of the following can you use to log into an EC2 instance?</p>', 'answers': ['Custom EC2 password', 'EC2 Connection Strings', 'Key Pairs', 'Access Keys']}, 'correct_response': ['c'], 'original_assessment_id': 2566800, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You work for a leading university as an AWS Infrastructure Engineer and also as a professor to aspiring AWS architects. As a way to familiarize your students with AWS, you gave them a project to host their applications to an EC2 instance. One of your students created an instance to host their online enrollment system project but is having a hard time connecting to their newly created EC2 instance. Your students have explored all of the troubleshooting guides by AWS and narrowed it down to login issues.\xa0 \xa0Which of the following can you use to log into an EC2 instance?', 'id': 10442402, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Cold HDD volumes provide low-cost magnetic storage that defines performance in terms of throughput rather than IOPS. With a lower throughput limit than Throughput Optimized HDD, this is a good fit ideal for large, sequential cold-data workloads. If you require infrequent access to your data and are looking to save costs, Cold HDD provides inexpensive block storage. Take note that bootable Cold HDD volumes are not supported.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-12-17_12-12-20-ce9961d8c4466dd46d97bc76a7004fb6.png" alt="" width="750" height="343" /></p> <p>&nbsp;</p> <p>Cold HDD provides the lowest cost HDD volume and is designed for less frequently accessed workloads. Hence, Option 4 is the correct answer.</p> <p>In the exam, always consider the difference between SSD and HDD as shown on the table below. This will allow you to easily eliminate specific EBS-types in the options which are not SSD or not HDD, depending on whether the question asks for a storage type which has <strong><em>small, random</em></strong> I/O operations or <strong><em>large, sequential</em></strong> I/O operations.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_22-34-15-d1fd30e8eaa8701ddd964e5878e78242.png" width="750" />&nbsp;</p> <p>&nbsp;</p> <p>Option 1 is incorrect because a General purpose SSD volume costs more and it is mainly used for a wide variety of workloads.&nbsp;It is recommended to be used as system boot volumes, virtual desktops, low-latency interactive apps, and many more.</p> <p>Option 2 is incorrect because Provisioned IOPS HDD costs more than the Cold HDD and thus, not cost-effective for this scenario. It provides the highest performance SSD volume for mission-critical low-latency or high-throughput workloads, which is not needed in the scenario.</p> <p>Option 3 is incorrect because Throughput Optimized HDD is primarily used for <strong>frequently</strong> accessed, throughput-intensive workloads. In this scenario, Cold HDD perfectly fits the requirement as it is used for their infrequently accessed data and provides the lowest cost, unlike Throughput Optimized HDD.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://aws.amazon.com/ebs/details/">https://aws.amazon.com/ebs/details/</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html"> https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>', 'relatedLectureIds': '', 'question': '<p>You are working as an IT consultant for a major telecommunications company. They have an application using an Oracle database deployed in a large EC2 instance which is used for their infrequently accessed data. In this scenario, what is the most cost-effective storage type for the EC2 instance that hosts the database?</p>', 'answers': ['<p>EBS General Purpose SSD</p>', '<p>Provisioned IOPS SSD</p>', '<p>Throughput Optimized HDD</p>', '<p>Cold HDD</p>']}, 'correct_response': ['d'], 'original_assessment_id': 2566814, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are working as an IT consultant for a major telecommunications company. They have an application using an Oracle database deployed in a large EC2 instance which is used for their infrequently accessed data. In this scenario, what is the most cost-effective storage type for the EC2 instance that hosts the database?', 'id': 10442416, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>In this scenario, the best option is to use Amazon S3. It&rsquo;s a simple storage service that offers a highly-scalable, reliable, and low-latency data storage infrastructure at very low costs.</p> <p>Options 1 and 4 are incorrect because these services do not provide durable storage.</p> <p>Option 2 is incorrect because Amazon Glacier is mainly used for data archives with data retrieval times that can take some few hours. Hence, it is not suitable for the transcription service where the data are stored and frequently accessed.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/s3/faqs/">https://aws.amazon.com/s3/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You are building a transcription service for a company in which a fleet of EC2 worker instances process an uploaded audio file and generate a text file as an output. You must store both of these files in the same durable storage until the text file is retrieved by the uploader. Due to an expected surge in demand, you have to ensure that the storage is scalable and can be retrieved within minutes. </p><p>Which storage option in AWS can you use in this situation, which is both cost-efficient and scalable?</p>', 'answers': ['Multiple Amazon EBS volume with snapshots', 'A single Amazon Glacier vault', 'A single Amazon S3 bucket', 'Multiple instance stores']}, 'correct_response': ['c'], 'original_assessment_id': 2566816, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are building a transcription service for a company in which a fleet of EC2 worker instances process an uploaded audio file and generate a text file as an output. You must store both of these files in the same durable storage until the text file is retrieved by the uploader. Due to an expected surge in demand, you have to ensure that the storage is scalable and can be retrieved within minutes. Which storage option in AWS can you use in this situation, which is both cost-efficient and scalable?', 'id': 10442418, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed cloud database and supports both document and key-value store models. Its flexible data model, reliable performance, and automatic scaling of throughput capacity makes it a great fit for mobile, web, gaming, ad tech, IoT, and many other applications.</p> <p>Option 1 is incorrect because RDS is a relational database while DynamoDB is non-relational.</p> <p>Option 3 is incorrect because Oracle RDS itself is a relational database.</p> <p>Option 4 is incorrect because it is used for large scale data warehouse service for use with business intelligence tools.</p> <p>The following diagram shows an example of data stored as key-value pairs in DynamoDB:</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-10-23_05-24-29-74b3e6dadc8ce683ccd2a5bd00f99889.png" /></p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/dynamodb/">https://aws.amazon.com/dynamodb/</a></p> <p><a href="https://aws.amazon.com/nosql/key-value/">https://aws.amazon.com/nosql/key-value/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>As an AWS Cloud Consultant working for a record company, you are building an application that will store both key-value store and document models like band ID, album ID, song ID and composer ID.\xa0 \xa0</p><p>Which AWS service will suit your needs for your application?</p>', 'answers': ['AWS RDS', 'DynamoDB', 'Oracle RDS', 'Elastic Map Reduce']}, 'correct_response': ['b'], 'original_assessment_id': 2566812, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'As an AWS Cloud Consultant working for a record company, you are building an application that will store both key-value store and document models like band ID, album ID, song ID and composer ID.\xa0 \xa0Which AWS service will suit your needs for your application?', 'id': 10442414, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudWatch', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Memory Usage is a metric not available by default in CloudWatch. You need to add a custom metric for it to work.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a></p>', 'relatedLectureIds': '', 'question': 'The operations team of your company asked you for a way to monitor the health of your production EC2 instances in AWS. You told them to use the CloudWatch service.<br><br>Which of the following metrics is not available by default in CloudWatch?', 'answers': ['CPU Usage', 'Memory Usage', 'Disk Read operations', 'Network In and Out']}, 'correct_response': ['b'], 'original_assessment_id': 2566808, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'The operations team of your company asked you for a way to monitor the health of your production EC2 instances in AWS. You told them to use the CloudWatch service.Which of the following metrics is not available by default in CloudWatch?', 'id': 10442410, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Lambda', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>If you\'re using the AWS Lambda compute platform, you must choose one of the following deployment configuration types to specify how traffic is shifted from the original AWS Lambda function version to the new AWS Lambda function version:</p> <div> <ul> <li><strong>-Canary</strong>: Traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.</li> <li><strong>-Linear</strong>: Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment.</li> <li><strong>-All-at-once</strong>: All traffic is shifted from the original Lambda function to the updated Lambda function version at once.</li> </ul> </div> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#blue-green-lambda-compute-type">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#blue-green-lambda-compute-type</a></p> <p>&nbsp;</p> <p><strong>Check out these AWS Lambda and CodeDeploy Cheat Sheets:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/">https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/</a></span></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-codedeploy/">https://tutorialsdojo.com/aws-cheat-sheet-aws-codedeploy/</a></span></p>', 'relatedLectureIds': '', 'question': '<p>A startup company has a serverless architecture that uses AWS Lambda, API Gateway, and DynamoDB. They received an urgent feature request from their client last month and now, it is ready to be pushed to production. The company is using AWS CodeDeploy as their deployment service. </p><p>Which of the following configuration types will allow you to specify the percentage of traffic shifted to your updated Lambda function version before the remaining traffic is shifted in the second increment?</p>', 'answers': ['<p>Canary</p>', '<p>Linear</p>', '<p>All-at-once</p>', '<p>Blue/Green deployment</p>']}, 'correct_response': ['a'], 'original_assessment_id': 2566804, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'A startup company has a serverless architecture that uses AWS Lambda, API Gateway, and DynamoDB. They received an urgent feature request from their client last month and now, it is ready to be pushed to production. The company is using AWS CodeDeploy as their deployment service. Which of the following configuration types will allow you to specify the percentage of traffic shifted to your updated Lambda function version before the remaining traffic is shifted in the second increment?', 'id': 10442406, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>In this scenario, you have two VPCs which have peering connections with each other. Note that a VPC peering connection does not support edge to edge routing. This means that if either VPC in a peering relationship has one of the following connections, you cannot extend the peering relationship to that connection:</p> <div class="itemizedlist"> <ul> <li>- A VPN connection or an AWS Direct Connect connection to a corporate network</li> <li>- An Internet connection through an Internet gateway</li> <li>- An Internet connection in a private subnet through a NAT device</li> <li>- A VPC endpoint to an AWS service; for example, an endpoint to Amazon S3.</li> <li>- (IPv6) A ClassicLink connection. You can enable IPv4 communication between a linked EC2-Classic instance and instances in a VPC on the other side of a VPC peering connection. However, IPv6 is not supported in EC2-Classic, so you cannot extend this connection for IPv6 communication.</li> </ul> <p>&nbsp;</p> </div> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/peering/images/edge-to-edge-vpn-diagram.png" alt="" width="421" height="208" />&nbsp;</p> <p>For example, if VPC A and VPC B are peered, and VPC A has any of these connections, then instances in VPC B cannot use the connection to access resources on the other side of the connection. Similarly, resources on the other side of a connection cannot use the connection to access VPC B.</p> <p>Hence, this means that you cannot use VPC-2 to extend the peering relationship that exists between VPC-1 and the on-premises network. For example, traffic from the corporate network can\'t directly access VPC-1 by using the VPN connection or the AWS Direct Connect connection to VPC-2, which is why Options 1, 3, and 4 are incorrect.</p> <p>The correct answers are options 2 and 5. You can do the following to provide a highly available, fault-tolerant network connection:</p> <ul> <li>- Establish a hardware VPN over the Internet between the VPC and the on-premises network.</li> <li>- Establish another AWS Direct Connect connection and private virtual interface in the same AWS region.&nbsp;</li> </ul> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html#edge-to-edge-vgw">https://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html#edge-to-edge-vgw</a></p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/" target="_blank" rel="noopener">https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/</a></p> <p><a href="https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/" target="_blank" rel="noopener">https://aws.amazon.com/answers/networking/aws-multiple-data-center-ha-network-connectivity/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>A media company has two VPCs: VPC-1 and VPC-2 with peering connection between each other. VPC-1 only contains private subnets while VPC-2 only contains public subnets. The company uses a single AWS Direct Connect connection and a virtual interface to connect their on-premises network with VPC-1. <br><br>Which of the following options increase the fault tolerance of the connection to VPC-1? (Select all that applies.)</p>', 'answers': ['<p>Use the AWS VPN CloudHub to create a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2.</p>', 'Establish a hardware VPN over the Internet between VPC-1 and the on-premises network.', 'Establish a hardware VPN over the Internet between VPC-2 and the on-premises network.', 'Establish a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2.', 'Establish another AWS Direct Connect connection and private virtual interface in the same AWS region as VPC-1.']}, 'correct_response': ['b', 'e'], 'original_assessment_id': 2566820, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'A media company has two VPCs: VPC-1 and VPC-2 with peering connection between each other. VPC-1 only contains private subnets while VPC-2 only contains public subnets. The company uses a single AWS Direct Connect connection and a virtual interface to connect their on-premises network with VPC-1. Which of the following options increase the fault tolerance of the connection to VPC-1? (Select all that applies.)', 'id': 10442420, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'RDS', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Amazon RDS detects and automatically recovers from the most common failure scenarios for Multi-AZ deployments so that you can resume database operations as quickly as possible without administrative intervention.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/con-multi-AZ.png" /></p> <p>&nbsp;</p> <p>Amazon RDS automatically performs a failover in the event of any of the following:</p> <ol> <li>Loss of availability in primary Availability Zone</li> <li>Loss of network connectivity to primary</li> <li>Compute unit failure on primary</li> <li>Storage failure on primary</li> </ol> <p>&nbsp;</p> <p>Options 3, 4 and 5 are incorrect because all these scenarios do not affect the primary database. Automatic failover only occurs if the primary database is the one that is affected.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/rds/details/multi-az/">https://aws.amazon.com/rds/details/multi-az/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>An online events registration system is hosted in AWS and uses ECS to host its front-end tier and a Multi-AZ RDS for its database tier, which also has a standby replica. What are the events that will make Amazon RDS automatically perform a failover to the standby replica? (Choose 2)</p>', 'answers': ['Loss of availability in primary Availability Zone', '<p>Storage failure on primary</p>', '<p>Storage failure on secondary DB instance</p>', '<p>In the event of Read Replica failure</p>', '<p>Compute unit failure on secondary DB instance</p>']}, 'correct_response': ['a', 'b'], 'original_assessment_id': 2566822, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'An online events registration system is hosted in AWS and uses ECS to host its front-end tier and a Multi-AZ RDS for its database tier, which also has a standby replica. What are the events that will make Amazon RDS automatically perform a failover to the standby replica? (Choose 2)', 'id': 10442422, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'CloudWatch', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Using Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.</p> <p>Options 2, 3 and 4 are incorrect because it is unnecessary to go through such lengths when CloudWatch Alarms already has such a feature for you, offered at a low cost.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>One of your EC2 instances is reporting an unhealthy system status check. The operations team is looking for an easier way to monitor and repair these instances instead of fixing them manually. <br><br>How will you automate the monitoring and repair of the system status check failure in an AWS environment?</p>', 'answers': ['<p>Create CloudWatch alarms that stop and start the instance based on status check alarms.</p>', 'Write a python script that queries the EC2 API for each instance status check', 'Write a shell script that periodically shuts down and starts instances based on certain stats.', 'Buy and implement a third party monitoring tool.']}, 'correct_response': ['a'], 'original_assessment_id': 2566826, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'One of your EC2 instances is reporting an unhealthy system status check. The operations team is looking for an easier way to monitor and repair these instances instead of fixing them manually. How will you automate the monitoring and repair of the system status check failure in an AWS environment?', 'id': 10442426, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Instance metadata is the data about your instance that you can use to configure or manage the running instance. You can get the instance ID, public keys, public IP address and many other information from the instance metadata by firing a URL command in your instance to this URL:</p> <p style="text-align: left;"><a href="http://169.254.169.254/latest/meta-data/">http://169.254.169.254/latest/meta-data/</a></p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/IAM/latest/UserGuide/images/roles-usingrole-ec2roleinstance.png" /></p> <p>Option 1 is incorrect because the instance user data is mainly used to perform common automated configuration tasks and run scripts after the instance starts.</p> <p>Option 2 is incorrect because resource tags are labels that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define.</p> <p>Option 4 is incorrect because Amazon Machine Image (AMI) mainly provides the information required to launch an instance, which is a virtual server in the cloud.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.htm">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.htm</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>As a Network Architect developing a food ordering application, you need to retrieve the instance ID, public keys, and public IP address of the EC2 server you made for tagging and grouping the attributes into your internal application running on-premises. </p><p>Which EC2 feature will help you achieve your requirements?</p>', 'answers': ['Instance user data', 'Resource tags', 'Instance metadata', 'Amazon Machine Image']}, 'correct_response': ['c'], 'original_assessment_id': 2566824, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'As a Network Architect developing a food ordering application, you need to retrieve the instance ID, public keys, and public IP address of the EC2 server you made for tagging and grouping the attributes into your internal application running on-premises. Which EC2 feature will help you achieve your requirements?', 'id': 10442424, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>AWS Identity and Access Management (IAM) is a web service for securely controlling access to AWS services. With IAM, you can centrally manage users, security credentials such as access keys, and permissions that control which AWS resources users and applications can access.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/IAM/latest/UserGuide/images/intro-diagram%20_policies_800.png" alt="" width="700" height="620" /></p> <p>&nbsp;</p> <p>Option 1 is correct because an IAM group is a collection of IAM users. Groups let you specify permissions for multiple users, which can make it easier to manage the permissions for those users.</p> <p>Option 2 is correct as you can manage identity providers using IAM Dashboard instead of creating IAM users in your AWS account. With an identity provider (IdP), you can manage your user identities outside of AWS and give these external user identities permission to use AWS resources in your account.</p> <p>Option 3 is incorrect because cost allocation reports are under AWS Billing and Cost Management.</p> <p>Option 4 is incorrect because security groups can be managed in the EC2 console and not in the IAM dashboard.</p> <p>Option 5 is incorrect because&nbsp;Network ACL is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets while security groups act as virtual firewall for your instance to control inbound and outbound traffic, both of which cannot be managed in the IAM dashboard.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p> <p>&nbsp;</p> <p><strong>Here is a short video tutorial on IAM Roles:</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/wY7FOFaPNuE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>', 'relatedLectureIds': '', 'question': "<p>You are working for a central bank as the Principal AWS Solutions Architect. Due to compliance requirements and security concerns, you are tasked to implement strict access to the central bank's AWS resources using the AWS Identity and Access Management service.\xa0 </p><p>Which of the following can you manage in the IAM dashboard? (Choose 2)</p>", 'answers': ['<p>Groups</p>', 'Identity providers', 'Cost Allocation Reports', '<p>Security Groups</p>', '<p>Network Access Control List</p>']}, 'correct_response': ['a', 'b'], 'original_assessment_id': 2566830, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': "You are working for a central bank as the Principal AWS Solutions Architect. Due to compliance requirements and security concerns, you are tasked to implement strict access to the central bank's AWS resources using the AWS Identity and Access Management service.\xa0 Which of the following can you manage in the IAM dashboard? (Choose 2)", 'id': 10442428, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'S3', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Amazon S3 Standard - Infrequent Access (Standard - IA) is an Amazon S3 storage class for data that is accessed less frequently, but requires rapid access when needed. Standard - IA offers the high durability, throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee.</p> <p>This combination of low cost and high performance make Standard - IA ideal for long-term storage, backups, and as a data store for disaster recovery. The Standard - IA storage class is set at the object level and can exist in the same bucket as Standard, allowing you to use lifecycle policies to automatically transition objects between storage classes without any application changes.</p> <p><strong>Key Features:</strong></p> <ul> <li>- Same low latency and high throughput performance of Standard</li> <li>- Designed for durability of 99.999999999% of objects</li> <li>- Designed for 99.9% availability over a given year</li> <li>- Backed with the Amazon S3 Service Level Agreement for availability</li> <li>- Supports SSL encryption of data in transit and at rest</li> <li>- Lifecycle management for automatic migration of objects</li> </ul> <p>&nbsp;</p> <p>Option 2 is incorrect as it actually refers to Amazon S3 - Reduced Redundancy Storage (RRS). In addition, RRS will be completely deprecated soon and AWS recommends to use S3 IA One-Zone instead.&nbsp;</p> <p>Option 4 is incorrect as it should be "low latency" and "high throughput" instead. S3 automatically scales performance to meet user demands.</p> <p>Option 5 is incorrect because this statement refers to Amazon Glacier. Glacier is a secure, durable, and extremely low-cost cloud storage service for data archiving and long-term backup.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/s3/storage-classes/">https://aws.amazon.com/s3/storage-classes/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>Your fellow AWS Engineer has created a new Standard-class S3 bucket to store financial reports that are not frequently accessed but should be immediately available when an auditor requests for it. To save costs, you changed the storage class of the S3 bucket from Standard to Infrequent Access storage class.\xa0 \xa0</p><p>In Amazon S3 Standard - Infrequent Access storage class, which of the following statements are true? (Choose 2)</p>', 'answers': ['<p>It is designed for data that is accessed less frequently.</p>', '<p>It is the best storage option to store noncritical and reproducible data</p>', '<p>It is designed for data that requires rapid access when needed.</p>', '<p>It provides high latency and low throughput performance</p>', '<p>Ideal to use for data archiving.</p>']}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2566714, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'Your fellow AWS Engineer has created a new Standard-class S3 bucket to store financial reports that are not frequently accessed but should be immediately available when an auditor requests for it. To save costs, you changed the storage class of the S3 bucket from Standard to Infrequent Access storage class.\xa0 \xa0In Amazon S3 Standard - Infrequent Access storage class, which of the following statements are true? (Choose 2)', 'id': 10442322, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'VPC', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>The best way to implement a bastion host is to create a small EC2 instance which should only have a security group from a particular IP address for maximum security. We use a small instance rather than a large one because this host will only act as a jump server to connect to other instances in your VPC and nothing else. Hence, there is no point of allocating a large instance simply because it doesn\'t need that much computing power to process SSH (port 22) or RDP (port 3389) connections. Hence, option 4 is the right answer for this scenario.&nbsp;</p> <p>Options 1 and 3 are incorrect because even though&nbsp;you have your own pre-configured password, the SSH connection can still be accessed by anyone over the Internet, which poses as a security vulnerability.</p> <p>Option 2 is incorrect because you don\'t need a large instance for a bastion host as it does not require much CPU resources.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html">https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html</a></p> <p><a href="https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/">https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>', 'relatedLectureIds': '', 'question': 'You are instructed by your manager to set up a bastion host to your Amazon VPC and that you should be the only person that can access it via SSH. What is the best way for you to achieve this?', 'answers': ['Create a large EC2 instance with a security group which only allows access on port 22 using your own pre-configured password.', 'Create a large EC2 instance with a security group which only allows access on port 22 via your IP address.', 'Create a small EC2 instance with a security group which only allows access on port 22 using your own pre-configured password.', 'Create a small EC2 instance and a security group which only allows access on port 22 via your IP address.']}, 'correct_response': ['d'], 'original_assessment_id': 2566716, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are instructed by your manager to set up a bastion host to your Amazon VPC and that you should be the only person that can access it via SSH. What is the best way for you to achieve this?', 'id': 10442324, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>In this scenario, the correct answers are:&nbsp;&nbsp;</p> <ul> <li>- Enable Multi-Factor Authentication</li> <li>- Assign an IAM role to the Amazon EC2 instance</li> </ul> <p>&nbsp;</p> <p>Always remember that you should associate IAM roles to EC2 instances and not an IAM user, for the purpose of accessing other AWS services. IAM roles are designed so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://dmhnzl5mp9mj6.cloudfront.net/security_awsblog/images/MFAAPI4.png" alt="" width="650" height="365" /></p> <p>&nbsp;</p> <p>AWS Multi-Factor Authentication (MFA) is a simple best practice that adds an extra layer of protection on top of your user name and password. With MFA enabled, when a user signs in to an AWS website, they will be prompted for their user name and password (the first factor&mdash;what they know), as well as for an authentication code from their AWS MFA device (the second factor&mdash;what they have). Taken together, these multiple factors provide increased security for your AWS account settings and resources. You can enable MFA for your AWS account and for individual IAM users you have created under your account. MFA can also be used to control access to AWS service APIs.</p> <p>Option 3 is incorrect because storing AWS access keys in an EC2 instance is not recommended by AWS, as it can be compromised. Instead of storing access keys on an EC2 instance for use by applications that run on the instance and make AWS API requests, you can use an IAM role to provide temporary access keys for these applications.</p> <p>Option 4 is incorrect because there is no need to create an IAM user for this scenario since IAM roles already provide greater flexibility and easier management.</p> <p>Option 5 is incorrect because ACM is just a service that lets you easily provision, manage, and deploy public and private SSL/TLS certificates for use with AWS services and your internal connected resources. It is not used as a secure storage for your access keys.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/iam/details/mfa/">https://aws.amazon.com/iam/details/mfa/</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</a></span></p> <p>&nbsp;</p> <p><strong>Here\'s a short video tutorial on how to enable MFA for your AWS user account:</strong></p> <p><iframe src="https://www.youtube.com/embed/A3AObXBJ4Lw" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>', 'relatedLectureIds': '', 'question': '<p>You are building a cloud infrastructure where you have EC2 instances that require access to various AWS services such as S3 and Redshift. You will also need to provision access to system administrators so they can deploy and test their changes. </p><p>Which configuration should be used to ensure<strong> </strong>that the access to your resources are secured and not compromised? (Choose 2)</p>', 'answers': ['Enable Multi-Factor Authentication.', 'Assign an IAM role to the Amazon EC2 instance.', 'Store the AWS Access Keys in the EC2 instance.', 'Assign an IAM user for each Amazon EC2 Instance.', '<p>Store the AWS Access Keys in ACM.</p>']}, 'correct_response': ['a', 'b'], 'original_assessment_id': 2566718, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are building a cloud infrastructure where you have EC2 instances that require access to various AWS services such as S3 and Redshift. You will also need to provision access to system administrators so they can deploy and test their changes. Which configuration should be used to ensure that the access to your resources are secured and not compromised? (Choose 2)', 'id': 10442326, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EC2', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>To SSH into your EC2 instance via the Internet, you need to ensure that your VPC has an attached Internet Gateway, so that your instance can reach the Internet. Your instance should also have either a public IP or Elastic IP address, depending on whether you need a persistent IP address or not.&nbsp;Also ensure that you have configured your security groups to allow SSH inbound.</p> <p>You don\'t need a Secondary Private IP Address since this address is only used when communicating within your VPC and thus, one Private IP address is enough. Hence, Option 1 is correct.</p> <p>To enable access to or from the Internet for instances in a VPC subnet, you must do the following:</p> <div> <p style="padding-left: 30px;">- Attach an internet gateway to your VPC.</p> <p style="padding-left: 30px;">- Ensure that your subnet\'s route table points to the Internet gateway.</p> <p style="padding-left: 30px;">- Ensure that instances in your subnet have a globally unique IP address (public IPv4 address, Elastic IP address, or IPv6 address).</p> <p style="padding-left: 30px;">- Ensure that your network access control lists and security groups allow the relevant traffic to flow to and from your instance.</p> </div> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/secondary-private-ip-address/">https://aws.amazon.com/premiumsupport/knowledge-center/secondary-private-ip-address/</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html/">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>', 'relatedLectureIds': '', 'question': 'You want to establish an SSH connection to a Linux instance hosted in your VPC via the Internet. Which of the following is not required in order for this to work?', 'answers': ['<p>Secondary Private IP Address</p>', '<p>Public IP Address or Elastic IP</p>', 'Internet Gateway', '<p>Network access control and security group rules which allow the relevant traffic to flow to and from your EC2 instance.</p>']}, 'correct_response': ['a'], 'original_assessment_id': 2566720, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You want to establish an SSH connection to a Linux instance hosted in your VPC via the Internet. Which of the following is not required in order for this to work?', 'id': 10442328, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IoT Core', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>AWS IoT Core is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core provides secure communication and data processing across different kinds of connected devices and locations so you can easily build IoT applications.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/IoT/diagrams/AWS%20IoT%20Core%20-%20Process%20and%20Act.2b1f03813fbd3b4416e45c096336497f22954520.png" width="550" height="207" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because CloudFormation is mainly used for creating and managing the architecture and not for handling connected devices. You have to use AWS IoT Core instead.</p> <p>Option 2 is incorrect because AWS Elastic Beanstalk is mainly used as a substitute to Infrastructure-as-a-Service with Platform-as-a-Service, which reduces management complexity without restricting choice or control and not for handling connected devices.</p> <p>Option 4 is incorrect because Amazon Elastic Container Services is mainly used for creating and managing docker instances and not for handling devices.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://aws.amazon.com/iot-core/">https://aws.amazon.com/iot-core/</a></p> <p><a href="https://aws.amazon.com/iot/">https://aws.amazon.com/iot/</a></p>', 'relatedLectureIds': '', 'question': '<p>You are working as a Cloud Consultant for a government agency with a mandate of improving traffic planning, maintenance of roadways and preventing accidents. There is a need to manage traffic infrastructure in real time, alert traffic engineers and emergency response teams when problems are detected, and automatically change traffic signals to get emergency personnel to accident scenes faster by using sensors and smart devices.\xa0 \xa0</p><p>Which AWS service will allow the developers of the agency to connect the said devices to your cloud-based applications?</p>', 'answers': ['CloudFormation', 'Elastic Beanstalk', 'AWS IoT Core', 'Container service']}, 'correct_response': ['c'], 'original_assessment_id': 2566790, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are working as a Cloud Consultant for a government agency with a mandate of improving traffic planning, maintenance of roadways and preventing accidents. There is a need to manage traffic infrastructure in real time, alert traffic engineers and emergency response teams when problems are detected, and automatically change traffic signals to get emergency personnel to accident scenes faster by using sensors and smart devices.\xa0 \xa0Which AWS service will allow the developers of the agency to connect the said devices to your cloud-based applications?', 'id': 10442394, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<div> <p>Remember that since the instance is using a RAID configuration, the snapshot process is different. You should stop all I/O activity of the volumes before creating a snapshot. Hence, option 2 is correct:</p> <ol> <li>Stop all applications from writing to the RAID array.</li> <li>Flush all caches to the disk.</li> <li>Confirm that the associated EC2 instance is no longer writing to the RAID array by taking actions such as freezing the file system, unmounting the RAID array, or even shutting down the EC2 instance.</li> <li>After taking steps to halt all disk-related activity to the RAID array, take a snapshot of each EBS volume in the array.</li> </ol> <p>&nbsp;</p> <div> <div> <p>When you take a snapshot of an attached Amazon EBS volume that is in use, the snapshot excludes data cached by applications or the operating system. For a single EBS volume, this is often not a problem. However, when cached data is excluded from snapshots of multiple EBS volumes in a RAID array, restoring the volumes from the snapshots can degrade the integrity of the array.</p> <p>When creating snapshots of EBS volumes that are configured in a RAID array, it is critical that there is no data I/O to or from the volumes when the snapshots are created. RAID arrays introduce data interdependencies and a level of complexity not present in a single EBS volume configuration.</p> </div> </div> <p>Option 1 is incorrect as you don\'t need to detach the volumes in the first place.</p> <p>Option 3 is incorrect as you don\'t need to create a new image of the instance.</p> <p>Option 4&nbsp;is incorrect because there are missing steps in the process. You have to flush all caches to the disk first and you have to ensure that the EC2 instance is no longer writing to the RAID Array.</p> <div> <p>&nbsp;</p> </div> </div> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/snapshot-ebs-raid-array/">https://aws.amazon.com/premiumsupport/knowledge-center/snapshot-ebs-raid-array/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>', 'relatedLectureIds': '', 'question': '<p>You need to back up your mySQL database hosted on a Reserved EC2 instance. It is using EBS volumes that are configured in a RAID array.\xa0 </p><p>What steps will you take to minimize the time during which the database cannot be written to and to ensure a consistent backup?</p>', 'answers': ['<p>1. Detach EBS volumes from the EC2 instance.</p><p><br></p><p>2. Start EBS snapshot of volumes.</p><p><br></p><p>3. Re-attach the EBS volumes.</p>', '<p>1. Stop all applications from writing to the RAID array.</p><p><br></p><p>2. Flush all caches to the disk.</p><p><br></p><p>3. Confirm that the associated EC2 instance is no longer writing to the RAID array by taking actions such as freezing the file system, unmounting the RAID array, or even shutting down the EC2 instance.</p><p><br></p><p>4. After taking steps to halt all disk-related activity to the RAID array, take a snapshot of each EBS volume in the array.</p>', '<p>1. Stop all I/O activity in the volumes.</p><p><br></p><p>2. Create an image of the EC2 Instance.</p><p><br></p><p>3. Resume all I/O activity in the volume.</p>', '<p>1. Stop all I/O activity in the volumes.</p><p><br></p><p>2. Start EBS snapshot of volumes.</p><p><br></p><p>3. While the snapshot is in progress, resume all I/O activity.</p>']}, 'correct_response': ['b'], 'original_assessment_id': 2566722, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You need to back up your mySQL database hosted on a Reserved EC2 instance. It is using EBS volumes that are configured in a RAID array.\xa0 What steps will you take to minimize the time during which the database cannot be written to and to ensure a consistent backup?', 'id': 10442330, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>The best practice in handling API Credentials is to create a new role in the Identity Access Management (IAM) service and then assign it to a specific EC2 instance. In this way, you have a secure and centralized way of storing and managing your credentials.</p> <p>Options 2, 3, and 4 are incorrect because it is not secure to store nor use the API credentials from an EC2 instance. You should use IAM service instead.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You are developing a meal planning application that provides meal recommendations for the week as well as the food consumption of your users. Your application resides on an EC2 instance which requires access to various AWS services for its day-to-day operations.\xa0 \xa0</p><p>Which of the following is the best way to allow your EC2 instance to access your S3 bucket and other AWS services?</p>', 'answers': ['Create a role in IAM and assign it to the EC2 instance.', 'Store the API credentials in the EC2 instance.', 'Add the API Credentials in the Security Group and assign it to the EC2 instance.', 'Store the API credentials in a bastion host.']}, 'correct_response': ['a'], 'original_assessment_id': 2566838, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are developing a meal planning application that provides meal recommendations for the week as well as the food consumption of your users. Your application resides on an EC2 instance which requires access to various AWS services for its day-to-day operations.\xa0 \xa0Which of the following is the best way to allow your EC2 instance to access your S3 bucket and other AWS services?', 'id': 10442436, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>One of the best practices in Amazon IAM is to <em>grant least privilege</em>.</p> <p>When you create IAM policies, follow the standard security advice of granting&nbsp;<em>least privilege</em>&mdash;that is, granting only the permissions required to perform a task. Determine what users need to do and then craft policies for them that let the users perform&nbsp;<em>only</em>&nbsp;those tasks. Therefore, option 1 is the correct answer.</p> <p>Start with a minimum set of permissions and grant additional permissions as necessary.</p> <p>Defining the right set of permissions requires some understanding of the user\'s objectives. Determine what is required for the specific task, what actions a particular service supports, and what permissions are required in order to perform those actions.</p> <p>Option 2 is incorrect, since you don\'t want your users to gain access to everything and perform unnecessary actions. Doing so is not a good security practice.</p> <p>Option 3 is incorrect because&nbsp;granting only the least number of people with full root access is not the correct definition of what the principle of least privilege is.</p> <p>Option 4 is incorrect as well since there are some users who you should not give administrative access to. You should follow the principle of least privilege when providing permissions and accesses to your resources.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-groups-for-permissions">https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-groups-for-permissions</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p> <p>&nbsp;</p> <p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-service-control-policies-scp-vs-iam-policies/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-service-control-policies-scp-vs-iam-policies/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>Your company just recently adopted a hybrid architecture that integrates their on-premises data center to their AWS cloud. You are assigned to configure the VPC as well as to implement the required IAM users, IAM roles, IAM groups and IAM policies. </p><p>In this scenario, what is a best practice when creating IAM policies?</p>', 'answers': ['Use the principle of least privilege which means granting only the permissions required to perform a task.', 'Grant all permissions to any EC2 user.', 'Use the principle of least privilege which means granting only the least number of people with full root access.', 'Determine what users need to do and then craft policies for them that let the users perform those tasks including additional administrative operations.']}, 'correct_response': ['a'], 'original_assessment_id': 2566724, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'Your company just recently adopted a hybrid architecture that integrates their on-premises data center to their AWS cloud. You are assigned to configure the VPC as well as to implement the required IAM users, IAM roles, IAM groups and IAM policies. In this scenario, what is a best practice when creating IAM policies?', 'id': 10442332, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Storage Gateway', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>All data transferred between any type of gateway appliance and AWS storage is encrypted using SSL. By default, all data stored by AWS Storage Gateway in S3 is encrypted server-side with Amazon S3-Managed Encryption Keys (SSE-S3). Also, when using the file gateway, you can optionally configure each file share to have your objects encrypted with AWS KMS-Managed Keys using SSE-KMS. This is the reason why Option 1 is correct.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/storagegateway/latest/userguide/images/aws-storage-gateway-cached-diagram.png" alt="" width="600" height="329" /></p> <p>&nbsp;</p> <p>Data stored in Amazon Glacier is protected by default; only vault owners have access to the Amazon Glacier resources they create. Amazon Glacier encrypts your data at rest by default and supports secure data transit with SSL. This is the reason why Option 4 is correct.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://media.amazonwebservices.com/blog/2014/s3_sse_customer_key_2.png" alt="" width="650" height="167" /></p> <p>&nbsp;</p> <p>Options 2, 3 and 5 are incorrect because although Amazon RDS, ECS and Lambda all support encryption, you still have to enable and configure them first with tools like AWS KMS to encrypt the data at rest.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/storagegateway/faqs/" target="_blank" rel="noopener">https://aws.amazon.com/storagegateway/faqs/</a>&nbsp;</p> <p><a href="https://aws.amazon.com/glacier/features">https://aws.amazon.com/glacier/features</a></p>', 'relatedLectureIds': '', 'question': '<p>You are working as a Solutions Architect for a start-up company that has a not-for-profit crowdfunding platform hosted in AWS. Their platform allows people around the globe to raise money for social enterprise projects including challenging circumstances like accidents and illnesses. Since the system handles financial transactions, you have to ensure that your cloud architecture is secure. </p><p>Which of the following AWS services encrypts data at rest by default? (Choose 2)</p>', 'answers': ['AWS Storage Gateway', 'Amazon RDS', '<p>Amazon ECS</p>', 'Amazon Glacier', '<p>AWS Lambda</p>']}, 'correct_response': ['a', 'd'], 'original_assessment_id': 2566726, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are working as a Solutions Architect for a start-up company that has a not-for-profit crowdfunding platform hosted in AWS. Their platform allows people around the globe to raise money for social enterprise projects including challenging circumstances like accidents and illnesses. Since the system handles financial transactions, you have to ensure that your cloud architecture is secure. Which of the following AWS services encrypts data at rest by default? (Choose 2)', 'id': 10442334, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'SQS', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>In this scenario, it is best to create 2 separate SQS queues for each type of members. The SQS queues for the premium members can be polled first by the EC2 Instances and once completed, the messages from the free members can be processed next.</p> <p>Option 1 is incorrect as you cannot set a priority to individual items in the SQS queue.</p> <p>Option 3 is incorrect as Amazon Kinesis is used to process streaming data and it is not applicable in this scenario.</p> <p>Option 4 is incorrect as Amazon S3 is used for durable storage and not for processing&nbsp;data.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You run a website which accepts high-quality photos and turns them into a downloadable video montage. The website offers a free account and a premium account that guarantees faster processing. All requests by both free and premium members go through a single SQS queue and then processed by a group of EC2 instances which generate the videos. You need to ensure that the premium users who paid for the service have higher priority than your free members.\xa0 \xa0</p><p>How do you re-design your architecture to address this requirement?</p>', 'answers': ['For the requests made by premium members, set a higher priority in the SQS queue so it will be processed first compared to the requests made by free members.', "Create an SQS queue for free members and another one for premium members. Configure your EC2 instances to consume messages from the premium queue first and if it is empty, poll from the free members' SQS queue.", 'Use Amazon Kinesis to process the photos and generate the video montage in real time.', '<p>Use Amazon S3 to store and process the photos and then generate the video montage afterwards.</p>']}, 'correct_response': ['b'], 'original_assessment_id': 2566732, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You run a website which accepts high-quality photos and turns them into a downloadable video montage. The website offers a free account and a premium account that guarantees faster processing. All requests by both free and premium members go through a single SQS queue and then processed by a group of EC2 instances which generate the videos. You need to ensure that the premium users who paid for the service have higher priority than your free members.\xa0 \xa0How do you re-design your architecture to address this requirement?', 'id': 10442340, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Perfect Forward Secrecy is a feature that provides additional safeguards against the eavesdropping of encrypted data, through the use of a unique random session key. This prevents the decoding of captured data, even if the secret long-term key is compromised.</p> <p>CloudFront and Elastic Load Balancing are the two AWS services that support&nbsp;Perfect Forward Secrecy. Hence, Option 3 is correct.</p> <p>Options 1, 2, and 4 are incorrect since these services do not use Perfect Forward Secrecy. SSL/TLS is commonly used when you have sensitive data travelling through the public network.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/about-aws/whats-new/2014/02/19/elastic-load-balancing-perfect-forward-secrecy-and-more-new-security-features/">https://aws.amazon.com/about-aws/whats-new/2014/02/19/elastic-load-balancing-perfect-forward-secrecy-and-more-new-security-features/</a></p> <p><a href="https://d1.awsstatic.com/whitepapers/Security/Secure_content_delivery_with_CloudFront_whitepaper.pdf">https://d1.awsstatic.com/whitepapers/Security/Secure_content_delivery_with_CloudFront_whitepaper.pdf</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>In Elastic Load Balancing, there are various security features that you can use such as Server Order Preference, Predefined Security Policy, Perfect Forward Secrecy and many others. Perfect Forward Secrecy is a feature that provides additional safeguards against the eavesdropping of encrypted data through the use of a unique random session key. This prevents the decoding of captured data, even if the secret long-term key is compromised.\xa0 \xa0</p><p>Perfect Forward Secrecy is used to offer SSL/TLS cipher suites for which two AWS services?</p>', 'answers': ['EC2 and S3', 'CloudTrail and CloudWatch', '<p>CloudFront and Elastic Load Balancing</p>', '<p>Trusted Advisor and GovCloud</p>']}, 'correct_response': ['c'], 'original_assessment_id': 2566734, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'In Elastic Load Balancing, there are various security features that you can use such as Server Order Preference, Predefined Security Policy, Perfect Forward Secrecy and many others. Perfect Forward Secrecy is a feature that provides additional safeguards against the eavesdropping of encrypted data through the use of a unique random session key. This prevents the decoding of captured data, even if the secret long-term key is compromised.\xa0 \xa0Perfect Forward Secrecy is used to offer SSL/TLS cipher suites for which two AWS services?', 'id': 10442342, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<div> <p>In this scenario, there are 4 EC2 instances that belong to the same security group that should be able to connect to the Internet. The main route table is properly configured but there is a problem connecting to one instance. Since the other three instances are working fine, we can assume that the security group and the route table are correctly configured. One possible reason for this issue is that the problematic instance does not have a public or an EIP address, hence, the correct answer is Option 4.</p> <p>Option 1 is incorrect because the other three instances, which are associated with the same route table and security group, do not have any issues.&nbsp;</p> <p>Option 2 is incorrect because there is no relationship between the Availability Zone and the Internet Gateway (IGW) that may have caused the issue.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> </div> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>One member of your DevOps team consulted you about a problem in connecting to one of the EC2 instances of your VPC over the Internet. Your environment is set up with four EC2 instances that all belong to a public subnet. The EC2 instances also belong to the same security group. Everything works well as expected except for one of the EC2 instances which is not able to send nor receive traffic over the Internet like the other three instances. </p><p>What could be the possible reason for this issue?</p>', 'answers': ['The route table is not properly configured to allow traffic to and from the Internet through the Internet gateway.', 'The EC2 instance is running in an Availability Zone that is not connected to an Internet gateway.', 'The EC2 instance does not have a private IP address associated with it.', 'The EC2 instance does not have a public IP address associated with it.']}, 'correct_response': ['d'], 'original_assessment_id': 2566738, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'One member of your DevOps team consulted you about a problem in connecting to one of the EC2 instances of your VPC over the Internet. Your environment is set up with four EC2 instances that all belong to a public subnet. The EC2 instances also belong to the same security group. Everything works well as expected except for one of the EC2 instances which is not able to send nor receive traffic over the Internet like the other three instances. What could be the possible reason for this issue?', 'id': 10442346, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Kinesis', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>A new shard iterator is returned by every&nbsp;<strong>GetRecords</strong>&nbsp;request (as&nbsp;<code class="code">NextShardIterator</code>), which you then use in the next&nbsp;<strong>GetRecords</strong>&nbsp;request (as&nbsp;<code class="code">ShardIterator</code>). Typically, this shard iterator does not expire before you use it. However, you may find that shard iterators expire because you have not called&nbsp;<strong>GetRecords</strong>&nbsp;for more than 5 minutes, or because you\'ve performed a restart of your consumer application.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png" width="750" /></p> <p>&nbsp;</p> <p>If the shard iterator expires immediately before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. This situation is more likely to happen if you have a large number of shards. To solve this problem, increase the write capacity assigned to the shard table. Hence, Option 1 is correct.&nbsp;</p> <p>Option 2 is incorrect because DynamoDB is a fully managed service which automatically scales its storage, without setting it up manually. The scenario refers to the <strong>write&nbsp;capacity</strong> of the shard table when it says that the DynamoDB table used by Kinesis does not have enough <em>capacity</em> to store the lease data.</p> <p>Option 3 is incorrect because&nbsp;the DAX feature is primarily used for read performance improvement of your DynamoDB table from <em>milliseconds</em> response time to <em>microseconds</em>. It does not have any relationship with Amazon Kinesis Data Stream in this scenario.</p> <p>Option 4 is incorrect because although&nbsp;Amazon Kinesis Data Analytics can support a data analytics application, it is still not a suitable solution for this issue. You simply need to increase the write capacity assigned to the shard table in order to rectify the problem which is why switching to Amazon Kinesis Data Analytics is not necessary.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-ddb.html">https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-ddb.html</a></p> <p><a href="https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html">https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon Kinesis Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You have a data analytics application that updates a real-time, foreign exchange dashboard and another separate application that archives data to Amazon Redshift. Both applications are configured to consume data from the same stream concurrently and independently by using Amazon Kinesis Data Streams. However, you noticed that there are a lot of occurrences where a shard iterator expires unexpectedly. Upon checking, you found out that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data.\xa0 \xa0</p><p>Which of the following is the most suitable solution to rectify this issue?</p>', 'answers': ['<p>Increase the write capacity assigned to the shard table.</p>', '<p>Upgrade the storage capacity of the DynamoDB table.</p>', '<p>Enable In-Memory Acceleration with DynamoDB Accelerator (DAX). </p>', '<p>Use Amazon Kinesis Data Analytics to properly support the data analytics application instead of Kinesis Data Stream.</p>']}, 'correct_response': ['a'], 'original_assessment_id': 2566740, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You have a data analytics application that updates a real-time, foreign exchange dashboard and another separate application that archives data to Amazon Redshift. Both applications are configured to consume data from the same stream concurrently and independently by using Amazon Kinesis Data Streams. However, you noticed that there are a lot of occurrences where a shard iterator expires unexpectedly. Upon checking, you found out that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data.\xa0 \xa0Which of the following is the most suitable solution to rectify this issue?', 'id': 10442348, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>First, the Network ACL should be properly set to allow communication between the two subnets. The security group should also be properly configured so that your web server can communicate with the database server. Hence, options 1 and 4 are the correct answers:</p> <ol> <li>Check if all security groups are set to allow the application host to communicate to the database on the right port and protocol.</li> <li>Check the Network ACL if it allows communication between the two subnets.</li> </ol> <p>&nbsp;</p> <p>Option 2 is incorrect because the EC2 instances do not need to be of the same class in order to communicate with each other.</p> <p>Option 3 is incorrect because an Internet gateway is primarily used to communicate to the Internet.</p> <p>Option 5 is incorrect because Placement Group is mainly used to provide low-latency network performance necessary for tightly-coupled node-to-node communication.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>', 'relatedLectureIds': '', 'question': 'You have two On-Demand EC2 instances inside your Virtual Private Cloud in the same Availability Zone but are deployed to different subnets. One EC2 instance is running a database and the other EC2 instance a web application that connects with the database. You want to ensure that these two instances can communicate with each other for your system to work properly.<br><br>What are the things you have to check so that these EC2 instances can communicate inside the VPC? (Choose 2)', 'answers': ['Check the Network ACL if it allows communication between the two subnets.', 'Check if both instances are the same instance class.', 'Check if the default route is set to a NAT instance or Internet Gateway (IGW) for them to communicate.', 'Check if all security groups are set to allow the application host to communicate to the database on the right port and protocol.', '<p>Ensure that the EC2 instances are in the same Placement Group.</p>']}, 'correct_response': ['a', 'd'], 'original_assessment_id': 2566742, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You have two On-Demand EC2 instances inside your Virtual Private Cloud in the same Availability Zone but are deployed to different subnets. One EC2 instance is running a database and the other EC2 instance a web application that connects with the database. You want to ensure that these two instances can communicate with each other for your system to work properly.What are the things you have to check so that these EC2 instances can communicate inside the VPC? (Choose 2)', 'id': 10442350, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'SQS', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>In this scenario, it is stated that the SQS queue is configured with the maximum message retention period. The maximum message retention in SQS is 14 days that is why option 3 is the correct answer i.e. there will be no missing messages.&nbsp;</p> <p>Options 1 and 2 are incorrect as there are no missing messages in the queue thus, there is no need to resubmit any previous requests.</p> <p>Option 4 is incorrect as the queue can contain an unlimited number of messages, not just 10,000 messages.</p> <p>In Amazon SQS, you can configure the message retention period to a value from 1 minute to 14 days. The default is 4 days. Once the message retention limit is reached, your messages are automatically deleted.</p> <p>A single Amazon SQS message queue can contain an unlimited number of messages. However, there is a 120,000 limit for the number of inflight messages for a standard queue and 20,000 for a FIFO queue. Messages are inflight after they have been received from the queue by a consuming component, but have not yet been deleted from the queue.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/sqs/">https://aws.amazon.com/sqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You developed a web application and deployed it on a fleet of EC2 instances, which is using Amazon SQS. The requests are saved as messages in the SQS queue which is configured with the maximum message retention period.\xa0 However, after thirteen days of operation, the web application suddenly crashed and there are 10,000 unprocessed messages that are still waiting in the queue. Since you developed the application, you can easily resolve the issue but you need to send a communication to the users on the issue.\xa0 </p><p>What information will you provide and what will happen to the unprocessed messages?</p>', 'answers': ['Tell the users that unfortunately, they have to resubmit all the requests again.', 'Tell the users that the application will be operational shortly however, requests sent over three days ago will need to be resubmitted.', 'Tell the users that the application will be operational shortly and all received requests will be processed after the web application is restarted.', 'Tell the users that unfortunately, they have to resubmit all of the requests since the queue would not be able to process the 10,000 messages together.']}, 'correct_response': ['c'], 'original_assessment_id': 2566744, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You developed a web application and deployed it on a fleet of EC2 instances, which is using Amazon SQS. The requests are saved as messages in the SQS queue which is configured with the maximum message retention period.\xa0 However, after thirteen days of operation, the web application suddenly crashed and there are 10,000 unprocessed messages that are still waiting in the queue. Since you developed the application, you can easily resolve the issue but you need to send a communication to the users on the issue.\xa0 What information will you provide and what will happen to the unprocessed messages?', 'id': 10442352, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Snowball', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Amazon Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud. Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns. Transferring data with Snowball is simple, fast, secure, and can be as little as one-fifth the cost of high-speed Internet. Hence, Option 4 is the correct answer.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/snowball/latest/ug/images/Snowball-opening-600w.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect since uploading it directly would take too long to finish.</p> <p>Option 2 is incorrect since provisioning a line for Direct Connect would take too much time, and might not give you the fastest data transfer solution.</p> <p>Option 3 is incorrect since Data Pipeline is a web service that makes it easy to schedule <strong>regular data movement</strong> and data processing activities in the AWS cloud. You only want a data transfer solution that is reliable, secure and fast while saving on costs.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/snowball/">https://aws.amazon.com/snowball/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Snowball Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-snowball/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-snowball/</span></a>&nbsp;</p> <p>&nbsp;</p> <p><strong>S3 Transfer Acceleration vs Direct Connect vs VPN vs Snowball vs Snowmobile:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/</span></a></p>', 'relatedLectureIds': '', 'question': 'You have started your new role as a Solutions Architect for a media company. They host large volumes of data for their operations which are about 250 TB in size on their internal servers. They have decided to store this data on S3 because of its durability and redundancy. The company currently has a 100 Mbps dedicated line connecting their head office to the Internet. <br><br>What is the fastest way to import all this data to Amazon S3?', 'answers': ['Upload it directly to S3', 'Use AWS Direct connect and transfer the data over to S3.', 'Upload the files using AWS Data pipeline.', 'Use AWS Snowball to upload the files.']}, 'correct_response': ['d'], 'original_assessment_id': 2566746, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You have started your new role as a Solutions Architect for a media company. They host large volumes of data for their operations which are about 250 TB in size on their internal servers. They have decided to store this data on S3 because of its durability and redundancy. The company currently has a 100 Mbps dedicated line connecting their head office to the Internet. What is the fastest way to import all this data to Amazon S3?', 'id': 10442354, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'AWS Shared Responsibility Model', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Security and Compliance is a shared responsibility between AWS and the customer. This shared model can help relieve customer&rsquo;s operational burden as AWS operates, manages and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the service operates. The customer assumes responsibility and management of the guest operating system (including updates and security patches), other associated application software as well as the configuration of the AWS provided security group firewall.</p> <p>Customers should carefully consider the services they choose as their responsibilities vary depending on the services used, the integration of those services into their IT environment, and applicable laws and regulations. The nature of this shared responsibility also provides the flexibility and customer control that permits the deployment. This differentiation of responsibility is commonly referred to as Security &ldquo;<strong>of</strong>&rdquo; the Cloud versus Security &ldquo;<strong>in</strong>&rdquo; the Cloud.</p> <p>The shared responsibility model for infrastructure services, such as Amazon Elastic Compute Cloud (Amazon EC2) for example, specifies that AWS manages the security of the following assets:</p> <ul> <li>-Facilities</li> <li>-Physical security of hardware</li> <li>-Network infrastructure</li> <li>-Virtualization infrastructure&nbsp;</li> </ul> <p>&nbsp;</p> <p>You as the customer are responsible for the security of the following assets:</p> <ul> <li>-Amazon Machine Images (AMIs)</li> <li>-Operating systems</li> <li>-Applications</li> <li>-Data in transit</li> <li>-Data at rest</li> <li>-Data stores</li> <li>-Credentials</li> <li>-Policies and configuration</li> </ul> <p>&nbsp;</p> <p>For a better understanding about this topic, refer to the&nbsp;AWS Security Best Practices whitepaper on the reference link below and also the Shared Responsibility Model diagram:</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/security-center/Shared_Responsibility_Model_V2.59d1eccec334b366627e9295b304202faf7b899b.jpg" alt="" width="650" height="356" /></p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://d0.awsstatic.com/whitepapers/aws-security-best-practices.pdf">https://d0.awsstatic.com/whitepapers/aws-security-best-practices.pdf</a></p> <p><a href="https://aws.amazon.com/compliance/shared-responsibility-model/">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p>', 'relatedLectureIds': '', 'question': '<p>You are working as a Solutions Architect for a leading commercial bank which has recently adopted a hybrid cloud architecture. You have to ensure that the required data security is in place on all of their AWS resources to meet the strict financial regulatory requirements.\xa0 \xa0</p><p>In the AWS Shared Responsibility Model, which security aspects are the responsibilities of the customer? (Choose 2)</p>', 'answers': ['<p>Managing the underlying network infrastructure</p>', 'Physical security of hardware', 'OS Patching of an EC2 instance', 'IAM Policies and Credentials Management', 'Virtualization infrastructure']}, 'correct_response': ['c', 'd'], 'original_assessment_id': 2566730, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are working as a Solutions Architect for a leading commercial bank which has recently adopted a hybrid cloud architecture. You have to ensure that the required data security is in place on all of their AWS resources to meet the strict financial regulatory requirements.\xa0 \xa0In the AWS Shared Responsibility Model, which security aspects are the responsibilities of the customer? (Choose 2)', 'id': 10442338, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EBS', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Option 4 is correct. You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are&nbsp;<em>incremental</em>&nbsp;backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/snapshot_1b.png" alt="" width="650" height="502" /></p> <p>&nbsp;</p> <p>This minimizes the time required to create the snapshot and saves on storage costs by not duplicating data. When you delete a snapshot, only the data unique to that snapshot is removed. Each snapshot contains all of the information needed to restore your data (from the moment the snapshot was taken) to a new EBS volume.</p> <p>Option 1 is incorrect since running an EBS-backed EC2 instance does not relate to your problem as you are already running a few of them in the first place.</p> <p>Option 2 is incorrect. Disk mirroring is not an efficient and cost-optimized solution for your problem. You should use EBS snapshots instead.</p> <p>Option 3 is incorrect. A placement group is a logical grouping of instances within a single Availability Zone (AZ) that allows low-latency communication between instances. Hence, this is not an efficient way to back up data.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>', 'relatedLectureIds': '', 'question': '<p>A corporate and investment bank has recently decided to adopt a hybrid cloud architecture for their Trade Finance web application which uses an Oracle database with Oracle Real Application Clusters (RAC) configuration. Since Oracle RAC is not supported in RDS, they decided to launch their database in a large On-Demand EC2 instance instead, with multiple EBS Volumes attached. As a Solutions Architect, you are responsible to ensure the security, availability, scalability, and disaster recovery of the whole architecture.  </p><p>In this scenario, which of the following will enable you to take backups of your EBS volumes that are being used by the Oracle database?</p>', 'answers': ['<p>EBS-backed EC2 instances.</p>', '<p>Use Disk Mirroring, which is also known as RAID 1, that replicates data to two or more disks/EBS Volumes.</p>', '<p>Launch the EBS Volumes to a Placement Group which will automatically back up your data.</p>', '<p>Create snapshots of the EBS Volumes.</p>']}, 'correct_response': ['d'], 'original_assessment_id': 2566748, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'A corporate and investment bank has recently decided to adopt a hybrid cloud architecture for their Trade Finance web application which uses an Oracle database with Oracle Real Application Clusters (RAC) configuration. Since Oracle RAC is not supported in RDS, they decided to launch their database in a large On-Demand EC2 instance instead, with multiple EBS Volumes attached. As a Solutions Architect, you are responsible to ensure the security, availability, scalability, and disaster recovery of the whole architecture.  In this scenario, which of the following will enable you to take backups of your EBS volumes that are being used by the Oracle database?', 'id': 10442356, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>An Amazon EBS volume is a durable, block-level storage device that you can attach to a single EC2 instance. You can use EBS volumes as primary storage for data that requires frequent updates, such as the system drive for an instance or storage for a database application. You can also use them for throughput-intensive applications that perform continuous disk scans. EBS volumes persist independently from the running life of an EC2 instance.</p> <p>Here is a list of important information about EBS Volumes:</p> <ul> <li>-When you create an EBS volume in an Availability Zone, it is automatically replicated within that zone to prevent data loss due to a failure of any single hardware component.</li> <li>-An EBS volume can only be attached to one EC2 instance at a time.</li> <li>-After you create a volume, you can attach it to any EC2 instance in the same Availability Zone</li> <li>-An EBS volume is off-instance storage that can persist independently from the life of an instance. You can specify not to terminate the EBS volume when you terminate the EC2 instance during instance creation.</li> <li>-EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions.</li> <li>-Amazon EBS encryption uses 256-bit Advanced Encryption Standard algorithms (AES-256)</li> <li>-EBS Volumes offer 99.999% SLA.</li> </ul> <p>&nbsp;</p> <p>Option 1 is incorrect because&nbsp;when you create an EBS volume in an Availability Zone, it is automatically replicated within that zone only, and not on a separate AWS region, to prevent data loss due to a failure of any single hardware component.</p> <p>Option 2 is incorrect as EBS volumes can only be attached to an EC2 instance in the same Availability Zone.</p> <p>Option 5 is almost correct. But instead of storing the volume to Amazon RDS, the EBS Volume snapshots are actually sent to Amazon S3.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html</a></p> <p><a href="https://aws.amazon.com/ebs/features/">https://aws.amazon.com/ebs/features/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p> <p>&nbsp;</p> <p><strong>Here is a short video tutorial on EBS:</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/ljYH5lHQdxo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>', 'relatedLectureIds': '', 'question': '<p>You are working as a Solutions Architect for an investment bank and your Chief Technical Officer intends to migrate all of your applications to AWS. You are looking for block storage to store all of your data and have decided to go with EBS volumes. Your boss is worried that EBS volumes are not appropriate for your workloads due to compliance requirements, downtime scenarios, and IOPS performance.\xa0 \xa0</p><p>Which of the following are valid points in proving that EBS is the best service to use for your migration? (Select all that applies) </p>', 'answers': ['<p>When you create an EBS volume in an Availability Zone, it is automatically replicated on a separate AWS region to prevent data loss due to a failure of any single hardware component.</p>', 'EBS volumes can be attached to any EC2 Instance in any Availability Zone.', 'An EBS volume is off-instance storage that can persist independently from the life of an instance.', 'EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions.', '<p>Amazon EBS provides the ability to create snapshots (backups) of any EBS volume and write a copy of the data in the volume to Amazon RDS, where it is stored redundantly in multiple Availability Zones </p>']}, 'correct_response': ['c', 'd'], 'original_assessment_id': 2566784, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are working as a Solutions Architect for an investment bank and your Chief Technical Officer intends to migrate all of your applications to AWS. You are looking for block storage to store all of your data and have decided to go with EBS volumes. Your boss is worried that EBS volumes are not appropriate for your workloads due to compliance requirements, downtime scenarios, and IOPS performance.\xa0 \xa0Which of the following are valid points in proving that EBS is the best service to use for your migration? (Select all that applies)', 'id': 10442388, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'CloudWatch', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Options 2 and 5 are correct. In this requirement, you can use Amazon CloudWatch to monitor the database and then Amazon SNS to send the emails to the Operations team. Take note that you should use SNS instead of SES (Simple Email Service) when you want to monitor your EC2 instances.</p> <p>CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications, and services that run on AWS, and on-premises servers.</p> <p>SNS is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.</p> <p>Option 1 is incorrect. SES is a cloud-based email sending service designed to send notification&nbsp;and transactional emails.</p> <p>Option 3 is incorrect. SQS is a fully-managed message queuing service. It does not monitor applications nor send email notifications unlike SES.</p> <p>Option 4 is incorrect. Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It does not monitor applications nor send email notifications.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/cloudwatch/">https://aws.amazon.com/cloudwatch/</a></p> <p><a href="https://aws.amazon.com/sns/">https://aws.amazon.com/sns/</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>Your company has a top priority requirement to monitor a few database metrics and then afterwards, send email notifications to the Operations team in case there is an issue. Which AWS services can accomplish this requirement? (Choose 2)</p>', 'answers': ['Amazon Simple Email Service', 'Amazon CloudWatch', 'Amazon Simple Queue Service (SQS)', 'Amazon Route 53', 'Amazon Simple Notification Service (SNS)']}, 'correct_response': ['b', 'e'], 'original_assessment_id': 2566750, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'Your company has a top priority requirement to monitor a few database metrics and then afterwards, send email notifications to the Operations team in case there is an issue. Which AWS services can accomplish this requirement? (Choose 2)', 'id': 10442358, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'API Gateway', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. With a few clicks in the AWS Management Console, you can create an API that acts as a &ldquo;front door&rdquo; for applications to access data, business logic, or functionality from your back-end services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, or any web application. Since it can use AWS Lambda, you can run your APIs without servers.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/serverless/Serverless%20Migration/Serverlesswebapp.45052e1feb8f1748d96a678311d73434599095b1.png" alt="" width="706" height="392" /></p> <p>Amazon API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management. Amazon API Gateway has no minimum fees or startup costs. You pay only for the API calls you receive and the amount of data transferred out.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/api-gateway/">https://aws.amazon.com/api-gateway/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon API Gateway Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-api-gateway/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-api-gateway/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You are a Solutions Architect of a multi-national gaming company which develops video games for PS4, Xbox One and Nintendo Switch consoles, plus a number of mobile games for Android and iOS. Due to the wide range of their products and services, you proposed that they use API Gateway.\xa0 \xa0</p><p>What are the key features of API Gateway that you can tell your client? (Choose 2) </p>', 'answers': ['<p>It automatically provides a query language for your APIs similar to GraphQL.</p>', '<p>You can run your APIs with quantum computer servers.</p>', '<p>You can run your APIs without any servers.</p>', '<p>Provides durable data\xa0storage</p>', '<p>You pay only for the API calls you receive and the amount of data transferred out.</p>']}, 'correct_response': ['c', 'e'], 'original_assessment_id': 2566752, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are a Solutions Architect of a multi-national gaming company which develops video games for PS4, Xbox One and Nintendo Switch consoles, plus a number of mobile games for Android and iOS. Due to the wide range of their products and services, you proposed that they use API Gateway.\xa0 \xa0What are the key features of API Gateway that you can tell your client? (Choose 2)', 'id': 10442360, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'CloudFront', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>SNI Custom SSL relies on the SNI extension of the Transport Layer Security protocol, which allows multiple domains to serve SSL traffic over the same IP address by including the hostname which the viewers are trying to connect to.</p> <p>Amazon CloudFront delivers your content from each edge location and offers the same security as the Dedicated IP Custom SSL feature. SNI Custom SSL works with most modern browsers, including Chrome version 6 and later (running on Windows XP and later or OS X 10.5.7 and later), Safari version 3 and later (running on Windows Vista and later or Mac OS X 10.5.6. and later), Firefox 2.0 and later, and Internet Explorer 7 and later (running on Windows Vista and later).</p> <p>&nbsp;<img style="display: block; margin-left: auto; margin-right: auto;" src="https://d2908q01vomqb2.cloudfront.net/22d200f8670dbdb3e253a90eee5098477c95c23d/2016/10/17/image12_new_a.png" alt="" width="750" height="415" /></p> <p>&nbsp;</p> <p>Some users may not be able to access your content because some older browsers do not support SNI and will not be able to establish a connection with CloudFront to load the HTTPS version of your content. If you need to support non-SNI compliant browsers for HTTPS content, it is recommended to use the Dedicated IP Custom SSL feature.</p> <p>Option 1 is incorrect because&nbsp;a Classic Load Balancer does not support&nbsp;Server Name Indication (SNI). You have to use an Application Load Balancer instead or a CloudFront web distribution to allow the SNI feature.</p> <p>Option 3 is incorrect because just like Option 1,&nbsp;a Classic Load Balancer does not support&nbsp;Server Name Indication (SNI) and the use of an Elastic IP is not a suitable solution to allow multiple domains to serve SSL traffic. You have to use&nbsp;Server Name Indication (SNI).</p> <p>Option 4 is incorrect because AWS does support&nbsp;the use of Server Name Indication (SNI).</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/about-aws/whats-new/2014/03/05/amazon-cloudront-announces-sni-custom-ssl/">https://aws.amazon.com/about-aws/whats-new/2014/03/05/amazon-cloudront-announces-sni-custom-ssl/</a></p> <p><a href="https://aws.amazon.com/blogs/security/how-to-help-achieve-mobile-app-transport-security-compliance-by-using-amazon-cloudfront-and-aws-certificate-manager/">https://aws.amazon.com/blogs/security/how-to-help-achieve-mobile-app-transport-security-compliance-by-using-amazon-cloudfront-and-aws-certificate-manager/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</span></a></p> <p>&nbsp;</p> <p><strong>SNI Custom SSL vs Dedicated IP Custom SSL:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-sni-custom-ssl-vs-dedicated-ip-custom-ssl/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-sni-custom-ssl-vs-dedicated-ip-custom-ssl/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>A web application, which is used by your clients around the world, is hosted in an Auto Scaling group of EC2 instances behind a Classic Load Balancer. You need to secure your application by allowing multiple domains to serve SSL traffic over the same IP address. </p><p>Which of the following should you do to meet the above requirement?</p>', 'answers': ['<p>Use Server Name Indication (SNI) on your Classic Load Balancer by adding multiple SSL certificates to allow multiple domains to serve SSL traffic.</p>', '<p>Generate an SSL certificate with AWS Certificate Manager and create a CloudFront web distribution. Associate the certificate with your web distribution and enable the support for Server Name Indication (SNI).</p>', '<p>Use an Elastic IP and upload multiple 3rd party certificates in your Classic Load Balancer using the AWS Certificate Manager.</p>', '<p>It is not possible to allow multiple domains to serve SSL traffic over the same IP address in AWS</p>']}, 'correct_response': ['b'], 'original_assessment_id': 2566754, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'A web application, which is used by your clients around the world, is hosted in an Auto Scaling group of EC2 instances behind a Classic Load Balancer. You need to secure your application by allowing multiple domains to serve SSL traffic over the same IP address. Which of the following should you do to meet the above requirement?', 'id': 10442362, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Route53', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Here are the prerequisites for routing traffic to a website that is hosted in an Amazon S3 Bucket:</p> <p style="padding-left: 30px;">- An S3 bucket that is configured to host a static website. The bucket must have the same name as your domain or subdomain. For example, if you want to use the subdomain portal.tutorialsdojo.com, the name of the bucket must be portal.tutorialsdojo.com.</p> <p style="padding-left: 30px;">- A registered domain name. You can use Route 53 as your domain registrar, or you can use a different registrar.</p> <p style="padding-left: 30px;">- Route 53 as the DNS service for the domain. If you register your domain name by using Route 53, we automatically configure Route 53 as the DNS service for the domain.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_01-03-54-a79dccacb816c6b4a8da3bd3ac9c2ce6.png" /></p> <p>&nbsp;</p> <p>Option 3 is incorrect since an MX record specifies the mail server responsible for accepting email messages on behalf of a domain name. This is not what is being asked by the question.</p> <p>Option 4 is incorrect. There is no constraint that the S3 bucket must be in the same region as the hosted zone, in order for the Route 53 service to route traffic into it.</p> <p>Option 5 is incorrect because you only need to enable Cross-Origin Resource Sharing (CORS) when your client web application on one domain interacts with the resources in a different domain.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You have a static corporate website hosted in a standard S3 bucket and a new web domain name which was registered using Route 53. You are instructed by your manager to integrate these two services in order to successfully launch their corporate website. </p><p>What are the prerequisites when routing traffic using Amazon Route 53 to a website that is hosted in an Amazon S3 Bucket? (Choose 2)</p>', 'answers': ['The S3 bucket name must be the same as the domain name', 'A registered domain name', 'The record set must be of type "MX"', 'The S3 bucket must be in the same region as the hosted zone', '<p>The Cross-Origin Resource Sharing (CORS) option should be enabled in the S3 bucket</p>']}, 'correct_response': ['a', 'b'], 'original_assessment_id': 2566756, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You have a static corporate website hosted in a standard S3 bucket and a new web domain name which was registered using Route 53. You are instructed by your manager to integrate these two services in order to successfully launch their corporate website. What are the prerequisites when routing traffic using Amazon Route 53 to a website that is hosted in an Amazon S3 Bucket? (Choose 2)', 'id': 10442364, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'SQS', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Amazon Simple Queue Service (SQS) and&nbsp;Amazon Simple Workflow Service (SWF) are the services that you can use for creating a decoupled architecture in AWS.&nbsp;Decoupled architecture is a type of computing architecture that enables computing components or layers to execute independently while still interfacing with each other.</p> <p>Amazon SQS offers reliable, highly-scalable hosted queues for storing messages while they travel between applications or microservices. Amazon SQS lets you move data between distributed application components and helps you decouple these components. Amazon SWF is a web service that makes it easy to coordinate work across distributed application components.</p> <p>Options 2 and 5 are incorrect as RDS and DynamoDB are database services.</p> <p>Option 4 is incorrect because there is no such thing as Amazon Simple Decoupling Service.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/sqs/">https://aws.amazon.com/sqs/</a></p> <p><a href="http://docs.aws.amazon.com/amazonswf/latest/developerguide/swf-welcome.html">http://docs.aws.amazon.com/amazonswf/latest/developerguide/swf-welcome.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p> <p>&nbsp;</p> <p><strong>Amazon Simple Workflow (SWF) vs AWS Step Functions vs Amazon SQS:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-swf-vs-aws-step-functions-vs-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-swf-vs-aws-step-functions-vs-amazon-sqs/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>A software company has resources hosted in AWS and on-premises servers. You have been requested to create a decoupled architecture for applications which make use of both resources. <br><br>Which of the following options are valid? (Choose 2)</p>', 'answers': ['Use SWF to utilize both on-premises servers and EC2 instances for your decoupled application', 'Use RDS to utilize both on-premises servers and EC2 instances for your decoupled application', 'Use SQS to utilize both on-premises servers and EC2 instances for your decoupled application', 'Use Amazon Simple Decoupling Service to utilize both on-premises servers and EC2 instances for your decoupled application', 'Use DynamoDB to utilize both on-premises servers and EC2 instances for your decoupled application']}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2566760, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'A software company has resources hosted in AWS and on-premises servers. You have been requested to create a decoupled architecture for applications which make use of both resources. Which of the following options are valid? (Choose 2)', 'id': 10442366, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EC2', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>An elastic network interface (ENI) is a logical networking component in a VPC that represents a virtual network card. You can attach a network interface to an EC2 instance in the following ways:</p> <ol> <li>When it\'s running (hot attach)</li> <li>When it\'s stopped (warm attach)</li> <li>When the instance is being launched (cold attach).</li> </ol> <p>Therefore, option 1 is the correct answer.</p> <p>Option 2 is incorrect because this describes a "cold attach" scenario.</p> <p>Option 3 is incorrect because&nbsp;this describes a "hot attach" scenario.</p> <p>Option 4 is incorrect because there is no specific name for attaching an ENI to an idle EC2 instance.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#attach_eni_launch">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#attach_eni_launch</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>', 'relatedLectureIds': '', 'question': "<p>You are an AWS Network Engineer working for a utilities provider where you are managing a monolithic application with EC2 instance using a Windows AMI. You want to implement a cost-effective and highly available architecture for your application where you have an exact replica of the Windows server that is in a running state. If the primary instance terminates, you can attach the ENI to the standby secondary instance which allows the traffic flow to resume within a few seconds. </p><p>When it comes to the ENI attachment to an EC2 instance, what does 'warm attach' refer to?</p>", 'answers': ['Attaching an ENI to an instance when it is stopped.', 'Attaching an ENI to an instance during the launch process.', 'Attaching an ENI to an instance when it is running.', 'Attaching an ENI to an instance when it is idle.']}, 'correct_response': ['a'], 'original_assessment_id': 2566840, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': "You are an AWS Network Engineer working for a utilities provider where you are managing a monolithic application with EC2 instance using a Windows AMI. You want to implement a cost-effective and highly available architecture for your application where you have an exact replica of the Windows server that is in a running state. If the primary instance terminates, you can attach the ENI to the standby secondary instance which allows the traffic flow to resume within a few seconds. When it comes to the ENI attachment to an EC2 instance, what does 'warm attach' refer to?", 'id': 10442438, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SQS', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>In this scenario, the main culprit is that your application does not issue a delete command to the SQS queue after processing the message, which is why this message went back to the queue and was processed multiple times.</p> <p>Option 1 is incorrect as&nbsp;there is no sqsSendEmailMessage attribute in SQS.</p> <p>Option 2 is a valid answer but since the scenario did not mention that the EC2 instances deleted the processed messages, option 4 is a better answer than this option.</p> <p>Option 3 is incorrect as SQS does not automatically delete the messages.&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/sqs/faqs/">https://aws.amazon.com/sqs/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>', 'relatedLectureIds': '', 'question': 'You have built a web application that checks for new items in an S3 bucket once every hour. If new items exist, a message is added to an SQS queue. You have a fleet of EC2 instances which retrieve messages from the SQS queue, process the file, and finally, send you and the user an email confirmation that the item has been successfully processed. Your officemate uploaded one test file to the S3 bucket and after a couple of hours,  you noticed that you and your officemate have 50 emails from your application with the same message. <br><br>Which of the following is most likely the root cause why the application has sent you and the user multiple emails?', 'answers': ['The sqsSendEmailMessage attribute of the SQS queue is configured to 50.', 'There is a bug in the application.', 'By default, SQS automatically deletes the messages that were processed by the consumers. It might be possible that your officemate has submitted the request 50 times which is why you received a lot of emails.', 'Your application does not issue a delete command to the SQS queue after processing the message, which is why this message went back to the queue and was processed multiple times.']}, 'correct_response': ['d'], 'original_assessment_id': 2566762, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You have built a web application that checks for new items in an S3 bucket once every hour. If new items exist, a message is added to an SQS queue. You have a fleet of EC2 instances which retrieve messages from the SQS queue, process the file, and finally, send you and the user an email confirmation that the item has been successfully processed. Your officemate uploaded one test file to the S3 bucket and after a couple of hours,  you noticed that you and your officemate have 50 emails from your application with the same message. Which of the following is most likely the root cause why the application has sent you and the user multiple emails?', 'id': 10442368, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration identifying the events you want Amazon S3 to publish, and the destinations where you want Amazon S3 to send the event notifications.</p> <p>Amazon S3 supports the following destinations where it can publish events:<br /><br /><strong>Amazon Simple Notification Service (Amazon SNS) topic -&nbsp;</strong>A web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients.</p> <p><strong>Amazon Simple Queue Service (Amazon SQS) queue -&nbsp;</strong>Offers reliable and scalable hosted queues for storing messages as they travel between computer.</p> <p><strong>AWS Lambda -&nbsp;</strong>AWS Lambda is a compute service where you can upload your code and the service can run the code on your behalf using the AWS infrastructure. You package up and upload your custom code to AWS Lambda when you create a Lambda function</p> <p>Option 1 is incorrect because Amazon Kinesis is used to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information, and not used for event notifications. You have to use SNS, SQS or Lambda.</p> <p>Option 2 is incorrect because SES is mainly used for sending emails designed to help digital marketers and application developers send marketing, notification, and transactional emails, and not for sending event notifications from S3. You have to use SNS, SQS or Lambda.</p> <p>Option 5 is incorrect because SWF is mainly used to build applications that use Amazon\'s cloud to coordinate work across distributed components and not used as a way to trigger event notifications from S3. You have to use SNS, SQS or Lambda.</p> <p>Here&rsquo;s what you need to do in order to start using this new feature with your application:</p> <ol> <li>Create the queue, topic, or Lambda function (which I&rsquo;ll call the target for brevity) if necessary.</li> <li>Grant S3 permission to publish to the target or invoke the Lambda function. For SNS or SQS, you do this by applying an appropriate policy to the topic or the queue. For Lambda, you must create and supply an IAM role, then associate it with the Lambda function.</li> <li>Arrange for your application to be invoked in response to activity on the target. As you will see in a moment, you have several options here.</li> <li>Set the bucket&rsquo;s Notification Configuration to point to the target.</li> </ol> <p>&nbsp;</p> <p><img src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-10-23_07-00-07-8b41add7c02e86428c5743b6790ac876.png" /></p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>', 'relatedLectureIds': '', 'question': "<p>You are working for a litigation firm as the Data Engineer for their case history application. You need to keep track of all the cases your firm has handled. The static assets like .jpg, .png, and .pdf files are stored in S3 for cost efficiency and high durability. As these files are critical to your business, you want to keep track of what's happening in your S3 bucket. You found out that S3 has an event notification whenever a delete or write operation happens within the S3 bucket.\xa0 \xa0</p><p>What are the possible Event Notification destinations available for S3 buckets? (Choose 2)</p>", 'answers': ['<p>Kinesis</p>', 'SES', 'SQS', 'Lambda function', '<p>SWF</p>']}, 'correct_response': ['c', 'd'], 'original_assessment_id': 2566836, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': "You are working for a litigation firm as the Data Engineer for their case history application. You need to keep track of all the cases your firm has handled. The static assets like .jpg, .png, and .pdf files are stored in S3 for cost efficiency and high durability. As these files are critical to your business, you want to keep track of what's happening in your S3 bucket. You found out that S3 has an event notification whenever a delete or write operation happens within the S3 bucket.\xa0 \xa0What are the possible Event Notification destinations available for S3 buckets? (Choose 2)", 'id': 10442434, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Route53', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>For this scenario, Option 3 is correct. You can create a new Route 53 with the failover option to a static S3 website bucket or CloudFront distribution as an alternative.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_01-05-29-0096a577e991922c675f02801d48a8a4.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because running a duplicate system is not a cost-effective solution. Remember that you are trying to build a failover mechanism for your web app, not a distributed setup.</p> <p>Option 2 is incorrect because, although you can set up failover to your on-premises data center, you are not maximizing the AWS environment such as using Route 53 failover.</p> <p>Option 4 is incorrect because this is not the best way to handle a failover event. If you&nbsp;add more servers only in case the application fails, then there would be a period of downtime in which your application is unavailable. Since there are no running servers on that period,&nbsp;your application will be unavailable for a certain period of time until your new server is up and running.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/fail-over-s3-r53/" target="_blank" rel="noopener">https://aws.amazon.com/premiumsupport/knowledge-center/fail-over-s3-r53/</a></p> <p><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p>', 'relatedLectureIds': '', 'question': 'You have a new, dynamic web app written in MEAN stack that is going to be launched in the next month. There is a probability that the traffic will be quite high in the first couple of weeks. In the event of a load failure, how can you set up DNS failover to a static website?', 'answers': ['Duplicate the exact application architecture in another region and configure DNS weight-based routing.', '<p>Enable failover to an application hosted in an on-premises data center.</p>', '<p>Use Route 53 with the failover option to a static S3 website bucket or CloudFront distribution.</p>', 'Add more servers in case the application fails.']}, 'correct_response': ['c'], 'original_assessment_id': 2566764, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You have a new, dynamic web app written in MEAN stack that is going to be launched in the next month. There is a probability that the traffic will be quite high in the first couple of weeks. In the event of a load failure, how can you set up DNS failover to a static website?', 'id': 10442370, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>DynamoDB is a NoSQL database that supports key-value and document data structures.&nbsp;A key-value store is a database service that provides support for storing, querying, and updating collections of objects that are identified using a key and values that contain the actual content being stored. Meanwhile, a&nbsp;document data store provides support for storing, querying, and updating items in a document format such as JSON, XML, and HTML.</p> <p>Option 2 is correct because the DynamoDB Time-to-Live (TTL) mechanism enables you to manage web sessions of your application easily. It lets you set a specific timestamp to delete expired items from your tables. Once the timestamp expires, the corresponding item is marked as expired and is subsequently deleted from the table. By using this functionality, you do not have to track expired data and delete it manually. TTL can help you reduce storage usage and reduce the cost of storing data that is no longer relevant.&nbsp;</p> <p>Option 4 is correct because the Amazon DynamoDB stores structured data indexed by primary key&nbsp;and allow low latency read and write access to items ranging from 1 byte up to 400KB. Amazon S3 stores unstructured blobs and is suited for storing large objects up to 5 TB. In order to optimize your costs across AWS services, large objects or infrequently accessed data sets should be stored in Amazon S3, while smaller data elements or file pointers (possibly to Amazon S3 objects) are best saved in Amazon DynamoDB.</p> <p>To speed up access to relevant data, you can pair Amazon S3 with a search engine such as Amazon CloudSearch or a database such as Amazon DynamoDB or Amazon RDS. In these scenarios, Amazon S3 stores the actual information, and the search engine or database serves as the repository for associated metadata such as the object name, size, keywords, and so on. Metadata in the database can easily be indexed and queried, making it very efficient to locate an object&rsquo;s reference by using a search engine or a database query. This result can be used to pinpoint and retrieve the object itself from Amazon S3.&nbsp;</p> <p>Option 1 is incorrect since DynamoDB is a NoSQL database solution and not a relational database.</p> <p>Option 3 is incorrect because DynamoDB is not meant to store large amounts of infrequently accessed data, due to factors like sizing, scaling and cost. Amazon Glacier would be a better option for this scenario.</p> <p>Option 5 is incorrect because BLOB data is too large a chunk of data to be put into a NoSQL database such as DynamoDB.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/dynamodb/faqs/">https://aws.amazon.com/dynamodb/faqs/</a></p> <p><a href="https://d1.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf#page=9">https://d1.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf#page=9</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>As a Junior Software Engineer, you are developing a hotel reservations application and are given the task of improving the database aspect of the app. You found out that RDS does not satisfy the needs of your application because it does not scale as easily compared with DynamoDB. You need to demonstrate to your Senior Software Engineer the advantages of using DynamoDB over RDS.\xa0 \xa0</p><p>What are the valid use cases for Amazon DynamoDB? (Choose 2)</p>', 'answers': ['Running relational SQL joins and complex data updates.', 'Managing web sessions.', '<p>Storing large amounts of infrequently accessed data.</p>', 'Storing metadata for Amazon S3 objects.', 'Storing BLOB data.']}, 'correct_response': ['b', 'd'], 'original_assessment_id': 2566766, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'As a Junior Software Engineer, you are developing a hotel reservations application and are given the task of improving the database aspect of the app. You found out that RDS does not satisfy the needs of your application because it does not scale as easily compared with DynamoDB. You need to demonstrate to your Senior Software Engineer the advantages of using DynamoDB over RDS.\xa0 \xa0What are the valid use cases for Amazon DynamoDB? (Choose 2)', 'id': 10442372, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Snowball', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Although an AWS Snowball device costs less than AWS Snowball Edge, it cannot store 80 TB of data in one device. Take note that the storage capacity is different from the usable capacity for Snowball and Snowball Edge. Remember that an 80 TB Snowball appliance and 100 TB Snowball Edge appliance only have 72 TB and 83 TB of usable capacity respectively. Hence, it would be costly if you use two Snowball devices compared to using just one&nbsp;AWS Snowball Edge device.</p> <p>The AWS Snowball Edge is a type of Snowball device with on-board storage and compute power for select AWS capabilities. Snowball Edge can undertake local processing and edge-computing workloads in addition to transferring data between your local environment and the AWS Cloud.</p> <p>Each Snowball Edge device can transport data at speeds faster than the internet. This transport is done by shipping the data in the appliances through a regional carrier. The appliances are rugged shipping containers, complete with E Ink shipping labels. The AWS Snowball Edge device differs from the standard Snowball because it can bring the power of the AWS Cloud to your on-premises location, with local storage and compute functionality.</p> <p>Snowball Edge devices have three options for device configurations &ndash; storage optimized, compute optimized, and with GPU. When this guide refers to Snowball Edge devices, it\'s referring to all options of the device. Whenever specific information applies only to one or more optional configurations of devices, like how the Snowball Edge with GPU has an on-board GPU, it will be called out.</p> <p>&nbsp;</p> <p style="text-align: center;"><img src="https://docs.aws.amazon.com/snowball/latest/ug/images/SnowballEdgeAppliance.png" /></p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html">https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html</a>&nbsp;</p> <p><a href="https://docs.aws.amazon.com/snowball/latest/ug/device-differences.html">https://docs.aws.amazon.com/snowball/latest/ug/device-differences.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Snowball Edge Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-snowball-edge/">https://tutorialsdojo.com/aws-cheat-sheet-aws-snowball-edge/</a></span></p> <p>&nbsp;</p> <p><strong>Here is a quick introduction on AWS Snowball Edge:</strong></p> <p><iframe src="https://www.youtube.com/embed/bxSD1Nha2k8" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p> <p>&nbsp;</p> <p><strong>Using AWS Snowball Edge and AWS DMS for Database Migration:</strong></p> <p><iframe src="https://www.youtube.com/embed/6Hw--HE8ILg" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>', 'relatedLectureIds': '', 'question': '<p>You are working for a large telecommunications company. They have a requirement to move 83 TB data warehouse to the cloud. It would take 2 months to transfer the data given their current bandwidth allocation.\xa0 \xa0</p><p>Which is the most cost-effective service that would allow you to quickly upload their data into AWS?</p>', 'answers': ['Amazon Snowball', '<p>Amazon Snowball Edge</p>', '<p>Amazon Direct Connect</p>', '<p>Amazon S3 MultiPart Upload</p>']}, 'correct_response': ['b'], 'original_assessment_id': 2566768, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are working for a large telecommunications company. They have a requirement to move 83 TB data warehouse to the cloud. It would take 2 months to transfer the data given their current bandwidth allocation.\xa0 \xa0Which is the most cost-effective service that would allow you to quickly upload their data into AWS?', 'id': 10442374, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>AWS Security Token Service (AWS STS) is the service that you can use to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use.</p> <p>In this diagram, IAM user Alice in the Dev account (the role-assuming account) needs to access the Prod account (the role-owning account). Here&rsquo;s how it works:</p> <ol> <li>Alice in the Dev account assumes an IAM role (WriteAccess) in the Prod account by calling AssumeRole.</li> <li>STS returns a set of temporary security credentials.</li> <li>Alice uses the temporary security credentials to access services and resources in the Prod account. Alice could, for example, make calls to Amazon S3 and Amazon EC2, which are granted by the WriteAccess role.</li> </ol> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-10-23_06-52-31-201df4af92968773479c7a09268baf1e.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because the Amazon Cognito service is primarily used for user authentication and not for providing access to your AWS resources. A&nbsp;JSON Web Token (JWT) is meant to be used for user authentication and session management.</p> <p>Option 3 is incorrect because although the AWS SSO service uses STS, it does not issue short-lived credentials by itself.&nbsp;AWS Single Sign-On (SSO) is a cloud SSO service that makes it easy to centrally manage SSO access to multiple AWS accounts and business applications.</p> <p>Option 4 is incorrect as only STS has the ability to provide&nbsp;temporary security credentials.&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You work for an Intelligence Agency as its Principal Consultant developing a missile tracking application, which is hosted on both development and production AWS accounts. Alice, the Intelligence agency’s Junior Developer, only has access to the development account. She has received security clearance to access the agency’s production account but the access is only temporary and only write access to EC2 and S3 is allowed. </p><p>Which of the following allows you to issue short-lived access tokens that acts as temporary security credentials to allow access to your AWS resources?</p>', 'answers': ['<p>Use AWS Cognito to issue JSON Web Tokens (JWT)</p>', '<p>Use AWS STS</p>', '<p>Use AWS SSO</p>', '<p>All of the above.</p>']}, 'correct_response': ['b'], 'original_assessment_id': 2566834, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You work for an Intelligence Agency as its Principal Consultant developing a missile tracking application, which is hosted on both development and production AWS accounts. Alice, the Intelligence agency’s Junior Developer, only has access to the development account. She has received security clearance to access the agency’s production account but the access is only temporary and only write access to EC2 and S3 is allowed. Which of the following allows you to issue short-lived access tokens that acts as temporary security credentials to allow access to your AWS resources?', 'id': 10442432, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>You can use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation, retention, and deletion of snapshots taken to back up your Amazon EBS volumes. Automating snapshot management helps you to:</p> <ul> <li>-Protect valuable data by enforcing a regular backup schedule.</li> <li>-Retain backups as required by auditors or internal compliance.</li> <li>-Reduce storage costs by deleting outdated backups.</li> </ul> <p>&nbsp;</p> <p>Combined with the monitoring features of Amazon CloudWatch Events and AWS CloudTrail, Amazon DLM provides a complete backup solution for EBS volumes at no additional cost. Hence, Option 5 is the correct answer as it is the fastest and cost-effective solution in providing an automated way of backing up your EBS volumes.</p> <p>Option 1 is incorrect because even though this is a valid solution, you would still need additional time to create a scheduled job that calls the "create-snapshot" command. It would be better to use Amazon Data Lifecycle Manager (Amazon DLM) instead as this provides you the fastest solution which enables you&nbsp;to automate the creation, retention, and deletion of the EBS snapshots without having to write custom shell scripts or creating scheduled jobs.</p> <p>Option 2 is incorrect as the Amazon Storage Gateway is used only for creating a backup of data from your on-premises server and not from the Amazon Virtual Private Cloud.</p> <p>Option 3 is incorrect as there is no such thing as EBS-cycle policy in Amazon S3.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html</a></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>', 'relatedLectureIds': '', 'question': '<p>As part of the Business Continuity Plan of your company, your IT Director instructed you to set up an automated backup of all of the EBS Volumes for your EC2 instances as soon as possible.\xa0 </p><p>What is the fastest and most cost-effective solution to automatically back up all of your EBS Volumes?</p>', 'answers': ['<p>For an automated solution, create a scheduled job that calls the "create-snapshot" command via the AWS CLI to take a snapshot of production EBS volumes periodically.\xa0 </p>', '<p>Set your Amazon Storage Gateway with EBS volumes as the data source and store the backups in your on-premises servers through the storage gateway.</p>', '<p>Use an EBS-cycle policy in Amazon S3 to automatically back up the EBS volumes.</p>', '<p>Use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation of EBS snapshots.</p>']}, 'correct_response': ['d'], 'original_assessment_id': 2566770, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'As part of the Business Continuity Plan of your company, your IT Director instructed you to set up an automated backup of all of the EBS Volumes for your EC2 instances as soon as possible.\xa0 What is the fastest and most cost-effective solution to automatically back up all of your EBS Volumes?', 'id': 10442376, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Auto Scaling', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>In Auto Scaling, the following statements are correct regarding the cooldown period:</p> <ol> <li>It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect.</li> <li>Its default value is 300 seconds.</li> <li>It is a configurable setting for your Auto Scaling group.</li> </ol> <p>Options 1, 2, and 5 are incorrect as these statements are false in depicting what the word "cooldown" actually means for Auto Scaling. The cooldown period is a configurable setting for your Auto Scaling group that helps to ensure that it doesn\'t launch or terminate additional instances before the previous scaling activity takes effect. After the Auto Scaling group dynamically scales using a simple scaling policy, it waits for the cooldown period to complete before resuming scaling activities.</p> <p><span style="font-weight: 400;">The figure below demonstrates the scaling cooldown:</span></p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-10-23_05-13-47-8ff2ec72179862c346ba76ede3994182.png" /></p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/as-instance-termination.html">http://docs.aws.amazon.com/autoscaling/latest/userguide/as-instance-termination.html</a></p><p>&nbsp;</p> <p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You are working for a commercial bank as an AWS Infrastructure Engineer handling the forex trading application of the bank. You have an Auto Scaling group of EC2 instances that allow your company to cope up with the current demand of traffic and achieve cost-efficiency. You want the Auto Scaling group to behave in such a way that it will follow a predefined set of parameters before it scales down the number of EC2 instances, which protects your system from unintended slowdown or unavailability.\xa0 \xa0</p><p>Which of the following statements are true regarding the cooldown period? (Select all that applies)</p>', 'answers': ['It ensures that before the Auto Scaling group scales out, the EC2 instances have an ample time to cooldown.', 'It ensures that the Auto Scaling group launches or terminates additional EC2 instances without any downtime.', 'It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect.', 'Its default value is 300 seconds.', 'Its default value is 600 seconds.']}, 'correct_response': ['c', 'd'], 'original_assessment_id': 2566802, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are working for a commercial bank as an AWS Infrastructure Engineer handling the forex trading application of the bank. You have an Auto Scaling group of EC2 instances that allow your company to cope up with the current demand of traffic and achieve cost-efficiency. You want the Auto Scaling group to behave in such a way that it will follow a predefined set of parameters before it scales down the number of EC2 instances, which protects your system from unintended slowdown or unavailability.\xa0 \xa0Which of the following statements are true regarding the cooldown period? (Select all that applies)', 'id': 10442404, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'AWS Shared Responsibility Model', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Security and Compliance is a shared responsibility between AWS and the customer. This shared model can help relieve customer&rsquo;s operational burden as AWS operates, manages and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the service operates. The customer assumes responsibility and management of the guest operating system (including updates and security patches), other associated application software as well as the configuration of the AWS provided security group firewall.</p> <p>Customers should carefully consider the services they choose as their responsibilities vary depending on the services used, the integration of those services into their IT environment, and applicable laws and regulations. The nature of this shared responsibility also provides the flexibility and customer control that permits the deployment. As shown in the chart below, this differentiation of responsibility is commonly referred to as Security &ldquo;of&rdquo; the Cloud versus Security &ldquo;in&rdquo; the Cloud.</p> <p>Option 1 is incorrect since providing you customer data would be a breach in security protocols and data privacy laws.</p> <p>Option 3 is incorrect because it is your responsibility to set up the security tools AWS has provided you to secure your instances in your cloud environment.</p> <p>Option 4 is incorrect since it is your responsibility to delegate user access to your cloud environment.</p> <p>Refer to this diagram for a better understanding of the shared responsibility model.</p> <p>&nbsp;</p> <p><img src="https://d1.awsstatic.com/security-center/NewSharedResponsibilityModel.749924fb27d7c6de5cd59376dbaab323f86ce5dd.png" width="570" height="291" /></p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/compliance/shared-responsibility-model/">https://aws.amazon.com/compliance/shared-responsibility-model/</a></p> <p>&nbsp;</p>', 'relatedLectureIds': '', 'question': '<p>You are a Solutions Architect of a media company and you are instructed to migrate an on-premises web application architecture to AWS. During your design process, you have to give consideration to current on-premises security and determine which security attributes you are responsible for on AWS. <br><br>Which of the following does AWS provide for you as part of the shared responsibility model?</p>', 'answers': ['Customer Data', 'Physical network infrastructure', 'Instance security', 'User access to the AWS environment']}, 'correct_response': ['b'], 'original_assessment_id': 2566772, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are a Solutions Architect of a media company and you are instructed to migrate an on-premises web application architecture to AWS. During your design process, you have to give consideration to current on-premises security and determine which security attributes you are responsible for on AWS. Which of the following does AWS provide for you as part of the shared responsibility model?', 'id': 10442378, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>A&nbsp;<em>network access control list (ACL)</em>&nbsp;is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p> <p>Network ACL Rules are evaluated by rule number, from lowest to highest, and executed immediately when a matching allow/deny rule is found.</p> <p>&nbsp;</p> <p><img src="https://docs.aws.amazon.com/vpc/latest/userguide/images/nacl-example-diagram.png" alt="" width="611" height="354" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect since rules are evaluated from lowest to highest, not the other way around.</p> <p>Option 2 is incorrect because the Network ACL Rules are evaluated by rule number, from lowest to highest, and executed immediately when a matching allow/deny rule is found.</p> <p>Option 4 is incorrect since rules are executed immediately when a match is found and not&nbsp;after all rules are checked for conflicting allow/deny rules.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></span></p> <p>&nbsp;</p> <p><strong>Here is a handy comparison between Network ACL and Security Groups:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-security-group-vs-nacl/">https://tutorialsdojo.com/aws-cheat-sheet-security-group-vs-nacl/</a></span></p>', 'relatedLectureIds': '', 'question': '<p>To protect your enterprise applications against unauthorized access, you configured multiple rules for your Network ACLs in your VPC. How are the access rules evaluated?</p>', 'answers': ['Network ACL Rules are evaluated by rule number, from highest to lowest and are executed immediately when a matching allow/deny rule is found.', 'By default, all Network ACL Rules are evaluated before any traffic is allowed or denied.', 'Network ACL Rules are evaluated by rule number, from lowest to highest, and executed immediately when a matching allow/deny rule is found.', 'Network ACL Rules are evaluated by rule number, from lowest to highest, and executed after all rules are checked for conflicting allow/deny rules.']}, 'correct_response': ['c'], 'original_assessment_id': 2566774, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'To protect your enterprise applications against unauthorized access, you configured multiple rules for your Network ACLs in your VPC. How are the access rules evaluated?', 'id': 10442380, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Amazon EC2 has a soft limit of 20 instances per region, which can be easily resolved by completing the Amazon EC2 instance request form where your use case and your instance increase will be considered. Limit increases are tied to the region they were requested for.</p> <p>Option 2 is incorrect as there is no such limit in the Availability Zone.&nbsp;</p> <p>Option 3 is incorrect. Network Access List is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. It does not affect the creation of new EC2 instances.</p> <p>Option 4 is incorrect as there is no problem with your API credentials.&nbsp;</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2 " target="_blank" rel="noopener">https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2</a></p> <p><a href="http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>Using the EC2 API, you requested 40 m5.large On-Demand EC2 instances in a single Availability Zone. Twenty instances were successfully created but the other 20 requests failed.\xa0 \xa0</p><p>What is the solution for this issue and what is the root cause?</p>', 'answers': ['For new accounts, there is a soft limit of 20 EC2 instances per region. Submit an Amazon EC2 instance Request Form in order to lift this limit.', 'You can only create 20 instances per Availability Zone.  Select a different Availability Zone and retry creating the instances again.', 'A certain Inbound Rule in your Network Access List is preventing you to create more than 20 instances. Remove this rule and the issue will be resolved.', 'The API credentials that you are using has a limit of only 20 requests per hour. Try submitting the request again after one hour.']}, 'correct_response': ['a'], 'original_assessment_id': 2566776, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'Using the EC2 API, you requested 40 m5.large On-Demand EC2 instances in a single Availability Zone. Twenty instances were successfully created but the other 20 requests failed.\xa0 \xa0What is the solution for this issue and what is the root cause?', 'id': 10442382, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Option 1 is correct because when you create an EBS volume in an Availability Zone, it is automatically replicated within that zone only to prevent data loss due to a failure of any single hardware component. After you create a volume, you can attach it to any EC2 instance in the same Availability Zone.</p> <p>Option 2 is incorrect because it is the EBS snapshots, not the EBS volume, that has a copy of the data which is stored redundantly in multiple Availability Zones.</p> <p>Option 3 is incorrect because there is no option to span an EBS volume in different availability zones.</p> <p>Option 4 is incorrect because it doesn&rsquo;t matter which AWS region the EBS volume is created. EBS volumes only exist in a single availability zone while EBS snapshots are available in one AWS region.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>', 'relatedLectureIds': '', 'question': '<p>You work for a brokerage firm as an AWS Infrastructure Engineer who handles the stocks trading application. You host your database in an EC2 server with two EBS volumes for OS and data storage in ap-southeast-1a. Due to the fault tolerance requirements, there is a need to assess if the EBS volumes will be affected in the event of ap-southeast-1a availability zone outage. </p><p>Can EBS tolerate an Availability Zone failure each and every time?</p>', 'answers': ['<p>No, all EBS volumes are stored and replicated in a single AZ only.</p>', '<p>Yes, EBS volume is fault-tolerant and has multiple copies across multiple AZ.</p>', 'Depends on how the EBS volume is set up.', '<p>Depends on the AWS region where the EBS volume is created.</p>']}, 'correct_response': ['a'], 'original_assessment_id': 2566832, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You work for a brokerage firm as an AWS Infrastructure Engineer who handles the stocks trading application. You host your database in an EC2 server with two EBS volumes for OS and data storage in ap-southeast-1a. Due to the fault tolerance requirements, there is a need to assess if the EBS volumes will be affected in the event of ap-southeast-1a availability zone outage. Can EBS tolerate an Availability Zone failure each and every time?', 'id': 10442430, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'AMI', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>By default, instances that you launch into a virtual private cloud (VPC) can\'t communicate with your own network. You can enable access to your network from your VPC by attaching a virtual private gateway to the VPC, creating a custom route table, updating your security group rules, and creating an AWS managed VPN connection.</p> <p>Although the term&nbsp;<em>VPN connection</em>&nbsp;is a general term, in the Amazon VPC documentation, a VPN connection refers to the connection between your VPC and your own network. AWS supports Internet Protocol security (IPsec) VPN connections.</p> <p>A&nbsp;<em>customer gateway</em>&nbsp;is a physical device or software application on your side of the VPN connection.</p> <p>To create a VPN connection, you must create a customer gateway resource in AWS, which provides information to AWS about your customer gateway device. Next, you have to set up an Internet-routable IP address (static) of the customer gateway\'s external interface.</p> <p><span style="font-weight: 400;">The following diagram illustrates single VPN connections. The VPC has an attached virtual private gateway, and your remote network includes a customer gateway, which you must configure to enable the VPN connection. You set up the routing so that any traffic from the VPC bound for your network is routed to the virtual private gateway.</span></p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-10-27_22-45-01-dbcb3de60063eaba73e8d2d12c61d6dc.png" /></p> <p>&nbsp;</p> <p>Options 1 and 3 are incorrect since you don\'t need a NAT instance for you to be able to create a VPN connection.</p> <p>Option 4 is incorrect since you do not attach an EIP to a VPG.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html"><span style="font-weight: 400;">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html</span></a></p> <p><span style="font-weight: 400;"><a href="https://docs.aws.amazon.com/vpc/latest/userguide/SetUpVPNConnections.html">https://docs.aws.amazon.com/vpc/latest/userguide/SetUpVPNConnections.html</a></span></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></span></p> <p>&nbsp;</p>', 'relatedLectureIds': '', 'question': '<p>Your client is an insurance company that utilizes SAP HANA for their day-to-day ERP operations. Since you can’t migrate this database due to customer preferences, you need to integrate it with your current AWS workload in your VPC in which you are required to establish a site-to-site VPN connection.\xa0 \xa0</p><p>What needs to be configured outside of the VPC for you to have a successful site-to-site VPN connection? </p>', 'answers': ['A dedicated NAT instance in a public subnet', "<p>An Internet-routable IP address (static) of the customer gateway's external interface for the on-premises network</p>", 'The main route table in your VPC to route traffic through a NAT instance', 'An EIP to the Virtual Private Gateway']}, 'correct_response': ['b'], 'original_assessment_id': 2566778, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'Your client is an insurance company that utilizes SAP HANA for their day-to-day ERP operations. Since you can’t migrate this database due to customer preferences, you need to integrate it with your current AWS workload in your VPC in which you are required to establish a site-to-site VPN connection.\xa0 \xa0What needs to be configured outside of the VPC for you to have a successful site-to-site VPN connection?', 'id': 10442384, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>The correct answer is to deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow RDP access to bastion only from the corporate IP addresses.</p> <p>A bastion host is a special purpose computer on a network specifically designed and configured to withstand attacks. If you have a bastion host in AWS, it&nbsp;is basically just an EC2 instance. It should be in a public subnet with either a public or Elastic IP address with sufficient RDP or SSH access defined in the security group. Users log on to the bastion host via SSH or RDP and then use that session to manage other hosts in the private subnets.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_01-07-49-c56d7bdad437af14a0a3b8a17c9dcfd2.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect since you do not deploy the Bastion host to your corporate network. It should be in the public subnet of a VPC.</p> <p>Option 2 is incorrect since it should be deployed in a public subnet, not a private subnet.</p> <p>Option 3 is incorrect. Since it is a Windows bastion, you should allow RDP access and not SSH as this is mainly used for Linux-based systems.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html">https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>', 'relatedLectureIds': '', 'question': 'Your company is running a multi-tier web application farm in a virtual private cloud (VPC) that is not connected to their corporate network. They are connecting to the VPC over the Internet to manage the fleet of Amazon EC2 instances running in both the public and private subnets. You have added a bastion host with Microsoft Remote Desktop Protocol (RDP) access to the application instance security groups, but the company wants to further limit administrative access to all of the instances in the VPC. <br><br>Which of the following bastion host deployment options will meet this requirement?', 'answers': ['Deploy a Windows Bastion host on the corporate network that has RDP access to all EC2 instances in the VPC.', 'Deploy a Windows Bastion host with an Elastic IP address in the private subnet, and restrict RDP access to the bastion from only the corporate public IP addresses.', 'Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow SSH access to the bastion from anywhere.', 'Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow RDP access to bastion only from the corporate IP addresses.']}, 'correct_response': ['d'], 'original_assessment_id': 2566780, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'Your company is running a multi-tier web application farm in a virtual private cloud (VPC) that is not connected to their corporate network. They are connecting to the VPC over the Internet to manage the fleet of Amazon EC2 instances running in both the public and private subnets. You have added a bastion host with Microsoft Remote Desktop Protocol (RDP) access to the application instance security groups, but the company wants to further limit administrative access to all of the instances in the VPC. Which of the following bastion host deployment options will meet this requirement?', 'id': 10442386, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<div> <p>Server-side encryption is about data encryption at rest&mdash;that is, Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects. For example, if you share your objects using a pre-signed URL, that URL works the same way for both encrypted and unencrypted objects.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/security-center/SecurityBlog/bucket_policies_defense_s3.43e6c93a095f2f55b33b30276f4782ab9ec79f47.png" alt="" width="650" height="366" /></p> <p>You have three mutually exclusive options depending on how you choose to manage the encryption keys:</p> <ol> <li>Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</li> <li>Use Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS)</li> <li>Use Server-Side Encryption with Customer-Provided Keys (SSE-C)</li> </ol> <br /> <p>Options 1 and 4 are correct because they are using&nbsp;Amazon S3-Managed Keys (SSE-S3) and&nbsp;Customer-Provided Keys (SSE-C).&nbsp;SSE-S3 uses&nbsp;AES-256 encryption and&nbsp;SSE-C allows you to use your own encryption key.</p> <p>Options 2 and 3 are incorrect because both options use EBS encryption and not S3.</p> <p>Option 5 is incorrect as S3 doesn\'t provide AES-128 encryption, only&nbsp;AES-256.</p> <p>&nbsp;</p> </div> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>', 'relatedLectureIds': '', 'question': 'For data privacy, a healthcare company has been asked to comply with the Health Insurance Portability and Accountability Act (HIPAA). They have been told that all of the data being backed up or stored on Amazon S3 must be encrypted. <br><br>What is the best option to do this? (Choose 2)', 'answers': ['Before sending the data to Amazon S3 over HTTPS, encrypt the data locally first using your own encryption keys.', 'Store the data on EBS volumes with encryption enabled instead of using Amazon S3.', 'Store the data in encrypted EBS snapshots.', 'Enable Server-Side Encryption on an S3 bucket to make use of AES-256 encryption.', 'Enable Server-Side Encryption on an S3 bucket to make use of AES-128 encryption.']}, 'correct_response': ['a', 'd'], 'original_assessment_id': 2566786, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'For data privacy, a healthcare company has been asked to comply with the Health Insurance Portability and Accountability Act (HIPAA). They have been told that all of the data being backed up or stored on Amazon S3 must be encrypted. What is the best option to do this? (Choose 2)', 'id': 10442390, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Redshift', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Amazon Redshift is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake. Redshift delivers ten times faster performance than other data warehouses by using machine learning, massively parallel query execution, and columnar storage on high-performance disk.</p> <p>Option 1 is incorrect. DynamoDB is a NoSQL database which is based on key-value pairs used for fast processing of small data that dynamically grows and changes. But if you need to scan large amounts of data (ie a lot of keys all in one query), the performance will not be optimal.</p> <p>Option 2 is incorrect because Elasticache is used to increase the performance, speed and redundancy with which applications can retrieve data by providing an in-memory database caching system, and not for database analytical processes.</p> <p>Option 3 is incorrect because RDS is mainly used for On-Line Transaction Processing (OLTP) applications and not for Online Analytics Processing (OLAP).</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html">https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html</a></p> <p><a href="https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.htm">https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.htm</a>l</p> <p>&nbsp;</p> <p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You are working for an online hotel booking firm with terabytes of customer data coming from your websites and applications. There is an annual corporate meeting where you need to present the booking behavior and acquire new insights from your customers’ data. You are looking for a service to perform super-fast analytics on massive data sets in near real-time.\xa0 \xa0</p><p>Which of the following services gives you the ability to store huge amounts of data and perform quick and flexible queries on it?\xa0 </p>', 'answers': ['DynamoDB', 'ElastiCache', 'RDS', 'Redshift']}, 'correct_response': ['d'], 'original_assessment_id': 2566788, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are working for an online hotel booking firm with terabytes of customer data coming from your websites and applications. There is an annual corporate meeting where you need to present the booking behavior and acquire new insights from your customers’ data. You are looking for a service to perform super-fast analytics on massive data sets in near real-time.\xa0 \xa0Which of the following services gives you the ability to store huge amounts of data and perform quick and flexible queries on it?', 'id': 10442392, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>The route table entries enable EC2 instances in the subnet to use IPv4 to communicate with other instances in the VPC, and to communicate directly over the Internet. A subnet that\'s associated with a route table that has a route to an Internet gateway is known as a public subnet.</p> <p>If you could not connect to your EC2 instance even if there is already an Internet Gateway in your VPC and there is no issue in the security group, then you must check if the entries in the route table are properly configured.</p> <p>Option 1 is incorrect since you already have a public IP address for your EC2 instance, and doesn\'t require an EIP anymore.</p> <p>Option 2 is incorrect because having a secondary private IP address is only used within the VPC, not when connecting to the outside Internet.</p> <p>Option 4 is incorrect because it is better to go through your setup and make sure that you didn\'t miss a step, such as adding a route in the route table, before you check the actual CloudWatch logs to see if an instance has an issue.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You have set up a VPC with public subnet and an Internet gateway. You set up an EC2 instance with a public IP as well. However, you are still not able to connect to the instance via the Internet. You checked its associated security group and it seems okay.<br><br>What should you do to ensure you can connect to the EC2 instance from the Internet?</p>', 'answers': ['Set an Elastic IP Address to the EC2 instance.', 'Set a Secondary Private IP Address to the EC2 instance.', 'Check the main route table and ensure that the right route entry to the Internet Gateway (IGW) is configured.', 'Check the CloudWatch logs as there must be some issue in the EC2 instance.']}, 'correct_response': ['c'], 'original_assessment_id': 2566792, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You have set up a VPC with public subnet and an Internet gateway. You set up an EC2 instance with a public IP as well. However, you are still not able to connect to the instance via the Internet. You checked its associated security group and it seems okay.What should you do to ensure you can connect to the EC2 instance from the Internet?', 'id': 10442396, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Glacier', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Amazon Glacier is an extremely low-cost storage service that provides secure, durable, and flexible storage for data backup and archival. Amazon Glacier is designed to store data that is infrequently accessed. Amazon Glacier enables customers to offload the administrative burdens of operating and scaling storage to AWS so that they don&rsquo;t have to worry about capacity planning, hardware provisioning, data replication, hardware failure detection and repair, or time-consuming hardware migrations.</p> <p>Option 1 is incorrect because storing cached session data is the main use case for ElastiCache and not Amazon Glacier.</p> <p>Option 4 is incorrect because you should use RDS or DynamoDB for your active database storage as S3,&nbsp;in general, is used for storing your data or files.</p> <p>Option 5 is incorrect because storing it for data warehousing is the main use case of Amazon Redshift. It does not meet the requirement of being able to archive your infrequently accessed data. You can use S3 standard instead for frequently accessed data or Glacier for infrequently accessed data and archiving.</p> <p>It is advisable to transition the standard data to infrequent access first then transition it to Amazon Glacier. You can specify in the lifecycle rule the time it will sit in standard tier and infrequent access. You can also delete the objects after a certain amount of time.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-11-12_21-01-50-8e238b3286aa6c7c72f18b468ce19ead.png" /></p> <p>&nbsp;</p> <p>In transitioning S3 standard to Glacier you need to tell S3 which objects are to be archived to the new Glacier storage option, and under what conditions. You do this by setting up a lifecycle rule using the following elements:</p> <ul> <li>-A prefix to specify which objects in the bucket are subject to the policy.</li> <li>-A relative or absolute time specifier and a time period for transitioning objects to Glacier. The time periods are interpreted with respect to the object&rsquo;s creation date. They can be relative (migrate items that are older than a certain number of days) or absolute (migrate items on a specific date)</li> <li>-An object age at which the object will be deleted from S3. This is measured from the original PUT of the object into the service, and the clock is not reset by a transition to Glacier.</li> </ul> <p>&nbsp;</p> <p>You can create a lifecycle rule in the AWS Management Console.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/glacier/faqs/">https://aws.amazon.com/glacier/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Glacier Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-glacier/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-glacier/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You are working for an advertising company as their Senior Solutions Architect handling the S3 storage data. Your company has terabytes of data sitting on AWS S3 standard storage class, which accumulates significant operational costs. The management wants to cut down on the cost of their cloud infrastructure so you were instructed to switch to Glacier to lessen the cost per GB storage.\xa0 \xa0</p><p>The Amazon Glacier storage service is primarily used for which use case? (Choose 2)\xa0 </p>', 'answers': ['Storing cached session data', 'Storing infrequently accessed data', 'Storing Data archives', 'Used for active database storage', 'Used as a data warehouse']}, 'correct_response': ['b', 'c'], 'original_assessment_id': 2566842, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are working for an advertising company as their Senior Solutions Architect handling the S3 storage data. Your company has terabytes of data sitting on AWS S3 standard storage class, which accumulates significant operational costs. The management wants to cut down on the cost of their cloud infrastructure so you were instructed to switch to Glacier to lessen the cost per GB storage.\xa0 \xa0The Amazon Glacier storage service is primarily used for which use case? (Choose 2)', 'id': 10442440, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Disaster Recovery', 'prompt': {'answers': ['<p>Backup &amp; Restore</p>', 'Pilot Light', '<p>Warm Standby</p>', '<p>Multi Site</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': "<p>You are working for a University as their AWS Consultant. They want to have a disaster recovery strategy in AWS for mission-critical applications after suffering a disastrous outage wherein they lost student and employee records. They don't want this to happen again but at the same time want to minimize the monthly costs. You are instructed to set up a minimum version of the application that is always available in case of any outages.\xa0 \xa0</p><p>Which of the following disaster recovery architectures is the most suitable one to use in this scenario?</p>", 'explanation': '<p>The correct answer is option 2 - Pilot Light.</p> <p>The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in the cloud. The idea of the pilot light is an analogy that comes from the gas heater. In a gas heater, a small flame that&rsquo;s always on can quickly ignite the entire furnace to heat up a house. This scenario is similar to a backup-and-restore scenario.</p> <p>For example, with AWS you can maintain a pilot light by configuring and running the most critical core elements of your system in AWS. When the time comes for recovery, you can rapidly provision a full-scale production environment around the critical core.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-02-13_01-09-16-5d7907b9394b71dc4064a6e8abae0647.jpg" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because you are running mission-critical applications, and the speed of recovery from backup and restore solution might not meet your RTO and RPO.</p> <p>Option 3 is incorrect. Warm standby is a method of redundancy in which the scaled-down secondary system runs in the background of the primary system. Doing so would not optimize your savings as much as running a pilot light recovery since some of your services are always running in the background.</p> <p>Option 4 is incorrect as well. Multi-site is the most expensive solution out of disaster recovery solutions. You are trying to save monthly costs so this should be the least probable choice for you.</p> <p>&nbsp;</p> <p><strong>References</strong>:</p> <p class="p1"><span class="s1"><a href="https://d1.awsstatic.com/whitepapers/aws-disaster-recovery.pdf">https://d1.awsstatic.com/whitepapers/aws-disaster-recovery.pdf</a></span></p> <p>&nbsp;</p> <p><strong>Backup and Restore vs Pilot Light vs Warm Standby vs Multi-site:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-backup-and-restore-vs-pilot-light-vs-warm-standby-vs-multi-site/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-backup-and-restore-vs-pilot-light-vs-warm-standby-vs-multi-site/</span></a></p> <p>&nbsp;</p> <p><strong>RPO and RTO Explained:</strong></p> <p><iframe src="https://www.youtube.com/embed/rD3nBaS3OG4" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566796, '_class': 'assessment', 'updated': '2019-06-23T01:50:51Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': "You are working for a University as their AWS Consultant. They want to have a disaster recovery strategy in AWS for mission-critical applications after suffering a disastrous outage wherein they lost student and employee records. They don't want this to happen again but at the same time want to minimize the monthly costs. You are instructed to set up a minimum version of the application that is always available in case of any outages.\xa0 \xa0Which of the following disaster recovery architectures is the most suitable one to use in this scenario?", 'id': 10442398, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Highly Available Network Design', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Using an Elastic Load Balancer is an ideal solution for adding elasticity to your application. Alternatively, you can also create a policy in Route53, such as a Weighted routing policy, to evenly distribute the traffic to 2 or more EC2 instances. Hence Options 1 and 3 are the correct answers.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/govcloud-us/latest/ug-west/images/r53-cf-elb.png" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect because setting up an S3 cache does not provide elasticity and scalability to your EC2 instances.</p> <p>Option 4 is incorrect because AWS WAF is a web application firewall that helps protect your web applications from common web exploits. This service is more on providing security to your applications.</p> <p>Option 5 is incorrect because AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. It does not provide scalability or elasticity to your instances.</p> <p>&nbsp;</p> <p><strong>References</strong>:</p> <p><a href="https://aws.amazon.com/elasticloadbalancing">https://aws.amazon.com/elasticloadbalancing</a></p> <p><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>A start-up company has an EC2 instance that is hosting a web application. The volume of users is expected to grow in the coming months and hence, you need to add more elasticity and scalability in your AWS architecture to cope with the demand.\xa0 </p><p>Which of the following options can satisfy the above requirement for the given scenario? (Choose 2)</p>', 'answers': ['<p>Set up two EC2 instances and then put them behind an Elastic Load balancer (ELB).</p>', '<p>Set up an S3 Cache in front of the EC2 instance.</p>', '<p>Set up two EC2 instances and use Route 53 to route traffic based on a Weighted Routing Policy.</p>', '<p>Set up an AWS WAF behind your EC2 Instance.</p>', '<p>Set up two EC2 instances deployed using Launch Templates and integrated with AWS Glue. </p>']}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2566798, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'A start-up company has an EC2 instance that is hosting a web application. The volume of users is expected to grow in the coming months and hence, you need to add more elasticity and scalability in your AWS architecture to cope with the demand.\xa0 Which of the following options can satisfy the above requirement for the given scenario? (Choose 2)', 'id': 10442400, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'RDS', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>In Amazon RDS, failover is automatically handled so that you can resume database operations as quickly as possible without administrative intervention in the event that your primary database instance went down. When failing over, Amazon RDS simply flips the canonical name record (CNAME) for your DB instance to point at the standby, which is in turn promoted to become the new primary.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="http://d1nqddva888cns.cloudfront.net/rds_ha_5.png" alt="" width="500" height="498" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect since IP addresses are per subnet, and subnets cannot span multiple AZs.</p> <p>Option 2 is incorrect since in the event of a failure, there is no database to reboot with.</p> <p>Option 3 is incorrect since with multi-AZ enabled, you already have a standby database in another AZ.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/rds/details/multi-az/">https://aws.amazon.com/rds/details/multi-az/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>An accounting application uses an RDS database configured with Multi-AZ deployments to improve availability. What would happen to RDS if the primary database instance fails?</p>', 'answers': ['The IP address of the primary DB instance is switched to the standby DB instance.', 'The primary database instance will reboot.', 'A new database instance is created in the standby Availability Zone.', 'The canonical name record (CNAME) is switched from the primary to standby instance.']}, 'correct_response': ['d'], 'original_assessment_id': 2566736, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'An accounting application uses an RDS database configured with Multi-AZ deployments to improve availability. What would happen to RDS if the primary database instance fails?', 'id': 10442344, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>To allow the custom port, you have to change the Inbound Rules in your Security Group to allow traffic coming from the mobile devices.&nbsp;Security Groups usually control the list of ports that are allowed to be used by your EC2 instances and the NACLs control which network or list of IP addresses can connect to your whole VPC.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/userguide/images/nacl-example-diagram.png" alt="" width="611" height="354" /></p> <p>&nbsp;</p> <p>When you create a security group, it has no inbound rules. Therefore, no inbound traffic originating from another host to your instance is allowed until you add inbound rules to the security group. By default, a security group includes an outbound rule that allows all outbound traffic. You can remove the rule and add outbound rules that allow specific outbound traffic only. If your security group has no outbound rules, no outbound traffic originating from your instance is allowed.</p> <p>Options 1 and 4 are both incorrect because any changes to the Security Groups or Network Access Control Lists&nbsp;are applied immediately and not after 60 minutes or after the instance reboot.</p> <p>Option 2 is incorrect because&nbsp;the scenario says that VPC is using a default configuration. Since by default, Network ACL allows <strong>all</strong> inbound and outbound IPv4 traffic, then there is no point of explicitly allowing the port in the Network ACL. Security Groups, on the other hand, does not allow incoming traffic by default, unlike Network ACL.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You are a Solutions Architect working for a large insurance company that deployed their production environment on a custom Virtual Private Cloud in AWS with a default configuration. The VPC consists of two private subnets and one public subnet. Inside the public subnet is a group of EC2 instances which are created by an Auto Scaling group and all of the instances are in the same Security Group. Your development team has created a new web application which connects to mobile devices using a custom port. This application has been deployed to the production environment and you need to open this port globally to the Internet.\xa0 \xa0</p><p>Which of the following is the correct procedure?</p>', 'answers': ['Open the custom port on the Security Group. Your EC2 instances will be able to use this port after 60 minutes.', 'Open the custom port on the Network Access Control List of your VPC. Your EC2 instances will be able to use this port immediately.', 'Open the custom port on the Security Group. Your EC2 instances will be able to use this port immediately.', 'Open the custom port on the Network Access Control List of your VPC. Your EC2 instances will be able to use this port after a reboot.']}, 'correct_response': ['c'], 'original_assessment_id': 2566806, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are a Solutions Architect working for a large insurance company that deployed their production environment on a custom Virtual Private Cloud in AWS with a default configuration. The VPC consists of two private subnets and one public subnet. Inside the public subnet is a group of EC2 instances which are created by an Auto Scaling group and all of the instances are in the same Security Group. Your development team has created a new web application which connects to mobile devices using a custom port. This application has been deployed to the production environment and you need to open this port globally to the Internet.\xa0 \xa0Which of the following is the correct procedure?', 'id': 10442408, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Elastic Load Balancing supports three types of load balancers. You can select the appropriate load balancer based on your application needs.</p> <p>If you need flexible application management and TLS termination then we recommend that you use Application Load Balancer. If extreme performance and static IP is needed for your application then we recommend that you use Network Load Balancer. If your application is built within the EC2 Classic network then you should use Classic Load Balancer.</p> <p>An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply, and then selects a target from the target group for the rule action. You can configure listener rules to route requests to different target groups based on the content of the application traffic. Routing is performed independently for each target group, even when a target is registered with multiple target groups.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/redirect_path.png" alt="" width="490" height="421" /></p> <p>&nbsp;</p> <p>Application Load Balancers support TLS termination capabilities, path-based routing, host-based routing and support for containerized applications hence, Option 1 is correct.</p> <p>Options 2, 3 and 4 are incorrect as none of these support&nbsp;path-based routing and host-based routing, unlike an Application Load Balancer.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html#application-load-balancer-benefits">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html#application-load-balancer-benefits</a></p> <p><a href="https://aws.amazon.com/elasticloadbalancing/faqs/">https://aws.amazon.com/elasticloadbalancing/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</a></span></p> <p>&nbsp;</p> <p><strong>Here is a deep dive on Elastic Load Balancing and Best Practices:</strong></p> <p><iframe src="https://www.youtube.com/embed/VIgAT7vjol8" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p> <p>&nbsp;</p> <p><strong>Application Load Balancer vs Network Load Balancer vs Classic Load Balancer:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You are helping out a new DevOps Engineer to design her first architecture in AWS. She is planning to develop a highly available and fault-tolerant architecture which is composed of an Elastic Load Balancer and an Auto Scaling group of EC2 instances deployed across multiple Availability Zones. This will be used by an online accounting application which requires TLS termination capabilities, path-based routing, host-based routing, and bi-directional communication channels using WebSockets.\xa0 </p><p>Which is the most suitable type of Elastic Load Balancer that you should recommend for her to use?</p>', 'answers': ['<p>Application Load Balancer</p>', '<p>Network Load Balancer</p>', '<p>Classic Load Balancer</p>', '<p>Either a Classic Load Balancer or a Network Load Balancer</p>']}, 'correct_response': ['a'], 'original_assessment_id': 4949130, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are helping out a new DevOps Engineer to design her first architecture in AWS. She is planning to develop a highly available and fault-tolerant architecture which is composed of an Elastic Load Balancer and an Auto Scaling group of EC2 instances deployed across multiple Availability Zones. This will be used by an online accounting application which requires TLS termination capabilities, path-based routing, host-based routing, and bi-directional communication channels using WebSockets.\xa0 Which is the most suitable type of Elastic Load Balancer that you should recommend for her to use?', 'id': 10442442, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Amazon S3 offers a highly durable, scalable, and secure destination for backing up and archiving your critical data. This is the correct option as the start-up company is looking for a durable storage to store the audio and text files. In addition, ElastiCache is only used for caching and not specifically as a Global Content Delivery Network (CDN).</p> <ul> <li>Option 1 is incorrect as Amazon Redshift is usually used as a Data Warehouse.</li> <li>Option 2 is incorrect as Amazon Glacier is usually used for data archives.</li> <li>Option 4 is&nbsp;incorrect as data stored in an instance store is not durable.</li> </ul> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/s3/" target="_blank" rel="noopener">https://aws.amazon.com/s3/</a></p> <p><a href="https://aws.amazon.com/caching/cdn/">https://aws.amazon.com/caching/cdn/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>A\xa0start-up company that offers an intuitive financial data analytics service has consulted you about their AWS architecture. They have a fleet of Amazon EC2 worker instances that process financial data and then outputs reports which are used by their clients. You must store the generated report files in a durable storage. The number of files to be stored can grow over time as the start-up company is expanding rapidly overseas and hence, they also need a way to distribute the reports faster to clients located across the globe.\xa0 </p><p>Which of the following is a cost-efficient and scalable storage option that you should use for this scenario?</p>', 'answers': ['<p>Use Amazon Redshift as the data storage and CloudFront as the CDN.</p>', '<p>Use Amazon Glacier as the data storage and ElastiCache as the CDN.</p>', '<p>Use Amazon S3 as the data storage and CloudFront as the CDN.</p>', '<p>Use multiple EC2 instance stores for data storage and ElastiCache as the CDN.</p>']}, 'correct_response': ['c'], 'original_assessment_id': 2566728, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'A\xa0start-up company that offers an intuitive financial data analytics service has consulted you about their AWS architecture. They have a fleet of Amazon EC2 worker instances that process financial data and then outputs reports which are used by their clients. You must store the generated report files in a durable storage. The number of files to be stored can grow over time as the start-up company is expanding rapidly overseas and hence, they also need a way to distribute the reports faster to clients located across the globe.\xa0 Which of the following is a cost-efficient and scalable storage option that you should use for this scenario?', 'id': 10442336, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Route53', 'prompt': {'feedbacks': ['', '', '', '', ''], 'explanation': '<p>Options 2 and 4 are correct.&nbsp;To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer. An alias record is a Route 53 extension to DNS. It\'s similar to a CNAME record, but you can create an alias record both for the root domain, such as tutorialsdojo.com, and for subdomains, such as portal.tutorialsdojo.com. (You can create CNAME records only for subdomains.) To enable IPv6 resolution, you would need to create a second resource record, tutorialsdojo.com ALIAS AAAA -&gt; myelb.us-west-2.elb.amazonnaws.com, this is assuming your Elastic Load Balancer has IPv6 support.</p> <p>Option 1 is incorrect because you only use Non-Alias with a type &ldquo;A&rdquo; record set for IP addresses.</p> <p>Option 3 is incorrect because you can\'t create a CNAME record at the zone apex. For example, if you register the DNS name tutorialsdojo.com, the zone apex is tutorialsdojo.com.</p> <p>Option 5 is incorrect because an MX record is primarily used for mail servers. It includes a priority number and a domain name, for example, 10 mailserver.tutorialsdojo.com.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p>', 'relatedLectureIds': '', 'question': '<p>You are an IT Consultant for an advertising company that is currently working on a proof of concept project that automatically provides SEO analytics for their clients. Your company has a VPC in AWS that operates in dual-stack mode in which IPv4 and IPv6 communication is allowed. You deployed the application to an Auto Scaling group of EC2 instances with an Application Load Balancer in front that evenly distributes the incoming traffic. You are ready to go live but you need to point your domain name (tutorialsdojo.com) to the Application Load Balancer.\xa0 \xa0</p><p>In Route 53, which record types will you use to point the DNS name of the Application Load Balancer? (Choose 2)</p>', 'answers': ['<p>Non-Alias with a type "A" record set</p>', '<p>Alias with a type "AAAA" record set</p>', '<p>Alias with a type "CNAME" record set</p>', '<p>Alias with a type "A" record set</p>', '<p>Alias with a type of “MX” record set</p>']}, 'correct_response': ['b', 'd'], 'original_assessment_id': 5711302, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': 'You are an IT Consultant for an advertising company that is currently working on a proof of concept project that automatically provides SEO analytics for their clients. Your company has a VPC in AWS that operates in dual-stack mode in which IPv4 and IPv6 communication is allowed. You deployed the application to an Auto Scaling group of EC2 instances with an Application Load Balancer in front that evenly distributes the incoming traffic. You are ready to go live but you need to point your domain name (tutorialsdojo.com) to the Application Load Balancer.\xa0 \xa0In Route 53, which record types will you use to point the DNS name of the Application Load Balancer? (Choose 2)', 'id': 10442444, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Cognito', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>You can use Amazon Cognito to deliver temporary, limited-privilege credentials to your application so that your users can access AWS resources. Amazon Cognito identity pools support both authenticated and unauthenticated identities. You can retrieve a unique Amazon Cognito identifier (identity ID) for your end user immediately if you\'re allowing unauthenticated users or after you\'ve set the login tokens in the credentials provider if you\'re authenticating users.</p> <p>That is why the correct answer for this question is Option 3: Cognito ID.</p> <p>Option 1 is incorrect because Cognito SDK is not the unique Amazon Cognito identifier but a software development kit that is available in various programming languages.</p> <p>Option 2 is incorrect because Cognito Key Pair is not the unique Amazon Cognito identifier but a cryptography key.</p> <p>Option 4 is incorrect because the Cognito API is not the unique Amazon Cognito identifier and is primarily used as an Application Programming Interface.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="http://docs.aws.amazon.com/cognito/latest/developerguide/getting-credentials.html">http://docs.aws.amazon.com/cognito/latest/developerguide/getting-credentials.html</a></p>', 'relatedLectureIds': '', 'question': "<p>A San Francisco-based tech startup is building a cross-platform mobile app that can notify the user with upcoming astronomical events such as eclipses, blue moon, novae or a meteor shower. Your mobile app authenticates with the Identity Provider (IdP) using the provider's SDK and Amazon Cognito. Once the end user is authenticated with the IdP, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito. </p><p>Which of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?</p>", 'answers': ['<p>Cognito SDK</p>', '<p>Cognito Key Pair</p>', '<p>Cognito ID</p>', '<p>Cognito API</p>']}, 'correct_response': ['c'], 'original_assessment_id': 7572958, '_class': 'assessment', 'updated': '2019-06-23T01:50:49Z', 'created': '2019-06-23T01:50:49Z', 'question_plain': "A San Francisco-based tech startup is building a cross-platform mobile app that can notify the user with upcoming astronomical events such as eclipses, blue moon, novae or a meteor shower. Your mobile app authenticates with the Identity Provider (IdP) using the provider's SDK and Amazon Cognito. Once the end user is authenticated with the IdP, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito. Which of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?", 'id': 10442446, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Lambda', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>AWS Lambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke a Lambda function. When you use an AWS services as a trigger, the invocation type is predetermined for each service. You have no control over the invocation type that these event sources use when they invoke your Lambda function. Since the processing only takes 5 minutes, Lambda is also a cost-effective choice.</p> <p>Option 1 is incorrect because the AWS Step Functions service lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Although this can be a valid solution, it&nbsp;is not cost-effective since the application does not have a lot of components to orchestrate. Lambda functions can effectively meet the requirements in this scenario without using Step Functions. This service is not as cost-effective as Lambda.</p> <p>Options 2 and 4 are incorrect as SQS and SNS are messaging services and do not perform asynchronous function calls.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/lambda/latest/dg/welcome.html"> https://docs.aws.amazon.com/lambda/latest/dg/welcome.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Lambda Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/</span></a></p> <p>&nbsp;</p>', 'relatedLectureIds': '', 'question': '<p>Your customer is building an internal application that serves as a repository for images uploaded by a couple of users. Whenever a user uploads an image, it would be sent to Kinesis for processing before it is stored in an S3 bucket. Afterwards, if the upload was successful, the application will return a prompt telling the user that the upload is successful. The entire processing typically takes about 5 minutes to finish. </p><p>Which of the following options will allow you to asynchronously process the request to the application in the most cost-effective manner?</p>', 'answers': ['<p>Use a combination of Lambda and Step Functions to orchestrate service components and asynchronously process the requests.</p>', '<p>Use a combination of a Lambda function and SQS to queue the requests and then asynchronously process them by using Lambda.</p>', '<p>Create a Lambda function that will asynchronously process the requests.</p>', '<p>Use a combination of a Lambda function and SNS to asynchronously process the requests by sending a notification back to Lambda once the image has been successfully processed.</p>']}, 'correct_response': ['c'], 'original_assessment_id': 8159704, '_class': 'assessment', 'updated': '2019-06-23T01:50:50Z', 'created': '2019-06-23T01:50:50Z', 'question_plain': 'Your customer is building an internal application that serves as a repository for images uploaded by a couple of users. Whenever a user uploads an image, it would be sent to Kinesis for processing before it is stored in an S3 bucket. Afterwards, if the upload was successful, the application will return a prompt telling the user that the upload is successful. The entire processing typically takes about 5 minutes to finish. Which of the following options will allow you to asynchronously process the request to the application in the most cost-effective manner?', 'id': 10442448, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Route53', 'prompt': {'feedbacks': ['', '', '', ''], 'explanation': '<p>Geolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region.</p> <p>When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users. You can also use geolocation routing to restrict distribution of content to only the locations in which you have distribution rights. Another possible use is for balancing load across endpoints in a predictable, easy-to-manage way, so that each user location is consistently routed to the same endpoint.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/images/how-route-53-routes-traffic.png" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect because Elastic Load Balancers distribute traffic among EC2 instances across multiple Availability&nbsp;Zones but not across AWS regions.</p> <p>Option 3 is incorrect because the CloudFront geo-restriction feature is primarily used to prevent users in specific geographic locations from accessing content that you\'re distributing through a CloudFront web distribution. It does not let you choose the resources that serve your traffic based on the geographic location of your users, unlike the Geolocation routing policy in Route 53.</p> <p>Option 4 is incorrect because the Route 53 Weighted Routing policy is not a suitable solution to meet the requirements of this scenario. It just lets you associate multiple resources with a single domain name (tutorialsdojo.com) or subdomain name (forums.tutorialsdojo.com) and choose how much traffic is routed to each resource. You have to use a&nbsp;Geolocation routing policy instead.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/geolocation-routing-policy"> https://aws.amazon.com/premiumsupport/knowledge-center/geolocation-routing-policy</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p> <p>&nbsp;</p> <p><strong>Latency Routing vs Geoproximity Routing vs Geolocation Routing:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-latency-routing-vs-geoproximity-routing-vs-geolocation-routing/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-latency-routing-vs-geoproximity-routing-vs-geolocation-routing/</span></a></p> <p>&nbsp;</p>', 'relatedLectureIds': '', 'question': '<p>You have a cryptocurrency exchange portal which is hosted in an Auto Scaling group of EC2 instances behind an Application Load Balancer, and are deployed across multiple AWS regions. Your users can be found all around the globe, but the majority are from Japan and Sweden. Because of the compliance requirements in these two locations, you want your Japanese users to connect to the servers in the <code>ap-northeast-1</code> Asia Pacific (Tokyo) region, while your Swedish users should be connected to the servers in the <code>eu-west-1</code> EU (Ireland) region. </p><p>Which of the following services would allow you to easily fulfill this requirement?</p>', 'answers': ['<p>Use Route 53 Geolocation Routing policy.</p>', '<p>Set up an Application Load Balancers that will automatically route the traffic to the proper AWS region.</p>', '<p>Set up a new CloudFront web distribution with the geo-restriction feature enabled.</p>', '<p>Use Route 53 Weighted Routing policy.</p>']}, 'correct_response': ['a'], 'original_assessment_id': 8159728, '_class': 'assessment', 'updated': '2019-06-23T01:50:50Z', 'created': '2019-06-23T01:50:50Z', 'question_plain': 'You have a cryptocurrency exchange portal which is hosted in an Auto Scaling group of EC2 instances behind an Application Load Balancer, and are deployed across multiple AWS regions. Your users can be found all around the globe, but the majority are from Japan and Sweden. Because of the compliance requirements in these two locations, you want your Japanese users to connect to the servers in the ap-northeast-1 Asia Pacific (Tokyo) region, while your Swedish users should be connected to the servers in the eu-west-1 EU (Ireland) region. Which of the following services would allow you to easily fulfill this requirement?', 'id': 10442450, 'related_lectures': [], 'assessment_type': 'multiple-choice'}]}, 'type': 'practice-test', 'title': 'AWS Certified Solutions Architect Associate Practice Test 2'}, {'quiz_data': {'next': None, 'count': 65, 'previous': None, 'results': [{'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Include the fifth EC2 instance to the Placement Group of the other four EC2 instances and enable Enhanced Networking.</p>', '<p>Set up a NAT gateway to allow access to the fifth EC2 instance.</p>', '<p>Configure the routing table for the public subnet to explicitly include the fifth EC2 instance.</p>', '<p>Associate an Elastic IP address to the fifth EC2 instance.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A VPC has a non-default public subnet which has four On-Demand EC2 instances that can be accessed over the Internet. Using the AWS CLI, you launched a fifth instance that uses the same subnet, Amazon Machine Image (AMI), and security group which are being used by the other instances. Upon testing, you are not able to access the new instance.\xa0 \xa0</p><p>Which of the following is the most suitable solution to solve this problem?</p>', 'explanation': '<div> <p>By default, a "<em>default subnet</em>" of your VPC is actually a public subnet, because the main route table sends the subnet\'s traffic that is destined for the internet to the internet gateway. You can make a default subnet into a private subnet by removing the route from the destination 0.0.0.0/0 to the internet gateway. However, if you do this, any EC2 instance running in that subnet can\'t access the internet.</p> <p>Instances that you launch into a default subnet receive both a public IPv4 address and a private IPv4 address, and both public and private DNS hostnames. Instances that you launch into a nondefault subnet in a default VPC don\'t receive a public IPv4 address or a DNS hostname. You can change your subnet\'s default public IP addressing behavior</p> <p>By default, nondefault subnets have the IPv4 public addressing attribute set to&nbsp;<strong><code>false</code></strong>, and default subnets have this attribute set to&nbsp;<strong><code>true</code></strong>. An exception is a nondefault subnet created by the Amazon EC2 launch instance wizard &mdash; the wizard sets the attribute to&nbsp;<strong><code>true</code></strong>.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/userguide/images/default-vpc-diagram.png" /></p> <p>&nbsp;</p> <p>Option 4 is correct because the fifth instance does not have a public IP address since it was deployed on a nondefault subnet. The other 4 instances are accessible over the Internet because they each have an Elastic IP address attached, unlike the last instance which only has a private IP address.&nbsp;An Elastic IP address is a public IPv4 address, which is reachable from the Internet. If your instance does not have a public IPv4 address, you can associate an Elastic IP address with your instance to enable communication with the Internet.</p> <p>Option 1 is incorrect because Placement Groups is primarily used to determine how your instances are placed on the underlying hardware while Enhanced Networking, on the other hand, is for providing high-performance networking capabilities using single root I/O virtualization (SR-IOV) on supported EC2 instance types.</p> <p>Option 2 is incorrect because you do not need a NAT Gateway nor a NAT instance in this scenario considering that the instances are already in <em>public</em> subnet. Remember that a NAT Gateway or a NAT instance is primarily used to enable instances in a <em><strong>private</strong></em> subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.</p> <p>Option 3 is incorrect because all four EC2 instances which are in the same subnet, same AMI, and same security group can be accessed over the Internet. This means that there is no problem on its routing table.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html">https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></strong></p> </div>'}, 'correct_response': ['d'], 'original_assessment_id': 2566980, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': 'A VPC has a non-default public subnet which has four On-Demand EC2 instances that can be accessed over the Internet. Using the AWS CLI, you launched a fifth instance that uses the same subnet, Amazon Machine Image (AMI), and security group which are being used by the other instances. Upon testing, you are not able to access the new instance.\xa0 \xa0Which of the following is the most suitable solution to solve this problem?', 'id': 9831668, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Amazon DynamoDB and AWS AppSync</p>', '<p>Amazon Redshift and AWS Mobile Hub</p>', '<p>Amazon Relational Database Service (RDS) and Amazon MQ</p>', '<p>Amazon Aurora and Amazon Cognito</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>Your manager has asked you to deploy a mobile application that can collect votes for a popular singing competition. Millions of users from around the world will submit votes using their mobile phones. These votes must be collected and stored in a highly scalable and highly available data store which will be queried for real-time ranking. </p><p>Which of the following combination of services should you use to meet this requirement?</p>', 'explanation': '<p>When the word durability pops out, the first service that should come to your mind is Amazon S3. Since this service is not available in the answer options, we can look at the other data store available which is Amazon DynamoDB.</p> <p>DynamoDB is durable, scalable, and highly available data store which can be used for real-time tabulation.&nbsp;You can also use AppSync with DynamoDB to make it easy for you to build collaborative apps that keep shared data updated in real time. You just specify the data for your app with simple code statements and AWS AppSync manages everything needed to keep the app data updated in real time. This will allow your app to access data in Amazon DynamoDB, trigger AWS Lambda functions, or run Amazon Elasticsearch queries and combine data from these services to provide the exact data you need for your app.</p> <p>Option 2 is incorrect as Amazon Redshift is mainly used as a data warehouse and for&nbsp;online analytic processing (<em>OLAP</em>). Although this service can be used for this scenario, DynamoDB is still the top choice given its better durability and scalability.&nbsp;</p> <p>Options 3 and 4 are possible answers in this scenario, however, DynamoDB is much more suitable for simple mobile apps which do not have complicated data relationships compared with enterprise web applications. The scenario says that the mobile app will be used from around the world, which is why you need a data storage service which can be supported globally. It would be a management overhead to implement multi-region deployment for your RDS and Aurora database instances compared to using the Global table feature of DynamoDB.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/dynamodb/faqs/">https://aws.amazon.com/dynamodb/faqs/</a></p> <p><a href="https://aws.amazon.com/appsync/">https://aws.amazon.com/appsync/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</span></a></p> <p>&nbsp;</p> <p><strong>Here is a deep dive on Amazon DynamoDB Design Patterns:</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/HaEPXoXVf2k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'}, 'correct_response': ['a'], 'original_assessment_id': 2566982, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': 'Your manager has asked you to deploy a mobile application that can collect votes for a popular singing competition. Millions of users from around the world will submit votes using their mobile phones. These votes must be collected and stored in a highly scalable and highly available data store which will be queried for real-time ranking. Which of the following combination of services should you use to meet this requirement?', 'id': 9831670, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['Security is automatically managed by AWS.', '<p>You can connect your AWS cloud resources to on-premises data centers using VPN connections.</p>', 'You can provision unlimited number of Amazon S3 and Glacier resources.', 'None of the above'], 'feedbacks': ['', '', '', ''], 'question': "<p>You were recently promoted to a technical lead role in your DevOps team. Your company has an existing VPC which is quite un-</p><p><br></p><p><br></p><p>utilized for the past few months. The business manager instructed you to integrate your on-premises data center and your VPC. You explained the list of tasks that you'll be doing and mentioned about a Virtual Private Network (VPN) connection. The business manager is not tech-savvy but he is interested to know what a VPN is and its benefits. </p><p>What is one of the major advantages of having a VPN in AWS?</p>", 'explanation': '<p>Option 2 is correct. One main advantage of having a VPN connection is that you will be able to connect your Amazon VPC to other remote networks.&nbsp;</p> <p>You can create an IPsec VPN connection between your VPC and your remote network. On the AWS side of the VPN connection, a&nbsp;<em>virtual private gateway</em>&nbsp;provides two VPN endpoints (tunnels) for automatic failover. You configure your&nbsp;<em>customer gateway</em>&nbsp;on the remote side of the VPN connection.&nbsp;If you have more than one remote network (for example, multiple branch offices), you can create multiple AWS managed VPN connections via your virtual private gateway to enable communication between these networks.</p> <p>You can create a VPN connection to your remote network by using an Amazon EC2 instance in your VPC that\'s running a third party software VPN appliance. AWS does not provide nor maintain third party software VPN appliances; however, you can choose from a range of products provided by partners and open source communities.</p> <p>Option 1 is incorrect because it is not entirely true. Take note that AWS extends to its customers the responsibility of securing the cloud, which is called the \'Shared Responsibility Model\'. AWS handles the security <strong>OF</strong> the cloud, while you manage the security <strong>IN</strong> the cloud.</p> <p>Option 3 is incorrect. Having a VPN connection does not change the limits on the number of AWS resources you can provision.</p> <p>Option 4 is incorrect because&nbsp;there is a way to connect your AWS cloud resources to on-premises data centers using VPN connections.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2566984, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': "You were recently promoted to a technical lead role in your DevOps team. Your company has an existing VPC which is quite un-utilized for the past few months. The business manager instructed you to integrate your on-premises data center and your VPC. You explained the list of tasks that you'll be doing and mentioned about a Virtual Private Network (VPN) connection. The business manager is not tech-savvy but he is interested to know what a VPN is and its benefits. What is one of the major advantages of having a VPN in AWS?", 'id': 9831672, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'relatedLectureIds': '', 'answers': ['Create a new IAM group and then add the users that require access to the S3 bucket. Afterwards, apply the policy to IAM group.', 'Create a new policy and apply it to multiple IAM users using a shell script.', 'Create a new S3 bucket access policy with unlimited access for each IAM user.', 'Create a new IAM role and add each user to the IAM role.'], 'feedbacks': ['', '', '', ''], 'question': "You are the Solutions Architect for your company's AWS account of approximately 300 IAM users. They have a new company policy that will change the access of 100 of the IAM users to have a particular sort of access to Amazon S3 buckets. <br><br>What will you do to avoid the time-consuming task of applying the policy at the individual user?", 'explanation': '<p>In this scenario, the best option is to group the set of users in an IAM Group and then apply a policy with the required access to the Amazon S3 bucket. This will enable you to easily add, remove, and manage the users instead of manually adding a policy to each and every 100 IAM users.&nbsp;</p> <p>Option 2 is incorrect because you need a new IAM Group for this scenario and not assign a policy to each user via a shell script. This method can save you time but afterwards, it will be difficult to manage all 100 users that are not contained in an IAM Group.</p> <p>Option 3 is incorrect because you need a new IAM Group and the method is also time-consuming.</p> <p>Option 4 is incorrect because you need to use an IAM Group and not an IAM role.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/IAM/latest/UserGuide/images/Relationship_Between_Entities_Example.diagram.png" /></p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566986, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': "You are the Solutions Architect for your company's AWS account of approximately 300 IAM users. They have a new company policy that will change the access of 100 of the IAM users to have a particular sort of access to Amazon S3 buckets. What will you do to avoid the time-consuming task of applying the policy at the individual user?", 'id': 9831674, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['The Amazon EC2 instance does not have a public IP address associated with it.', 'The Amazon EC2 instance is not a member of the same Auto Scaling group.', 'The Amazon EC2 instance is running in an Availability Zone that does not support Internet access.', 'The route table is not configured properly to send traffic from the EC2 instance to the Internet through the Internet gateway.', '<p>The route table is not configured properly to send traffic from the EC2 instance to the Internet through the customer gateway (CGW).</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are a Solutions Architect for a global news company. You are configuring a fleet of EC2 instances in a subnet which currently is in a VPC with an Internet gateway attached. All of these EC2 instances can be accessed from the Internet. You then launch another subnet and launch an EC2 instance in it, however you are not able to access the EC2 instance from the Internet.\xa0 \xa0</p><p>What could be the possible reasons for this issue? (Choose 2)</p>', 'explanation': '<p>In cases where your EC2 instance cannot access the Internet, you usually have to check two things:</p> <ol> <li>Does it have an EIP or public IP address?</li> <li>Is the route table properly configured?</li> </ol> <p>&nbsp;</p> <p>Below are the correct answers:&nbsp;</p> <ol> <li>Amazon EC2 instance does not have a public IP address associated with it.</li> <li>The route table is not configured properly to send traffic from the EC2 instance to the Internet through the Internet gateway.</li> </ol> <p>Option 2 is incorrect since Auto Scaling Groups do not affect Internet connectivity of EC2 instances.</p> <p>Option 3 is incorrect since there is no such Availability Zone where it does not specifically support Internet connectivity.</p> <p>Option 5 is incorrect since CGW is used when you are setting up a VPN. The correct gateway should be an Internet gateway.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['a', 'd'], 'original_assessment_id': 2566988, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': 'You are a Solutions Architect for a global news company. You are configuring a fleet of EC2 instances in a subnet which currently is in a VPC with an Internet gateway attached. All of these EC2 instances can be accessed from the Internet. You then launch another subnet and launch an EC2 instance in it, however you are not able to access the EC2 instance from the Internet.\xa0 \xa0What could be the possible reasons for this issue? (Choose 2)', 'id': 9831676, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Kinesis', 'prompt': {'relatedLectureIds': '', 'answers': ['Amazon Kinesis Data Firehose', 'Amazon Kinesis', 'Amazon Redshift', 'Amazon Macie'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a data analytics company as a Software Engineer, which has a client that is setting up an innovative checkout-free grocery store. You developed a monitoring application that uses smart sensors to collect the items that your customers are getting from the grocery’s refrigerators and shelves then automatically maps it to their accounts. To know more about the buying behavior of your customers, you want to analyze the items that are constantly being bought and store the results in S3 for durable storage.\xa0 \xa0</p><p>What service can you use to easily capture, transform, and load streaming data into Amazon S3, Amazon Elasticsearch Service, and Splunk?\xa0 </p>', 'explanation': '<p>Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you are already using today.</p> <p>It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</p> <p>In the diagram below, you gather the data from your smart refrigerators and use Kinesis Data firehouse to prepare and load the data. S3 will be used as a method of durably storing the data for analytics and the eventual ingestion of data for output using analytical tools.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-11-12_21-25-45-a0789b019c82e01bc6cbc2830a24f90f.png" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect because Amazon Kinesis is the streaming data platform of AWS and has four distinct services under it: Kinesis Data Firehose, Kinesis Data Streams, Kinesis Video Streams, and Amazon Kinesis Data Analytics. For a specific use case of the requirement by the question, use Kinesis Data Firehose.</p> <p>Option 3 is incorrect because Amazon Redshift is mainly used for data warehousing making it simple and cost-effective to analyze your data across your data warehouse and data lake. It does not meet the requirement of being able to load and stream data into data stores for analytics. You have to use Kinesis Data Firehose instead.</p> <p>Option 4 is incorrect because Amazon Macie is mainly used as a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. As a security feature of AWS, it does not meet the requirements of being able to load and stream data into data stores for analytics. You have to use Kinesis Data Firehose instead.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/kinesis/data-firehose/">https://aws.amazon.com/kinesis/data-firehose/</a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567000, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': 'You are working for a data analytics company as a Software Engineer, which has a client that is setting up an innovative checkout-free grocery store. You developed a monitoring application that uses smart sensors to collect the items that your customers are getting from the grocery’s refrigerators and shelves then automatically maps it to their accounts. To know more about the buying behavior of your customers, you want to analyze the items that are constantly being bought and store the results in S3 for durable storage.\xa0 \xa0What service can you use to easily capture, transform, and load streaming data into Amazon S3, Amazon Elasticsearch Service, and Splunk?', 'id': 9831686, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>1 subnet</p>', '<p>2 subnets</p>', '<p>3 subnets</p>', '<p>4 subnets</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A large financial company has recently adopted a hybrid cloud architecture to integrate their on-premises data center and their AWS Cloud. Your manager has instructed you to create a new VPC network topology which must support all Internet-facing web applications as well as the internally-facing applications that is accessed by employees only over VPN. To ensure high-availability and fault tolerance, both of their Internet-facing and internally-facing financial applications must be able to leverage at least two AZs for high availability. </p><p>What is the minimum number of subnets that you must create within your VPC to accommodate these requirements?</p>', 'explanation': '<p>In this scenario, the requirement is to have at least 2 Availability Zones for both Internet-facing (public) and Internally-facing (private) applications. Remember that one subnet is mapped into one specific Availability Zone. Since we need to have at least 2 public and 2 private subnets, the correct answer is 4 subnets.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/aws_regions.png" /></p> <p>&nbsp;</p> <p>A VPC spans all the Availability Zones in the region. After creating a VPC, you can add one or more subnets in each Availability Zone. When you create a subnet, you specify the CIDR block for the subnet, which is a subset of the VPC CIDR block. Each subnet must reside entirely within one Availability Zone and cannot span zones. Availability Zones are distinct locations that are engineered to be isolated from failures in other Availability Zones. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single location. AWS assigns a unique ID to each subnet.</p> <p>&nbsp;</p> <p><strong>Reference</strong>:</p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2566990, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': 'A large financial company has recently adopted a hybrid cloud architecture to integrate their on-premises data center and their AWS Cloud. Your manager has instructed you to create a new VPC network topology which must support all Internet-facing web applications as well as the internally-facing applications that is accessed by employees only over VPN. To ensure high-availability and fault tolerance, both of their Internet-facing and internally-facing financial applications must be able to leverage at least two AZs for high availability. What is the minimum number of subnets that you must create within your VPC to accommodate these requirements?', 'id': 9831678, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SQS', 'prompt': {'relatedLectureIds': '', 'answers': ['Use an SQS queue to decouple the application components', 'Keep one extra Spot EC2 instance always ready in case a spike occurs.', 'Use larger instances for your application', 'Pre-warm your Elastic Load Balancer'], 'feedbacks': ['', '', '', ''], 'question': 'A manufacturing company has EC2 instances running in AWS. The EC2 instances are configured with Auto Scaling. There are a lot of requests being lost because of too much load on the servers. The Auto Scaling is launching new EC2 instances to take the load accordingly yet, there are still some requests that are being lost. <br><br>Which of the following is the most cost-effective solution to avoid losing recently submitted requests?', 'explanation': '<p>In this scenario, Amazon SQS is the best solution to avoid having lost messages.</p> <p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications. Building applications from individual components that each perform a discrete function improves scalability and reliability, and is best practice design for modern applications. SQS makes it simple and cost-effective to decouple and coordinate the components of a cloud application. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be always available.</p> <p>Options 2 and 3 are wrong because using a Spot or a larger EC2 instance would not prevent data from being lost in case of a larger spike. You can take advantage of the durability and elasticity of SQS to keep the messages available for consumption by your instances.</p> <p>Option 4 is incorrect because it would be difficult to predict how much traffic your load balancer will be receiving in a certain period of time, which corresponds to how long you will pre-warm the load balancer. It is still better to use SQS&nbsp;in this scenario rather than re-configuring your load balancer.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/sqs/" target="_blank" rel="noopener">https://aws.amazon.com/sqs/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566992, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': 'A manufacturing company has EC2 instances running in AWS. The EC2 instances are configured with Auto Scaling. There are a lot of requests being lost because of too much load on the servers. The Auto Scaling is launching new EC2 instances to take the load accordingly yet, there are still some requests that are being lost. Which of the following is the most cost-effective solution to avoid losing recently submitted requests?', 'id': 9831680, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['In the Security Group, add an Inbound HTTP rule.', ' In the Security Group, remove the SSH rule.', 'In the Route table, add this new route entry: 0.0.0.0 -&gt; igw-b51618cc', 'In the Route table, add this new route entry: 10.0.0.0/27 -&gt; local'], 'feedbacks': ['', '', '', ''], 'question': '<p>You have an On-Demand EC2 instance located in a subnet in AWS which hosts a web application. The security group attached to this EC2 instance has the following Inbound Rules:<br><img src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-01-30_05-56-46-caf703faa13a0b156726e3dbd9b2f558.png">\u200b <br><br>The Route table attached to the VPC is shown below. You can establish an SSH connection into the EC2 instance from the internet. However, you are not able to connect to the web server using your Chrome browser.<br><img src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-01-30_05-56-46-3907edec8b91dda5adcc1d2073c61658.png">\u200b<br><br>Which of the below steps would resolve the issue?</p>', 'explanation': '<p>The scenario is that you can already connect to the EC2 instance via SSH. This means that there is no problem in the Route Table of your VPC. To fix this issue, you simply need to update your Security Group and&nbsp;add an Inbound rule to allow HTTP traffic.</p> <p>Option 2 is incorrect as removing the SSH rule will not solve the issue. It will just disable SSH traffic that is already available.</p> <p>Options 3 and 4 are incorrect as there is no need to change the Route Tables.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2566994, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': 'You have an On-Demand EC2 instance located in a subnet in AWS which hosts a web application. The security group attached to this EC2 instance has the following Inbound Rules:\u200b The Route table attached to the VPC is shown below. You can establish an SSH connection into the EC2 instance from the internet. However, you are not able to connect to the web server using your Chrome browser.\u200bWhich of the below steps would resolve the issue?', 'id': 9831682, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SWF', 'prompt': {'relatedLectureIds': '', 'answers': ['It defines all the activities in the workflow.', 'It tells the decider the state of the workflow execution.', 'It tells the worker to perform a function.', 'It represents a single task in the workflow.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a startup as its AWS Chief Architect. You are currently assigned on a project that develops an online registration platform for events, which uses Simple Workflow for complete control of your orchestration logic. A decider ingests the customer name, address, contact number, and email address while the activity workers update the customer with the status of their online application status via email. Recently, you were having problems with your online registration platform which was solved by checking the decision task of your workflow.\xa0 \xa0</p><p>In SWF, what is the purpose of a decision task?</p>', 'explanation': '<p>Option 2 is correct. The decider can be viewed as a special type of worker. Like workers, it can be written in any language and asks Amazon SWF for tasks. However, it handles special tasks called decision tasks.</p> <p>Amazon SWF issues decision tasks whenever a workflow execution has transitions such as an activity task completing or timing out. A decision task contains information on the inputs, outputs, and current state of previously initiated activity tasks. Your decider uses this data to decide the next steps, including any new activity tasks, and returns those to Amazon SWF. Amazon SWF in turn enacts these decisions, initiating new activity tasks where appropriate and monitoring them.</p> <p>By responding to decision tasks in an ongoing manner, the decider controls the order, timing, and concurrency of activity tasks and consequently the execution of processing steps in the application. SWF issues the first decision task when an execution starts. From there on, Amazon SWF enacts the decisions made by your decider to drive your execution. The execution continues until your decider makes a decision to complete it.</p> <p>Option 1 is incorrect because this is the definition of a workflow in SWF.</p> <p>Option 3 is incorrect because this is the definition of an activity task.</p> <p>Option 4 is incorrect because this is the definition of an SWF task.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/swf/faqs/">https://aws.amazon.com/swf/faqs/</a></p> <p><a href="http://docs.aws.amazon.com/amazonswf/latest/developerguide/swf-dg-dev-deciders.html">http://docs.aws.amazon.com/amazonswf/latest/developerguide/swf-dg-dev-deciders.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SWF Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-amazon-swf/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-amazon-swf/</a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567012, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working for a startup as its AWS Chief Architect. You are currently assigned on a project that develops an online registration platform for events, which uses Simple Workflow for complete control of your orchestration logic. A decider ingests the customer name, address, contact number, and email address while the activity workers update the customer with the status of their online application status via email. Recently, you were having problems with your online registration platform which was solved by checking the decision task of your workflow.\xa0 \xa0In SWF, what is the purpose of a decision task?', 'id': 9831698, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Reserved Instances', 'On-Demand instances', 'Spot instances', 'Dedicated Hosts'], 'feedbacks': ['', '', '', ''], 'question': 'You are a Solutions Architect in your company where you are tasked to set up a cloud infrastructure. In the planning, it was discussed that you will need two EC2 instances which should continuously run for three years. The CPU utilization of the EC2 instances is also expected to be stable and predictable.<br><br>Which is the most cost-efficient Amazon EC2 Pricing type that is most appropriate for this scenario?', 'explanation': '<p>Reserved Instances provide you with a significant discount (up to 75%) compared to On-Demand instance pricing. In addition, when Reserved Instances are assigned to a specific Availability Zone, they provide a capacity reservation, giving you additional confidence in your ability to launch instances when you need them.</p> <p>For applications that have steady state or predictable usage, Reserved Instances can provide significant savings compared to using On-Demand instances.</p> <p>Reserved Instances are recommended for:</p> <ul> <li>-Applications with steady state usage</li> <li>-Applications that may require reserved capacity</li> <li>-Customers that can commit to using EC2 over a 1 or 3 year term to reduce their total computing costs</li> </ul> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/ec2/pricing/">https://aws.amazon.com/ec2/pricing/</a></p> <p><a href="https://aws.amazon.com/ec2/pricing/reserved-instances/">https://aws.amazon.com/ec2/pricing/reserved-instances/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567002, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': 'You are a Solutions Architect in your company where you are tasked to set up a cloud infrastructure. In the planning, it was discussed that you will need two EC2 instances which should continuously run for three years. The CPU utilization of the EC2 instances is also expected to be stable and predictable.Which is the most cost-efficient Amazon EC2 Pricing type that is most appropriate for this scenario?', 'id': 9831688, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Multiple Availability Zones', 'AWS Direct Connect', 'EC2 Dedicated Instances', 'Placement Groups'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for an aerospace manufacturer which heavily uses AWS. They are running a cluster of multi-tier applications that spans multiple servers for your wind simulation model and how it affects your state-of-the-art wing design. Currently, you are experiencing a slowdown in your applications and upon further investigation, it was discovered that it is due to latency issues.\xa0 \xa0</p><p>Which of the following EC2 features should you use to optimize performance for a compute cluster that requires low network latency?\xa0 </p>', 'explanation': '<p>You can launch EC2 instances in a placement group, which determines how instances are placed on underlying hardware. When you create a placement group, you specify one of the following strategies for the group:</p> <ol> <li><strong>Cluster</strong> - clusters instances into a low-latency group in a single Availability Zone</li> <li><strong>Spread</strong> - spreads instances across underlying hardware</li> </ol> <p>Option 1 is incorrect because multiple availability zones are mainly used for achieving high availability when one of the AWS AZ&rsquo;s goes down, and are not used for low network latency. Use Spread Placement Groups instead for multiple availability zones.</p> <p>Option 2 is incorrect because Direct Connect bypasses the public Internet and establishes a secure, dedicated connection from your on-premises data center into AWS, and not for having low latency within your AWS network. Use Placement Groups instead for low network latency.</p> <p>Option 3 is incorrect because EC2 Dedicated Instances are EC2 instances that run in a VPC on hardware that is dedicated to a single customer and are physically isolated at the host hardware level from instances that belong to other AWS accounts. It is not used for reducing latency.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2566996, '_class': 'assessment', 'updated': '2019-05-15T01:23:24Z', 'created': '2019-05-15T01:23:24Z', 'question_plain': 'You are working as a Solutions Architect for an aerospace manufacturer which heavily uses AWS. They are running a cluster of multi-tier applications that spans multiple servers for your wind simulation model and how it affects your state-of-the-art wing design. Currently, you are experiencing a slowdown in your applications and upon further investigation, it was discovered that it is due to latency issues.\xa0 \xa0Which of the following EC2 features should you use to optimize performance for a compute cluster that requires low network latency?', 'id': 9831684, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Kinesis', 'prompt': {'relatedLectureIds': '', 'answers': ['There is a problem in the sensors. They probably had some intermittent connection hence, the data is not sent to the stream.', 'By default, Amazon S3 stores the data for 1 day and moves it to Amazon Glacier.', 'Your AWS account was hacked and someone has deleted some data in your Kinesis stream.', '<p>By default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You installed sensors to track the number of visitors that goes to the park. The data is sent everyday to an Amazon Kinesis stream with default settings for processing, in which a consumer is configured to process the data every other day. You noticed that your S3 bucket is not receiving all of the data that is being sent to the Kinesis stream. You checked the sensors if they are\xa0properly sending the data to Amazon Kinesis and verified that the data is indeed\xa0sent everyday.<br><br>What could be the reason for this?</p>', 'explanation': '<p>Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence of data records meant to be written to and read from in real-time. Data records are therefore stored in shards in your stream temporarily.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2017/07/06/near_real_time_streaming_1.gif" alt="" width="657" height="217" /></p> <p>&nbsp;</p> <p>The time period from when a record is added to when it is no longer accessible is called the&nbsp;<em>retention period</em>. A Kinesis data stream stores records from <strong>24 hours by default</strong>&nbsp;to a maximum of 168 hours.</p> <p>This is the reason why there are missing data in your S3 bucket. To fix this, you can either configure your sensors to send the data everyday instead of every other day or alternatively, you can increase the retention period of your Kinesis data stream.</p> <p>Option 1 is incorrect. You already verified that the sensors are working as they should be hence, this is not the root cause of the issue.</p> <p>Option 2 is incorrect because by default, Amazon S3 does not store the data for 1 day and moves it to Amazon Glacier. Take note that the minimum number of days before you can transfer objects from S3 to Glacier is 30 days.</p> <p>Option 3 is incorrect because although this could be a possibility, you should verify first if there are other more probable reasons for the missing data in your S3 bucket. Be sure to follow and apply security best practices as well to prevent being hacked by someone.&nbsp;By default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream and hence, Option 4&nbsp;depicts the root cause of this issue and not Option 3.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon Kinesis Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567004, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You installed sensors to track the number of visitors that goes to the park. The data is sent everyday to an Amazon Kinesis stream with default settings for processing, in which a consumer is configured to process the data every other day. You noticed that your S3 bucket is not receiving all of the data that is being sent to the Kinesis stream. You checked the sensors if they are\xa0properly sending the data to Amazon Kinesis and verified that the data is indeed\xa0sent everyday.What could be the reason for this?', 'id': 9831690, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['There was an issue with the Amazon EC2 API. Just resend the requests and these will be provisioned successfully.', 'By default, AWS allows you to provision a maximum of 20 instances per region. Select a different region and retry the failed request.', 'By default, AWS allows you to provision a maximum of 20 instances per Availability Zone. Select a different Availability Zone and retry the failed request.', 'There is a soft limit of 20 instances per region which is why subsequent requests failed. Just submit the limit increase form to AWS and retry the failed requests once approved.'], 'feedbacks': ['', '', '', ''], 'question': 'You are automating the creation of EC2 instances in your VPC. Hence, you wrote a python script to trigger the Amazon EC2 API to request 50 EC2 instances in a single Availability Zone. However, you noticed that after 20 successful requests, subsequent requests failed. <br><br>What could be a reason for this issue and how would you resolve it?', 'explanation': '<p>You are limited to running up to a total of 20 On-Demand instances across the instance family, purchasing 20 Reserved Instances and requesting Spot Instances per your dynamic Spot limit per region.&nbsp;If you wish to run more than 20 instances, complete the Amazon EC2 instance request form.</p> <p>Option 1 is incorrect. There is a soft limit of 20 instances that you can provision per region, which is why only 20 instances were started.</p> <p>Option 2 is incorrect. There is no need to select a different region since this limit can be increased after submitting a request form to AWS.</p> <p>Option 3 is incorrect because the&nbsp;maximum of 20 instances limit is set per region and not per Availability Zone.&nbsp;This can be increased after submitting a request form to AWS.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_ec2">https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_ec2</a></p> <p><a href="https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2">https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567006, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are automating the creation of EC2 instances in your VPC. Hence, you wrote a python script to trigger the Amazon EC2 API to request 50 EC2 instances in a single Availability Zone. However, you noticed that after 20 successful requests, subsequent requests failed. What could be a reason for this issue and how would you resolve it?', 'id': 9831692, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Redshift', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Amazon Redshift</p>', '<p>Amazon RDS</p>', '<p>Amazon Aurora</p>', '<p>DynamoDB</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as an IT Consultant for a large investment bank that generates large financial datasets with millions of rows. The data must be stored in a columnar fashion to reduce the number of disk I/O requests and reduce the amount of data needed to load from the disk. The bank has an existing third-party business intelligence application which will connect to the storage service and then generate daily and monthly financial reports for its clients around the globe.\xa0 </p><p>In this scenario, which is the best storage service to use to meet the requirement?</p>', 'explanation': '<p>Amazon Redshift is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake. Redshift delivers ten times faster performance than other data warehouses by using machine learning, massively parallel query execution, and columnar storage on high-performance disk.</p> <p>In this scenario, there is a requirement to have a storage service which will be used by a business intelligence application and where the data must be stored in a columnar fashion. Business Intelligence reporting systems is a type of Online Analytical Processing (OLAP) which Redshift is known to support. In addition, Redshift also provides columnar storage unlike the other options. Hence, the correct answer in this scenario is Option 1: Amazon Redshift.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html ">https://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html </a></p> <p><a href="https://aws.amazon.com/redshift/">https://aws.amazon.com/redshift/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/</span></a></p> <p>&nbsp;</p> <p><strong>Here is a case study on finding the most suitable analytical tool - Kinesis vs EMR vs Athena vs Redshift:</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/wEOm6aiN4ww" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'}, 'correct_response': ['a'], 'original_assessment_id': 2567022, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working as an IT Consultant for a large investment bank that generates large financial datasets with millions of rows. The data must be stored in a columnar fashion to reduce the number of disk I/O requests and reduce the amount of data needed to load from the disk. The bank has an existing third-party business intelligence application which will connect to the storage service and then generate daily and monthly financial reports for its clients around the globe.\xa0 In this scenario, which is the best storage service to use to meet the requirement?', 'id': 9831708, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>EBS Provisioned IOPS SSD</p>', '<p>EBS Throughput Optimized HDD</p>', '<p>EBS General Purpose SSD</p>', '<p>EBS Cold HDD</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A global online sports betting company has its popular web application hosted in AWS. They are planning to develop a new online portal for their new business venture and they hired you to implement the cloud architecture for a new online portal that will accept bets globally for world sports. You started to design the system with a relational database that runs on a single EC2 instance, which requires a single EBS volume that can support up to 30,000 IOPS.\xa0 \xa0</p><p>In this scenario, which Amazon EBS volume type can you use that will meet the performance requirements of this new online portal?</p>', 'explanation': '<p>The scenario requires a storage type for a relational database with a high IOPS performance. For these scenarios, SSD volumes are more suitable to use instead of HDD volumes. Remember that the dominant performance attribute of SSD is <strong>IOPS</strong> while HDD is <strong>Throughput</strong>.</p> <p>In the exam, always consider the difference between SSD and HDD as shown on the table below. This will allow you to easily eliminate specific EBS-types in the options which are not SSD or not HDD, depending on whether the question asks for a storage type which has <strong><em>small, random</em></strong> I/O operations or <strong><em>large, sequential</em></strong> I/O operations.</p> <p>Since the requirement is 30,000 IOPS, you have to use an EBS type of Provisioned IOPS SSD, as this type can handle a maximum of 32,000 IOPS. This provides sustained performance for mission-critical low-latency workloads. Hence, Option 1 is the correct answer.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_22-34-15-d1fd30e8eaa8701ddd964e5878e78242.png" width="750" height="563" /></p> <p>&nbsp;</p> <p>Options 2 and 4 are incorrect because these are HDD volumes which are more suitable for large streaming workloads rather than transactional database workloads.</p> <p>Option 3 is incorrect because although a General Purpose SSD volume can be used for this scenario, it does not provide the high IOPS required by the application, unlike the Provisioned IOPS SSD volume.</p> <p>&nbsp;&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/ebs/details/">https://aws.amazon.com/ebs/details/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567008, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'A global online sports betting company has its popular web application hosted in AWS. They are planning to develop a new online portal for their new business venture and they hired you to implement the cloud architecture for a new online portal that will accept bets globally for world sports. You started to design the system with a relational database that runs on a single EC2 instance, which requires a single EBS volume that can support up to 30,000 IOPS.\xa0 \xa0In this scenario, which Amazon EBS volume type can you use that will meet the performance requirements of this new online portal?', 'id': 9831694, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['Create a new VPC peering connection between PROD and DEV with the appropriate routes.', 'Create a new entry to PROD in the DEV route table using the VPC peering connection as the target.', '<p>Change the DEV and PROD VPCs to have overlapping CIDR blocks to be able to connect them.</p>', '<p>Do nothing. Since these two VPCs are already connected via UAT, they already have a connection to each other.</p>'], 'feedbacks': ['', '', '', ''], 'question': 'A large insurance company has an AWS account that contains three VPCs (DEV, UAT and PROD) in the same region. UAT is peered to both PROD and DEV using a VPC peering connection. All VPCs have non-overlapping CIDR blocks. The company wants to push minor code releases from Dev to Prod to speed up time to market. <br><br>Which of the following options helps the company accomplish this?', 'explanation': '<p>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/peering/images/peering-intro-diagram.png" /></p> <p>&nbsp;</p> <p>AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck.&nbsp;</p> <p>Option 2 is incorrect because even if you configure the route tables, the two VPCs will still be disconnected until you set up a VPC peering connection between them.</p> <p>Option 3 is incorrect because you cannot peer two VPCs with overlapping CIDR blocks.</p> <p>Option 4 is incorrect as transitive VPC peering is not allowed hence, even though DEV and PROD are both connected in UAT, these two VPCs do not have a direct connection to each other.&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html</a></p> <p>&nbsp;</p> <p><strong>Check out these Amazon VPC and VPC Peering Cheat Sheets:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></span></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-vpc-peering/">https://tutorialsdojo.com/aws-cheat-sheet-vpc-peering/</a></span></p> <p>&nbsp;</p> <p><strong>Here is a quick introduction to VPC Peering:</strong></p> <p><iframe src="https://www.youtube.com/embed/i1A1eH8vLtk" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567010, '_class': 'assessment', 'updated': '2019-05-15T01:23:28Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'A large insurance company has an AWS account that contains three VPCs (DEV, UAT and PROD) in the same region. UAT is peered to both PROD and DEV using a VPC peering connection. All VPCs have non-overlapping CIDR blocks. The company wants to push minor code releases from Dev to Prod to speed up time to market. Which of the following options helps the company accomplish this?', 'id': 9831696, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'relatedLectureIds': '', 'answers': ['Yes', 'Only with Microsoft SQL Server-based RDS', 'Only for Oracle RDS instances', 'No'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for an insurance firm as their Senior Solutions Architect. The firm has an application which processes thousands of customer data stored in an Amazon MySQL database with Multi-AZ deployments configuration for high availability in case of downtime. For the past few days, you noticed an increasing trend of read and write operations, which is increasing the latency of the queries to your database. You are planning to use the standby database instance to balance the read and write operations from the primary instance.\xa0 </p><p>When running your primary Amazon RDS Instance as a Multi-AZ deployment, can you use the standby instance for read and write operations?\xa0 </p>', 'explanation': '<p>The answer is No. The standby instance&nbsp;will not perform any read and write operations while the primary instance is running. Hence, Option 4 is the correct answer.</p> <p>Multi-AZ deployments for the MySQL, MariaDB, Oracle, and PostgreSQL engines utilize synchronous physical replication to keep data on the standby up-to-date with the primary. Multi-AZ deployments for the SQL Server engine use synchronous logical replication to achieve the same result, employing SQL Server-native Mirroring technology. Both approaches safeguard your data in the event of a DB Instance failure or loss of an Availability Zone.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/con-multi-AZ.png" /></p> <p>&nbsp;</p> <p>If a storage volume on your primary instance fails in a Multi-AZ deployment, Amazon RDS automatically initiates a failover to the up-to-date standby (or to a replica in the case of Amazon Aurora). Compare this to a Single-AZ deployment: in case of a Single-AZ database failure, a user-initiated point-in-time-restore operation will be required. This operation can take several hours to complete, and any data updates that occurred after the latest restorable time (typically within the last five minutes) will not be available.</p> <p>Options 1, 2 and 3 are incorrect because, regardless of the database engine, you cannot use a standby database for read and write operations.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/rds/details/multi-az/">https://aws.amazon.com/rds/details/multi-az/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567028, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working for an insurance firm as their Senior Solutions Architect. The firm has an application which processes thousands of customer data stored in an Amazon MySQL database with Multi-AZ deployments configuration for high availability in case of downtime. For the past few days, you noticed an increasing trend of read and write operations, which is increasing the latency of the queries to your database. You are planning to use the standby database instance to balance the read and write operations from the primary instance.\xa0 When running your primary Amazon RDS Instance as a Multi-AZ deployment, can you use the standby instance for read and write operations?', 'id': 9831714, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use SSL to encrypt the data while in transit to Amazon S3.</p>', 'Use Amazon S3 server-side encryption with customer-provided keys.', 'Use Amazon S3 bucket policies to restrict access to the data at rest.', 'Use Amazon S3 server-side encryption with EC2 key pair.', 'Encrypt the data on the client-side before ingesting to Amazon S3 using their own master key.'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>A music company is storing data on Amazon Simple Storage Service (S3). The company’s security policy requires that data are encrypted at rest. Which of the following methods can achieve this? (Choose 2)</p>', 'explanation': '<p>Data protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit by using SSL or by using client-side encryption. You have the following options for protecting data at rest in Amazon S3:</p> <p><strong>Use Server-Side Encryption</strong>&nbsp;&ndash; You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects.</p> <ol> <li>Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</li> <li>Use Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS)</li> <li>Use Server-Side Encryption with Customer-Provided Keys (SSE-C)</li> </ol> <p>&nbsp;</p> <p><strong>Use Client-Side Encryption</strong>&nbsp;&ndash; You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p> <ol> <li>Use Client-Side Encryption with AWS KMS&ndash;Managed Customer Master Key (CMK)</li> <li>Use Client-Side Encryption Using a Client-Side Master Key</li> </ol> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/security-center/SecurityBlog/bucket_policies_defense_s3.43e6c93a095f2f55b33b30276f4782ab9ec79f47.png" alt="" width="650" height="366" /></p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b', 'e'], 'original_assessment_id': 2567014, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'A music company is storing data on Amazon Simple Storage Service (S3). The company’s security policy requires that data are encrypted at rest. Which of the following methods can achieve this? (Choose 2)', 'id': 9831700, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Create an EBS Snapshot</p>', 'Enable EBS Encryption', '<p>Migrate the EC2 instances from the public to private subnet.</p>', 'Enable Amazon S3 Server-Side and Client-Side Encryption', '<p>Use AWS Shield and WAF</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working for an investment bank as their IT Consultant. You are working with their IT team to handle the launch of their digital wallet system. The applications will run on multiple EBS-backed EC2 instances which will store the logs, transactions, and billing statements of the user in an S3 bucket. Due to tight security and compliance requirements, you are exploring options on how to safely store sensitive data on the EBS volumes and S3.\xa0 \xa0</p><p>Which of the below options should be carried out on AWS when storing sensitive data? (Choose 2)</p>', 'explanation': '<p>Both Options 2 and 4 are correct. Amazon EBS encryption offers a simple encryption solution for your EBS volumes without the need to build, maintain, and secure your own key management infrastructure.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/security-center/SecurityBlog/bucket_policies_defense_s3.43e6c93a095f2f55b33b30276f4782ab9ec79f47.png" width="600" height="339" /></p> <p>&nbsp;</p> <p>In Amazon S3, data protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit by using SSL or by using client-side encryption. You have the following options to protect data at rest in Amazon S3.</p> <div> <ul type="disc"> <li> <p><strong>Use Server-Side Encryption</strong>&nbsp;&ndash; You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects.</p> </li> <li> <p><strong>Use Client-Side Encryption</strong>&nbsp;&ndash; You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p> </li> </ul> </div> <p>Option 1 is incorrect because taking EBS Snapshot is a backup solution of EBS. It does not provide security of data inside EBS volumes when executed.</p> <p>Option 3 is incorrect because the data you want to secure are those in EBS volumes and S3 buckets. Moving your EC2 instance to a private subnet involves a different matter of security practice, which does not achieve what you want in this scenario.</p> <p>Option 5 is incorrect because Shield and WAF protect you from common security threats for your web applications. However, what you are trying to achieve is securing and encrypting your data inside EBS and S3.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p> <p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['b', 'd'], 'original_assessment_id': 2567018, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working for an investment bank as their IT Consultant. You are working with their IT team to handle the launch of their digital wallet system. The applications will run on multiple EBS-backed EC2 instances which will store the logs, transactions, and billing statements of the user in an S3 bucket. Due to tight security and compliance requirements, you are exploring options on how to safely store sensitive data on the EBS volumes and S3.\xa0 \xa0Which of the below options should be carried out on AWS when storing sensitive data? (Choose 2)', 'id': 9831704, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['Use a single PUT request to upload the large file', 'Use Amazon Snowball', 'Use AWS Import/Export', 'Use Multipart Upload'], 'feedbacks': ['', '', '', ''], 'question': '<p>A document sharing website is using AWS as its cloud infrastructure. Free users can upload a total of 5 GB data while premium users can upload as much as 5 TB. Their application uploads the user files, which can have a max file size of 1 TB, to an S3 Bucket. </p><p>In this scenario, what is the best way for the application to upload the large files in S3?</p>', 'explanation': '<p>The total volume of data and number of objects you can store are unlimited. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes. The largest object that can be uploaded in a single PUT is 5 gigabytes. For objects larger than 100 megabytes, customers should consider using the Multipart Upload capability.</p> <p>The Multipart upload API enables you to upload large objects in parts. You can use this API to upload new large objects or make a copy of an existing object. Multipart uploading is a three-step process: you initiate the upload, you upload the object parts, and after you have uploaded all the parts, you complete the multipart upload. Upon receiving the complete multipart upload request, Amazon S3 constructs the object from the uploaded parts and you can then access the object just as you would any other object in your bucket.</p> <p>Option 1 is incorrect because the largest file size you can upload using a single PUT request is 5 GB. Files larger than this will fail to be uploaded.</p> <p>Option 2 is incorrect because Snowball is a migration tool that lets you transfer large amounts of data from your on-premises data center to AWS S3 and vice versa. This tool is not suitable for the given scenario. And when you provision Snowball, the device gets transported to you, and not to your customers. Therefore, you bear the responsibility of securing the device.</p> <p>Option 3 is incorrect because Import/Export is similar to AWS Snowball in such a way that it is meant to be used as a migration tool, and not for multiple customer consumption such as in the given scenario.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html</a></p> <p><a href="https://aws.amazon.com/s3/faqs/">https://aws.amazon.com/s3/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567016, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'A document sharing website is using AWS as its cloud infrastructure. Free users can upload a total of 5 GB data while premium users can upload as much as 5 TB. Their application uploads the user files, which can have a max file size of 1 TB, to an S3 Bucket. In this scenario, what is the best way for the application to upload the large files in S3?', 'id': 9831702, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['Setup a custom bucket policy limited to the Amazon S3 API in the Amazon Glacier archive “tutorialsdojobackup”.', 'A custom S3 bucket policy limited to the Amazon S3 API in “tutorialsdojobackup”.', 'A custom IAM user policy limited to the Amazon S3 API for the Amazon Glacier archive “tutorialsdojobackup”.', 'In IAM, setup a custom user policy for the third party software that is limited to the Amazon S3 API in the "tutorialsdojobackup" bucket.'], 'feedbacks': ['', '', '', ''], 'question': '<p>One of your clients wants to leverage on Amazon S3 and Amazon Glacier as part of their backup and archive infrastructure. They created a new S3 bucket called “tutorialsdojobackup”. To support this integration between AWS and their on-premises network, they decided to use a third-party software.<br><br>Which approach will limit the access of the third party software to the Amazon S3 bucket only and not to other AWS resources?</p>', 'explanation': '<p>In this scenario, you have to provide access to your VPC to the third party software by creating a new IAM user. Since you want to limit the access of the third party software, you can simply manage the available AWS resources that it can communicate with by setting up a custom user policy, which will only allow access to a specific S3 bucket.</p> <p>Options 1 and 3 are incorrect because you should be&nbsp;setting a user policy for S3, not Glacier.</p> <p>Option 2 is incorrect. It should be a user policy that is created, not a bucket policy, because you want to restrict the third-party app\'s access to the bucket and not the bucket itself.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/example-policies-s3.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-policies-s3.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/walkthrough1.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/walkthrough1.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567020, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'One of your clients wants to leverage on Amazon S3 and Amazon Glacier as part of their backup and archive infrastructure. They created a new S3 bucket called “tutorialsdojobackup”. To support this integration between AWS and their on-premises network, they decided to use a third-party software.Which approach will limit the access of the third party software to the Amazon S3 bucket only and not to other AWS resources?', 'id': 9831706, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Kinesis', 'prompt': {'relatedLectureIds': '', 'answers': ['Amazon S3', 'Amazon Redshift', 'Amazon SWF', 'Amazon Kinesis Data Streams'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a Social Media Analytics company as its head data analyst. You want to collect gigabytes of data per second from websites and social media feeds to gain insights from data generated by its offerings and continuously improve the user experience. To meet this design requirement, you have developed an application hosted on an Auto Scaling group of Spot EC2 instances which processes the data and stores the results to DynamoDB and Redshift.\xa0 \xa0</p><p>Which AWS service can you use to collect and process large streams of data records in real time?\xa0 </p>', 'explanation': '<p>Amazon Kinesis Data Streams is used to collect and process large streams of data records in real time. You can use Kinesis Data Streams for rapid and continuous data intake and aggregation. The type of data used includes IT infrastructure log data, application logs, social media, market data feeds, and web clickstream data. Because the response time for the data intake and processing is in real time, the processing is typically lightweight.</p> <p>The following diagram illustrates the high-level architecture of Kinesis Data Streams. The producers continually push data to Kinesis Data Streams, and the consumers process the data in real time. Consumers (such as a custom application running on Amazon EC2 or an Amazon Kinesis Data Firehose delivery stream) can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-11-12_22-17-34-e104581ef63b79cbd80887be57b24ecd.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because S3 is mainly used for object storage of frequently and infrequently accessed files with high durability. It does not meet the requirement of being able to collect and process large streams of data in real time. You have to use Kinesis data streams instead.</p> <p>Option 2 is incorrect because Amazon Redshift is mainly used for data warehousing making it simple and cost-effective to analyze your data across your data warehouse and data lake. Again, it does not meet the requirement of being able to collect and process large streams of data real time.</p> <p>Option 3 is incorrect because SWF is mainly used to build applications that use Amazon\'s cloud to coordinate work across distributed components and not used as a way to process large streams of data records.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/streams/latest/dev/introduction.html">https://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon Kinesis Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567032, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working for a Social Media Analytics company as its head data analyst. You want to collect gigabytes of data per second from websites and social media feeds to gain insights from data generated by its offerings and continuously improve the user experience. To meet this design requirement, you have developed an application hosted on an Auto Scaling group of Spot EC2 instances which processes the data and stores the results to DynamoDB and Redshift.\xa0 \xa0Which AWS service can you use to collect and process large streams of data records in real time?', 'id': 9831718, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudTrail', 'prompt': {'relatedLectureIds': '', 'answers': ['AWS CloudTrail', 'AWS VPC', 'AWS EC2', 'AWS Cloudwatch'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a startup company that has resources deployed on the AWS Cloud. Your company is now going through a set of scheduled audits by an external auditing firm for compliance.\xa0 \xa0</p><p>Which of the following services available in AWS can be utilized to help ensure the right information are present for auditing purposes?</p>', 'explanation': '<p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting.</p> <p>CloudTrail provides visibility into user activity by recording actions taken on your account. CloudTrail records important information about each action, including who made the request, the services used, the actions performed, parameters for the actions, and the response elements returned by the AWS service. This information helps you to track changes made to your AWS resources and troubleshoot operational issues. CloudTrail makes it easier to ensure compliance with internal policies and regulatory standards.</p> <p>Option&nbsp;2 is incorrect because a VPC is a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. It does not provide you the auditing information that were asked for in this scenario.</p> <p>Option 3 is incorrect because EC2 is a service that provides secure, resizable compute capacity in the cloud and does not provide the needed information in this scenario just like Option 2.</p> <p>Option 4 is incorrect because CloudWatch is a&nbsp;monitoring tool for your AWS resources. Like options 2 and 3, it does not provide the needed information to satisfy the requirement in the scenario.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudTrail Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567024, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working for a startup company that has resources deployed on the AWS Cloud. Your company is now going through a set of scheduled audits by an external auditing firm for compliance.\xa0 \xa0Which of the following services available in AWS can be utilized to help ensure the right information are present for auditing purposes?', 'id': 9831710, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['The underlying host for the instance is possibly changed.', 'The ENI (Elastic Network Interface) is detached.', 'All data on the attached instance-store devices will be lost.', '<p>The Elastic IP address is disassociated with the instance.</p>', 'There will be no changes.'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working for a FinTech startup as their AWS Solutions Architect. You deployed an application on different EC2 instances with Elastic IP addresses attached for easy DNS resolution and configuration. These servers are only accessed from 8 AM to 6 PM and can be stopped from 6 PM to 8 AM for cost efficiency using Lambda with the script that automates this based on tags.\xa0 \xa0</p><p>Which of the following will occur when an EC2-VPC instance with an associated Elastic IP is stopped and started? (Choose 2)\xa0 </p>', 'explanation': '<p>This question did not mention the specific type of EC2 instance however, it says that it will be stopped and started. Since only EBS-backed instances can be stopped and restarted, it is implied that the instance is EBS-backed. Remember that an&nbsp;instance store-backed instance can only be rebooted or terminated and its data will be erased if the EC2 instance is terminated.</p> <p>If you stopped an EBS-backed EC2 instance, the volume is preserved but the data in any attached Instance store volumes will be erased. Keep in mind that an EC2 instance has an underlying physical host computer. If the instance is stopped, AWS usually moves the instance to a new host computer. Your instance may stay on the same host computer if there are no problems with the host computer. In addition, its Elastic IP address is disassociated from the instance if it is an EC2-Classic instance. Otherwise, if it is an EC2-VPC instance, the Elastic IP address remains associated.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_lifecycle.png" />&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2567026, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working for a FinTech startup as their AWS Solutions Architect. You deployed an application on different EC2 instances with Elastic IP addresses attached for easy DNS resolution and configuration. These servers are only accessed from 8 AM to 6 PM and can be stopped from 6 PM to 8 AM for cost efficiency using Lambda with the script that automates this based on tags.\xa0 \xa0Which of the following will occur when an EC2-VPC instance with an associated Elastic IP is stopped and started? (Choose 2)', 'id': 9831712, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['Amazon EBS volume', 'Amazon S3', 'Amazon EC2 instance store', 'Amazon RDS instance'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a large IT consultancy company as a Solutions Architect. One of your clients is launching a file sharing web application in AWS which requires a durable storage service for hosting their static contents such as PDFs, Word Documents, high resolution images and many others. </p><p>Which type of storage service should you use to meet this requirement?</p>', 'explanation': '<p>Amazon S3 is storage for the Internet. It&rsquo;s a simple storage service that offers software developers a durable, highly-scalable, reliable, and low-latency data storage infrastructure at very low costs. Amazon S3 provides customers with a highly durable storage infrastructure. Versioning offers an additional level of protection by providing a means of recovery when customers accidentally overwrite or delete objects.&nbsp;Remember that the scenario requires a <strong><em>durable</em></strong><em> storage for </em><strong><em>static</em></strong><em> content. </em>These two keywords are actually referring to&nbsp;S3, since it is highly durable and suitable for storing static content. Hence, Option 2 is correct.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-27_08-08-30-95c0c6fa077cd4d2c0dc4dd23c98ef09.png" alt="" width="700" height="525" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because Amazon EBS volume is not as durable compared with S3. In addition, it is best to store the static contents in S3 rather than EBS.</p> <p>Option 3 is incorrect because Amazon EC2 instance store is definitely not suitable - the data it holds will be wiped out immediately once the EC2 instance is restarted.</p> <p>Option 4 is incorrect because an RDS instance is just a database and not suitable for storing static content. By default, RDS is not durable, unless you launch it to be in Multi-AZ deployments configuration.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/s3/faqs/">https://aws.amazon.com/s3/faqs/</a>&nbsp;</p> <p data-pm-slice="1 1 []"><a href="https://d1.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf#page=24" target="_blank" rel="noopener noreferrer">https://d1.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf#page=24</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567030, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working for a large IT consultancy company as a Solutions Architect. One of your clients is launching a file sharing web application in AWS which requires a durable storage service for hosting their static contents such as PDFs, Word Documents, high resolution images and many others. Which type of storage service should you use to meet this requirement?', 'id': 9831716, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Glacier', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Retrieve the data using Amazon Glacier Select.</p>', '<p>Use Expedited Retrieval to access the financial data.</p>', '<p>Use Bulk Retrieval to access the financial data.</p>', '<p>Specify a range, or portion, of the financial data archive to retrieve.</p>', '<p>Purchase provisioned retrieval capacity.</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>An online stocks trading application that stores financial data in an S3 bucket has a lifecycle policy that moves older data to Glacier every month. There is a strict compliance requirement where a surprise audit can happen at anytime and you should be able to retrieve the required data in under 15 minutes under all circumstances. Your manager instructed you to ensure that retrieval capacity is available when you need it and should handle up to 150 MB/s of retrieval throughput.\xa0 \xa0</p><p>Which of the following should you do to meet the above requirement? (Choose 2)</p>', 'explanation': '<p>Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required. For all but the largest archives (250 MB+), data accessed using Expedited retrievals are typically made available within 1&ndash;5 minutes. Provisioned Capacity ensures that retrieval capacity for Expedited retrievals is available when you need it.</p> <p>To make an Expedited, Standard, or Bulk retrieval, set the Tier parameter in the Initiate Job (POST jobs) REST API request to the option you want, or the equivalent in the AWS CLI or AWS SDKs. If you have purchased provisioned capacity, then all expedited retrievals are automatically served through your provisioned capacity.</p> <p>Provisioned capacity ensures that your retrieval capacity for expedited retrievals is available when you need it. Each unit of capacity provides that at least three expedited retrievals can be performed every five minutes and provides up to 150 MB/s of retrieval throughput. You should purchase provisioned retrieval capacity if your workload requires highly reliable and predictable access to a subset of your data in minutes. Without provisioned capacity Expedited retrievals are accepted, except for rare situations of unusually high demand. However, if you require access to Expedited retrievals under all circumstances, you must purchase provisioned retrieval capacity.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/amazonglacier/latest/dev/images/gl-purchase-provisoned-capacity.png" alt="" width="600" height="255" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because Amazon Glacier Select is not an archive retrieval option and is primarily used to perform filtering operations using simple Structured Query Language (SQL) statements directly on your data archive in Glacier.</p> <p>Option 3 is incorrect because Bulk retrievals typically complete within 5&ndash;12 hours hence, this does not satisfy the requirement of retrieving the data within 15 minutes. The provisioned capacity option is also not compatible with Bulk retrievals.</p> <p>Option 4 is incorrect because using ranged archive retrievals is not enough to meet the requirement of retrieving the whole archive in the given timeframe. In addition, it does not provide additional retrieval capacity which is what the provisioned capacity option can offer.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html ">https://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html </a></p> <p><a href="https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html">https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Glacier Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-glacier/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-glacier/</span></a></p>'}, 'correct_response': ['b', 'e'], 'original_assessment_id': 2567034, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'An online stocks trading application that stores financial data in an S3 bucket has a lifecycle policy that moves older data to Glacier every month. There is a strict compliance requirement where a surprise audit can happen at anytime and you should be able to retrieve the required data in under 15 minutes under all circumstances. Your manager instructed you to ensure that retrieval capacity is available when you need it and should handle up to 150 MB/s of retrieval throughput.\xa0 \xa0Which of the following should you do to meet the above requirement? (Choose 2)', 'id': 9831720, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['Use Amazon Glacier instead.', 'Add a random prefix to the key names.', 'Do nothing. Amazon S3 will automatically manage performance at this scale.', 'Use a predictable naming scheme in the key names such as sequential numbers or date time sequences.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as an IT Consultant for a large media company where you are tasked to design a web application that stores static assets in an Amazon Simple Storage Service (S3) bucket. You expect this S3 bucket to immediately receive over 2000 PUT requests and 3500 GET requests per second at peak hour. <br><br>What should you do to ensure optimal performance?</p>', 'explanation': '<p>Amazon S3 now provides increased performance to support at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data, which can save significant processing time for no additional charge.&nbsp;Each S3 prefix can support these request rates, making it simple to increase performance significantly.</p> <div class="aws-text-box section"> <div class="  "> <p>Applications running on Amazon S3 today will enjoy this performance improvement with no changes, and customers building new applications on S3 do not have to make any application customizations to achieve this performance. Amazon S3\'s support for parallel requests means you can scale your S3 performance by the factor of your compute cluster, without making any customizations to your application. Performance scales per prefix, so you can use as many prefixes as you need in parallel to achieve the required throughput. There are no limits to the number of prefixes.</p> </div> </div> <div class="aws-text-box section"> <div class="  "> <p>This S3 request rate performance increase removes any previous guidance to randomize object prefixes to achieve faster performance. That means you can now use logical or sequential naming patterns in S3 object naming without any performance implications. This improvement is now available in all&nbsp;AWS Regions.&nbsp;</p> </div> </div> <p>Option 1 is incorrect because it is an archival/long term storage solution, which is not optimal if you are serving objects frequently and fast retrieval is a must.</p> <p>Option 2 is incorrect. Adding a random prefix is not required in this scenario because S3 can now scale automatically to adjust perfomance. You do not need to add a random prefix anymore for this purpose since S3 has increased performance to support at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data, which covers the workload in the scenario.</p> <p>Option 4 is incorrect because Amazon S3 already maintains an index of object key names in each AWS region. S3 stores key names in alphabetical order. The key name dictates which partition the key is stored in. Using a sequential prefix increases the likelihood that Amazon S3 will target a specific partition for a large number of your keys, overwhelming the I/O capacity of the partition.</p> <p>&nbsp;&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html</a></p> <p><a href="https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/">https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567036, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working as an IT Consultant for a large media company where you are tasked to design a web application that stores static assets in an Amazon Simple Storage Service (S3) bucket. You expect this S3 bucket to immediately receive over 2000 PUT requests and 3500 GET requests per second at peak hour. What should you do to ensure optimal performance?', 'id': 9831722, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['FTP', 'AWS Direct Connect', 'Amazon S3 Transfer Acceleration', '<p>Use CloudFront Origin Access Identity</p>'], 'feedbacks': ['', '', '', ''], 'question': "<p>You are working as a Solutions Architect for a multinational financial firm. They have a global online trading platform in which the users from all over the world regularly upload terabytes of transactional data to a centralized S3 bucket.\xa0 What AWS feature should you use in your present system to improve throughput and ensure consistently fast data transfer to the Amazon S3 bucket, regardless of your user's location?</p>", 'explanation': '<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. Transfer Acceleration leverages Amazon CloudFront&rsquo;s globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/product-marketing/Storage/Cloud%20Data%20Migration/Data-Migration_Thumbnail.feca76e59885394411e50c92bcda19c4d05b733d.jpg" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because the File Transfer Protocol does not guarantee fast throughput and consistent, fast data transfer.</p> <p>Option 2 is incorrect because you have users all around the world and not just on your on-premises data center. Direct Connect would be too costly and is definitely not suitable for this purpose.</p> <p>Option 4 is incorrect because this is a feature which ensures that only CloudFront can serve S3 content. It does not increase throughput and ensure fast delivery of content to your customers.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</a></span></p> <p>&nbsp;</p> <p><strong>S3 Transfer Acceleration vs Direct Connect vs VPN vs Snowball vs Snowmobile:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567038, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': "You are working as a Solutions Architect for a multinational financial firm. They have a global online trading platform in which the users from all over the world regularly upload terabytes of transactional data to a centralized S3 bucket.\xa0 What AWS feature should you use in your present system to improve throughput and ensure consistently fast data transfer to the Amazon S3 bucket, regardless of your user's location?", 'id': 9831724, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['Use an EC2 instance and a scheduled job to transfer the obsolete data from their S3 location to Amazon Glacier.', 'Use Lifecycle Policies in S3 to move obsolete data to Glacier.', 'Use AWS SQS.', 'Use AWS SWF.'], 'feedbacks': ['', '', '', ''], 'question': 'You are a new Solutions Architect working for a financial company. Your manager wants to have the ability to automatically transfer obsolete data from their S3 bucket to a low cost storage system in AWS. <br><br>What is the best solution you can provide to them?', 'explanation': '<p>In this scenario, you can use lifecycle policies in S3 to automatically move obsolete data to Glacier.</p> <p>Lifecycle configuration in Amazon S3 enables you to specify the lifecycle management of objects in a bucket. The configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects. These actions can be classified as follows:</p> <ul> <li><strong>Transition actions</strong>&nbsp;&ndash; In which you define when objects transition to another storage class.&nbsp;For example, you may choose to transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation, or archive objects to the GLACIER storage class one year after creation.</li> <li><strong>Expiration actions</strong>&nbsp;&ndash; In which you specify when the objects expire. Then Amazon S3 deletes the expired objects on your behalf.</li> </ul> <p>&nbsp;</p> <p>Option 1 is incorrect because you don\'t need to create a scheduled job in EC2 as you can just simply use the lifecycle policy in S3.&nbsp;</p> <p>Options 3 and 4 are incorrect as SQS and SWF are not storage services.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p> <p><a href="https://aws.amazon.com/blogs/aws/archive-s3-to-glacier/">https://aws.amazon.com/blogs/aws/archive-s3-to-glacier/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567040, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are a new Solutions Architect working for a financial company. Your manager wants to have the ability to automatically transfer obsolete data from their S3 bucket to a low cost storage system in AWS. What is the best solution you can provide to them?', 'id': 9831726, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Security', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use Dedicated EC2 instances to ensure that each instance has the maximum performance possible.</p>', '<p>Add multiple elastic network interfaces (ENIs) to each EC2 instance to increase the network bandwidth.</p>', 'Use an Amazon CloudFront service for distributing both static and dynamic content.', 'Use an Application Load Balancer with Auto Scaling groups for your EC2 instances then restrict direct Internet traffic to your Amazon RDS database by deploying to a private subnet.', '<p>Use AWS Shield and AWS WAF.</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are designing a social media website for a startup company and the founders want to know the ways to mitigate distributed denial-of-service (DDoS) attacks to their website.\xa0 \xa0</p><p>Which of the following are not viable mitigation techniques? (Choose 2)</p>', 'explanation': '<p>Take note that the question asks about the viable mitigation techniques to avoid Distributed Denial of Service (DDoS) attack.</p> <p>A Denial of Service (DoS) attack is an attack that can make your website or application unavailable to end users. To achieve this, attackers use a variety of techniques that consume network or other resources, disrupting access for legitimate end users.</p> <p>To protect your system from SoS attack, you can do the following:</p> <ol> <li>Use an Amazon CloudFront service for distributing both static and dynamic content.</li> <li>Use an Application Load Balancer with Auto Scaling groups for your EC2 instances then restrict direct Internet traffic to your Amazon RDS database by deploying to a private subnet.</li> <li>Setup alerts in Amazon CloudWatch to look for high <strong><code>Network In</code></strong>&nbsp;and CPU utilization metrics.</li> </ol> <p>Services that are available within AWS Regions, like Elastic Load Balancing and Amazon Elastic Compute Cloud (EC2), allow you to build Distributed Denial of Service resiliency and scale to handle unexpected volumes of traffic within a given region. Services that are available in AWS edge locations, like Amazon CloudFront, AWS WAF, Amazon Route53, and Amazon API Gateway, allow you to take advantage of a global network of edge locations that can provide your application with greater fault tolerance and increased scale for managing larger volumes of traffic.&nbsp;</p> <p>In addition, you can also use AWS Shield and AWS WAF to fortify your cloud network.&nbsp;AWS Shield is a managed DDoS protection service that is available in two tiers: Standard and Advanced. AWS Shield Standard applies always-on detection and inline mitigation techniques, such as deterministic packet filtering and priority-based traffic shaping, to minimize application downtime and latency.&nbsp;</p> <p>AWS WAF is a web application firewall that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. You can use AWS WAF to define customizable web security rules that control which traffic accesses your web applications. If you use AWS Shield Advanced, you can use AWS WAF at no extra cost for those protected resources and can engage the DRT to create WAF rules.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/aws-answers/answers-images/lb-app-ddos-mitigation.5fd8c473963ad36250ce5b01e631aaa6f657aff4.png" alt="" width="600" height="227" />&nbsp;</p> <p>&nbsp;</p> <p>Option 1 is correct because&nbsp;using Dedicated EC2 instances are just an instance billing option. Although it may ensure that each instance has the maximum performance possible, that by itself is not enough to mitigate a&nbsp;DDoS attack.</p> <p>Option 2 is correct as adding multiple elastic network interfaces (ENIs) to each EC2 instance to increase the network bandwidth is mainly done for performance improvement, and not for DDoS attack mitigation.</p> <p>Options 3, 4 and 5 are incorrect because they are valid mitigation techniques that can be used to prevent DDoS.</p> <p>&nbsp;</p> <p><strong>References</strong>:</p> <p><a href="https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/">https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/</a></p> <p><a href="https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf">https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf</a></p> <p>&nbsp;</p> <p><strong>Best practices on DDoS Attack Mitigation:</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/HnoZS5jj7pk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'}, 'correct_response': ['a', 'b'], 'original_assessment_id': 2567042, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are designing a social media website for a startup company and the founders want to know the ways to mitigate distributed denial-of-service (DDoS) attacks to their website.\xa0 \xa0Which of the following are not viable mitigation techniques? (Choose 2)', 'id': 9831728, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['Create a copy of the EBS volume using the CopyEBSVolume command.', 'Store a snapshot of the volume.', 'Download the content to an EC2 instance.', 'Back up the data into a physical disk.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a tech company that uses a lot of EBS volumes in their EC2 instances. An incident occurred that requires you to delete the EBS volumes and then re-create them again.\xa0 \xa0</p><p>What step should you do before you delete the EBS volumes?</p>', 'explanation': '<p>You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are&nbsp;<em>incremental</em>&nbsp;backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved.&nbsp;</p> <p>When you no longer need an Amazon EBS volume, you can delete it. After deletion, its data is gone and the volume can\'t be attached to any instance. However, before deletion, you can store a snapshot of the volume, which you can use to re-create the volume later.</p> <ul> <li>Option 1 is incorrect as there is no such thing as&nbsp;CopyEBSVolume command.</li> <li>Options 3 and 4 are wrong as these actions take a lot of time. The best and easiest way is to create a snapshot.</li> </ul> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-deleting-volume.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-deleting-volume.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567044, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working for a tech company that uses a lot of EBS volumes in their EC2 instances. An incident occurred that requires you to delete the EBS volumes and then re-create them again.\xa0 \xa0What step should you do before you delete the EBS volumes?', 'id': 9831730, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['A single facility in ap-southeast-1 and a single facility in eu-central-1', 'A single facility in ap-southeast-1 and a single facility in us-east-1', 'Multiple facilities in ap-southeast-1', 'A single facility in ap-southeast-1'], 'feedbacks': ['', '', '', ''], 'question': "One of your clients is leveraging on Amazon S3 in the ap-southeast-1 region to store their training videos for their employee onboarding process. The client is storing the videos using the Standard Storage class. <br><br>Where are your client's training videos replicated?", 'explanation': '<p>Amazon S3 runs on the world&rsquo;s largest global cloud infrastructure and was built from the ground up to deliver a customer promise of 99.999999999% durability. Data is automatically distributed across a minimum of three physical facilities that are geographically separated within an AWS Region, and Amazon S3 can also automatically replicate data to any other AWS Region.</p> <p>Since the question did not say that the&nbsp;Cross-region replication (CRR) is enabled, then the correct answer is Option 3. Amazon S3 replicates the data to multiple facilities in the same region where it is located, which is ap-southeast-1</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/s3/">https://aws.amazon.com/s3/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567046, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': "One of your clients is leveraging on Amazon S3 in the ap-southeast-1 region to store their training videos for their employee onboarding process. The client is storing the videos using the Standard Storage class. Where are your client's training videos replicated?", 'id': 9831732, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Lambda', 'prompt': {'relatedLectureIds': '', 'answers': ['<p><code>ReservedConcurrentExecutions</code></p>', '<p><code>Invocations</code> </p>', '<p><code>Errors</code> </p>', '<p><code>IteratorSize</code> </p>', '<p><code>Dead Letter Queue</code> </p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>Your company has recently deployed a new web application which uses a serverless-based architecture in AWS. Your manager instructed you to implement CloudWatch metrics to monitor your systems more effectively. You know that Lambda automatically monitors functions on your behalf and reports metrics through Amazon CloudWatch.\xa0 \xa0</p><p>In this scenario, what types of data do these metrics monitor? (Choose 2)</p>', 'explanation': '<p>AWS Lambda automatically monitors functions on your behalf, reporting metrics through Amazon CloudWatch. These metrics include total invocation requests, latency, and error rates. The throttles, Dead Letter Queues errors and Iterator age for stream-based invocations are also monitored.</p> <p>You can monitor metrics for Lambda and view logs by using the Lambda console, the CloudWatch console, the AWS CLI, or the CloudWatch API.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/lambda/latest/dg/images/metrics-functions-list.png" alt="" width="700" height="483" />&nbsp;</p> <p>Option 1 is incorrect because CloudWatch does not monitor Lambda\'s reserved concurrent executions. You can view it through the Lambda console or via CLI manually.</p> <p>Options 4 and 5 are incorrect because these two are not Lambda metrics.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-access-metrics.html">https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-access-metrics.html</a></p> <p><a href="https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-metrics.html">https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-metrics.html</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this AWS Lambda Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/</span></a></p>'}, 'correct_response': ['b', 'c'], 'original_assessment_id': 2567048, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'Your company has recently deployed a new web application which uses a serverless-based architecture in AWS. Your manager instructed you to implement CloudWatch metrics to monitor your systems more effectively. You know that Lambda automatically monitors functions on your behalf and reports metrics through Amazon CloudWatch.\xa0 \xa0In this scenario, what types of data do these metrics monitor? (Choose 2)', 'id': 9831734, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Reserved Instances', 'On-Demand Instances', 'Spot Instances', '<p>Scheduled Reserved Instances</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A company is hosting EC2 instances that are on non-production environment and processing non-priority batch loads, which can be interrupted at any time.\xa0 \xa0</p><p>What is the best instance purchasing option which can be applied to your EC2 instances in this case?\xa0 </p>', 'explanation': '<p>Amazon EC2 Spot instances are spare compute capacity in the AWS cloud available to you at steep discounts compared to On-Demand prices. It can be interrupted by AWS EC2 with two minutes of notification when the EC2 needs the capacity back.&nbsp;</p> <p>To use Spot Instances, you create a Spot Instance request that includes the number of instances, the instance type, the Availability Zone, and the maximum price that you are willing to pay per instance hour. If your maximum price exceeds the current Spot price, Amazon EC2 fulfills your request immediately if capacity is available. Otherwise, Amazon EC2 waits until your request can be fulfilled or until you cancel the request.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/spot_lifecycle.png" /></p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p> <p><a href="https://aws.amazon.com/ec2/spot/" target="_blank" rel="noopener">https://aws.amazon.com/ec2/spot/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567050, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'A company is hosting EC2 instances that are on non-production environment and processing non-priority batch loads, which can be interrupted at any time.\xa0 \xa0What is the best instance purchasing option which can be applied to your EC2 instances in this case?', 'id': 9831736, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SQS', 'prompt': {'relatedLectureIds': '', 'answers': ['Standard queues provide at-least-once delivery, which means that each message is delivered at least once.', 'Standard queues preserve the order of messages.', 'Amazon SQS can help you build a distributed application with decoupled components.', 'FIFO queues provide exactly-once processing.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a leading airline company where you are building a decoupled application in AWS using EC2, Auto Scaling group, S3 and SQS. You designed the architecture in such a way that the EC2 instances will consume the message from the SQS queue and will automatically scale up or down based on the number of messages in the queue.\xa0 \xa0</p><p>In this scenario, which of the following statements is false about SQS?</p>', 'explanation': '<p>All of the answers are correct except for option 2. Only FIFO queues can preserve the order of messages and not standard queues.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/sqs/faqs/">https://aws.amazon.com/sqs/faqs/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567070, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working as a Solutions Architect for a leading airline company where you are building a decoupled application in AWS using EC2, Auto Scaling group, S3 and SQS. You designed the architecture in such a way that the EC2 instances will consume the message from the SQS queue and will automatically scale up or down based on the number of messages in the queue.\xa0 \xa0In this scenario, which of the following statements is false about SQS?', 'id': 9831754, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Reserved instances', 'Spot instances', 'Dedicated instances', 'On-demand instances'], 'feedbacks': ['', '', '', ''], 'question': '<p>The media company that you are working for has a video transcoding application running on Amazon EC2. Each EC2 instance polls a queue to find out which video should be transcoded, and then runs a transcoding process. If this process is interrupted, the video will be transcoded by another instance based on the queuing system. This application has a large backlog of videos which need to be transcoded. Your manager would like to reduce this backlog by adding more EC2 instances, however, these instances are only needed until the backlog is reduced. </p><p>In this scenario, which type of Amazon EC2 instance is the most cost-effective type to use without sacrificing performance?</p>', 'explanation': '<p>You require an instance that will be used not as a primary server but as a spare compute resource to augment the transcoding process of your application. These instances should also be terminated once the backlog has been significantly reduced. In addition, the scenario mentions that&nbsp;if the current process is interrupted, the video can be transcoded by another instance based on the queuing system. This means that the application can gracefully handle&nbsp;an unexpected termination of an EC2 instance, like in the event of a Spot instance termination when the Spot price is greater than your set maximum price. Hence, an Amazon EC2 Spot instance is the best and cost-effective option for this scenario.</p> <p>&nbsp;</p> <p><img src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/spot_lifecycle.png" /></p> <p>&nbsp;</p> <p>Amazon EC2 Spot instances are <strong>spare</strong> compute capacity in the AWS cloud available to you at steep discounts compared to On-Demand prices. EC2 Spot enables you to optimize your costs on the AWS cloud and scale your application\'s throughput up to 10X for the same budget. By simply selecting Spot when launching EC2 instances, you can save up-to 90% on On-Demand prices. The only difference between On-Demand instances and Spot Instances is that Spot instances can be interrupted by EC2 with two minutes of notification when the EC2 needs the capacity back.&nbsp;</p> <p data-pm-slice="1 1 []">You can specify whether Amazon EC2 should hibernate, stop, or terminate Spot Instances when they are interrupted. You can choose the interruption behavior that meets your needs.</p> <p data-pm-slice="1 1 []">Take note that there is no <em>"bid price"</em> anymore for Spot EC2 instances&nbsp;<strong>since March 2018</strong>. You simply have to set your&nbsp;<strong>maximum price</strong>&nbsp;instead.</p> <p data-pm-slice="1 1 []">Options 1 and 3 are incorrect as Reserved and Dedicated instances do not act as spare compute capacity.</p> <p>Option 4 is a valid option but a Spot instance is much cheaper than On-Demand.&nbsp;</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html</a></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-spot-instances-work.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-spot-instances-work.html</a></p> <p data-pm-slice="1 1 []"><a href="https://aws.amazon.com/blogs/compute/new-amazon-ec2-spot-pricing">https://aws.amazon.com/blogs/compute/new-amazon-ec2-spot-pricing</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567052, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'The media company that you are working for has a video transcoding application running on Amazon EC2. Each EC2 instance polls a queue to find out which video should be transcoded, and then runs a transcoding process. If this process is interrupted, the video will be transcoded by another instance based on the queuing system. This application has a large backlog of videos which need to be transcoded. Your manager would like to reduce this backlog by adding more EC2 instances, however, these instances are only needed until the backlog is reduced. In this scenario, which type of Amazon EC2 instance is the most cost-effective type to use without sacrificing performance?', 'id': 9831738, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Enable DynamoDB Accelerator (DAX) and ensure that the Auto Scaling is enabled and increase the maximum provisioned read and write capacity.</p>', '<p>Configure CloudFront with DynamoDB as the origin; cache frequently accessed data on client device using ElastiCache.</p>', '<p>Use AWS SSO and Cognito to authenticate users and have them directly access DynamoDB using single-sign on. Manually set the provisioned read and write capacity to a higher RCU and WCU.</p>', '<p>Use API Gateway in conjunction with Lambda and turn on the caching on frequently accessed data and enable DynamoDB global replication.</p>', '<p>Since Auto Scaling is enabled by default, the provisioned read and write capacity will adjust automatically. Also enable DynamoDB Accelerator (DAX) to improve the performance from milliseconds to microseconds.</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You currently have an Augment Reality (AR) mobile game which has a serverless backend. It is using a DynamoDB table which was launched using the AWS CLI to store all the user data and information gathered from the players and a Lambda function to pull the data from DynamoDB. The game is being used by millions of users each day to read and store data. </p><p>How would you design the application to improve its overall performance and make it more scalable while keeping the costs low? (Choose 2)</p>', 'explanation': '<p>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement &ndash; from milliseconds to microseconds &ndash; even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://media.amazonwebservices.com/blog/2017/ddb_as_set_read_1.png" /></p> <p><br />Amazon API Gateway lets you create an API that acts as a "front door" for applications to access data, business logic, or functionality from your back-end services, such as code running on AWS Lambda. Amazon API Gateway handles all of the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management. Amazon API Gateway has no minimum fees or startup costs.</p> <p>AWS Lambda scales your functions automatically on your behalf. Every time an event notification is received for your function, AWS Lambda quickly locates free capacity within its compute fleet and runs your code. Since your code is stateless, AWS Lambda can start as many copies of your function as needed without lengthy deployment and configuration delays.</p> <p>Option 2 is incorrect because although CloudFront delivers content faster to your users using edge locations, you still cannot integrate DynamoDB table with CloudFront as these two are incompatible. In addition, DataSync is a data transfer service that automates moving data between on-premises storage and Amazon S3 or Amazon EFS. You should not be caching large amounts of data on a client\'s mobile, but rather on your side.</p> <p>Option 3 is incorrect because AWS Single Sign-On (SSO) is a cloud SSO service that just makes it easy to centrally manage SSO access to multiple AWS accounts and business applications. This will not be of much&nbsp;help on the scalability and performance of the application. It is costly to manually set the provisioned read and write capacity to a higher RCU and WCU because this capacity will run round the clock and will still be the same even if the incoming traffic is stable and there is no need to scale.</p> <p>Option 5 is incorrect because, by default, Auto Scaling is not enabled in a DynamoDB table which is created using the AWS CLI.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/lambda/faqs/">https://aws.amazon.com/lambda/faqs/</a></p> <p><a href="https://aws.amazon.com/api-gateway/faqs/">https://aws.amazon.com/api-gateway/faqs/</a></p> <p><a href="https://aws.amazon.com/dynamodb/dax/">https://aws.amazon.com/dynamodb/dax/</a></p>'}, 'correct_response': ['a', 'd'], 'original_assessment_id': 2567054, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You currently have an Augment Reality (AR) mobile game which has a serverless backend. It is using a DynamoDB table which was launched using the AWS CLI to store all the user data and information gathered from the players and a Lambda function to pull the data from DynamoDB. The game is being used by millions of users each day to read and store data. How would you design the application to improve its overall performance and make it more scalable while keeping the costs low? (Choose 2)', 'id': 9831740, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Two VPCs peered to a specific CIDR block in one VPC</p>', 'Transitive Peering', 'Edge to Edge routing via a gateway', 'One to one relationship between two Virtual Private Cloud networks', '<p>One VPC Peered with two VPCs using longest prefix match</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>An online job site is using NGINX for its application servers hosted in EC2 instances and MongoDB Atlas for its database-tier. MongoDB Atlas is a fully automated third-party cloud service which is not provided by AWS, but supports VPC peering to connect to your VPC.\xa0 </p><p>Which of the following items are invalid VPC peering configurations? (Choose 2)</p>', 'explanation': '<p>Options 2 and 3 are invalid VPC Peering configurations, while the other options are valid ones.</p> <p>The following VPC peering connection configurations are not supported.</p> <ol> <li>Overlapping CIDR Blocks</li> <li>Transitive Peering</li> <li>Edge to Edge Routing Through a Gateway or Private Connection</li> </ol> <p>&nbsp;</p> <p><strong>Overlapping CIDR Blocks </strong></p> <p>You cannot create a VPC peering connection between VPCs with matching or overlapping IPv4 CIDR blocks.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/peering/images/overlapping-cidrs-diagram.png" /></p> <p>If the VPCs have multiple IPv4 CIDR blocks, you cannot create a VPC peering connection if any of the CIDR blocks overlap (regardless of whether you intend to use the VPC peering connection for communication between the non-overlapping CIDR blocks only).&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/peering/images/overlapping-multiple-cidrs-diagram.png" /></p> <p>&nbsp;</p> <p>This limitation also applies to VPCs that have non-overlapping IPv6 CIDR blocks. Even if you intend to use the VPC peering connection for IPv6 communication only, you cannot create a VPC peering connection if the VPCs have matching or overlapping IPv4 CIDR blocks. Communication over IPv6 is not supported for an inter-region VPC peering connection.</p> <p><strong>Transitive Peering</strong></p> <p>You have a VPC peering connection between VPC A and VPC B (pcx-aaaabbbb), and between VPC A and VPC C (pcx-aaaacccc). There is no VPC peering connection between VPC B and VPC C. You cannot route packets directly from VPC B to VPC C through VPC A.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/peering/images/transitive-peering-diagram.png" /></p> <p>&nbsp;</p> <p><strong>Edge to Edge Routing Through a Gateway or Private Connection</strong></p> <p>If either VPC in a peering relationship has one of the following connections, you cannot extend the peering relationship to that connection:</p> <div> <ol> <li>A VPN connection or an AWS Direct Connect connection to a corporate network</li> <li>An internet connection through an internet gateway</li> <li>An internet connection in a private subnet through a NAT device</li> <li>A VPC endpoint to an AWS service; for example, an endpoint to Amazon S3.</li> <li>(IPv6) A ClassicLink connection. You can enable IPv4 communication between a linked EC2-Classic instance and instances in a VPC on the other side of a VPC peering connection. However, IPv6 is not supported in EC2-Classic, so you cannot extend this connection for IPv6 communication.</li> </ol> </div> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/peering/images/edge-to-edge-vpn-diagram.png" /></p> <p>For example, if VPC A and VPC B are peered, and VPC A has any of these connections, then instances in VPC B cannot use the connection to access resources on the other side of the connection. Similarly, resources on the other side of a connection cannot use the connection to access VPC B.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/invalid-peering-configurations.html">http://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/invalid-peering-configurations.html</a></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/peering/peering-configurations-partial-access.html">https://docs.aws.amazon.com/vpc/latest/peering/peering-configurations-partial-access.html</a></p> <p>&nbsp;</p> <p><strong>Check out these Amazon VPC and VPC Peering Cheat Sheets:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></span></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-vpc-peering/">https://tutorialsdojo.com/aws-cheat-sheet-vpc-peering/</a></span></p> <p>&nbsp;</p> <p><strong>Here is a quick introduction to VPC Peering:</strong></p> <p><iframe src="https://www.youtube.com/embed/i1A1eH8vLtk" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['b', 'c'], 'original_assessment_id': 2567058, '_class': 'assessment', 'updated': '2019-05-15T01:26:07Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'An online job site is using NGINX for its application servers hosted in EC2 instances and MongoDB Atlas for its database-tier. MongoDB Atlas is a fully automated third-party cloud service which is not provided by AWS, but supports VPC peering to connect to your VPC.\xa0 Which of the following items are invalid VPC peering configurations? (Choose 2)', 'id': 9831742, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['Initially, it will be allowed and then after a while, the connection will be denied.', 'Initially, it will be denied and then after a while, the connection will be allowed.', 'It will be allowed.', 'It will be denied.'], 'feedbacks': ['', '', '', ''], 'question': 'You are a new Solutions Architect in your company. Upon checking the existing Inbound Rules of your Network ACL, you saw this configuration:<br><p><img src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-02-03_09-36-25-e4a7214623d1499d801d9fe0f021596a.png"></p><br><br>If a computer with an IP address of 110.238.109.37 sends a request to your VPC, what will happen?', 'explanation': '<p>Rules are evaluated starting with the lowest numbered rule. As soon as a rule matches traffic, it\'s applied immediately regardless of any higher-numbered rule that may contradict it.</p> <p>We have 3 rules here:</p> <p>1. Rule 100 permits all traffic from any source.</p> <p>2. Rule 101 denies all traffic coming from 110.238.109.37</p> <p>3. The Default Rule (*) denies all traffic from any source.</p> <p>The Rule 100 will first be evaluated. If there is a match, then it will allow the request. Otherwise, it will then go to Rule 101 to repeat the same process until it goes to the default rule. In this case, when there is a request from 110.238.109.37, it will go through Rule 100 first. As Rule 100 says it will permit all traffic from any source, it will allow this request and will not further evaluate Rule 101 (which denies 110.238.109.37) nor the default rule.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567060, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are a new Solutions Architect in your company. Upon checking the existing Inbound Rules of your Network ACL, you saw this configuration:If a computer with an IP address of 110.238.109.37 sends a request to your VPC, what will happen?', 'id': 9831744, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['Add a new bucket policy on the Amazon S3 bucket.', 'Configure the lifecycle configuration rules on the Amazon S3 bucket to purge the transaction logs after a month ', 'Create a new IAM policy for the Amazon S3 bucket that automatically deletes the logs after a month', 'Enable CORS on the Amazon S3 bucket which will enable the automatic monthly deletion of data'], 'feedbacks': ['', '', '', ''], 'question': 'Your company has an e-commerce application that saves the transaction logs to an S3 bucket. You are instructed by the CTO to configure the application to keep the transaction logs for one month for troubleshooting purposes, and then afterwards, purge the logs. What should you do to accomplish this requirement?', 'explanation': '<p>In this scenario, the best way to accomplish the requirement is to simply configure the lifecycle configuration rules on the Amazon S3 bucket to purge the transaction logs after a month.&nbsp;</p> <p>Lifecycle configuration enables you to specify the lifecycle management of objects in a bucket. The configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects. These actions can be classified as follows:</p> <div> <ul> <li><strong>- Transition actions</strong>&nbsp;&ndash; In which you define when objects transition to another&nbsp;storage class. For example, you may choose to transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation, or archive objects to the GLACIER storage class one year after creation.</li> <li><strong>- Expiration actions</strong>&nbsp;&ndash; In which you specify when the objects expire. Then Amazon S3 deletes the expired objects on your behalf.</li> </ul> </div> <p>Option 1 is incorrect as adding a new policy does not provide a solution to any of your needs in this scenario. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it.</p> <p>Option 3 is incorrect because IAM policies are primarily used to specify what actions are allowed or denied on your S3 buckets. You cannot configure an IAM policy to automatically purge logs for you in any way.</p> <p>Option 4 is incorrect. CORS allows client web applications that are loaded in one domain to interact with resources in a different domain.&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567062, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'Your company has an e-commerce application that saves the transaction logs to an S3 bucket. You are instructed by the CTO to configure the application to keep the transaction logs for one month for troubleshooting purposes, and then afterwards, purge the logs. What should you do to accomplish this requirement?', 'id': 9831746, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Removes one or more security groups from a rule.', 'Removes one or more security groups from an Amazon EC2 instance.', 'Removes one or more ingress rules from a security group.', 'Removes one or more egress rules from a security group.'], 'feedbacks': ['', '', '', ''], 'question': '<p>A company is using a custom shell script to automate the deployment and management of their EC2 instances. The script is using various AWS CLI commands such as <code>revoke-security-group-ingress</code>, <code>revoke-security-group-egress</code>, <code>run-scheduled-instances</code> and many others.\xa0 \xa0</p><p>In the shell script, what does the <code>revoke-security-group-ingress</code> command do?</p>', 'explanation': '<p>The <strong>revoke-security-group-ingress</strong> command removes one or more ingress rules from a security group.&nbsp;</p> <p>Each rule consists of the protocol and the CIDR range or source security group. For the TCP and UDP protocols, you must also specify the destination port or range of ports. For the ICMP protocol, you must also specify the ICMP type and code. If the security group rule has a description, you do not have to specify the description to revoke the rule.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/userguide/images/security-diagram.png" alt="" width="472" height="506" /></p> <p>Rule changes are propagated to instances within the security group as quickly as possible. However, a small delay might occur. This example removes TCP port 22 access for the&nbsp;<tt>203.0.113.0/24</tt>&nbsp;address range from the security group named&nbsp;<tt>MySecurityGroup</tt>. If the command succeeds, no output is returned.</p> <p>Command:</p> <div> <pre>aws ec2 revoke-security-group-ingress --group-name MySecurityGroup --protocol tcp --port 22 --cidr 203.0.113.0/24<br /><br /></pre> </div> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/revoke-security-group-ingress.html">https://docs.aws.amazon.com/cli/latest/reference/ec2/revoke-security-group-ingress.html</a></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567064, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'A company is using a custom shell script to automate the deployment and management of their EC2 instances. The script is using various AWS CLI commands such as revoke-security-group-ingress, revoke-security-group-egress, run-scheduled-instances and many others.\xa0 \xa0In the shell script, what does the revoke-security-group-ingress command do?', 'id': 9831748, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Security', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use Amazon Data Lifecycle Manager.</p>', '<p>Configure the Network Access Control List of your VPC to permit ingress traffic over port 22 from your IP.</p>', 'Configure the Security Group of the EC2 instance to permit ingress traffic over port 3389 from your IP.', 'Configure the Security Group of the EC2 instance to permit ingress traffic over port 22 from your IP.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are unable to connect to your new EC2 instance via SSH from your home computer, which you have recently deployed. However, you were able to successfully access other existing instances in your VPC without any issues.\xa0 \xa0 </p><p>Which of the following should you check and possibly correct to restore connectivity?</p>', 'explanation': '<p>When connecting to your EC2 instance via SSH, you need to ensure that port 22 is allowed on the security group of your EC2 instance.</p> <p>A&nbsp;<em>security group</em>&nbsp;acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you associate one or more security groups with the instance. You add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.&nbsp;&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d2908q01vomqb2.cloudfront.net/cb4e5208b4cd87268b208e49452ed6e89a68e0b8/2017/10/19/Picture5.png" alt="" width="650" height="244" /></p> <p>Option 1 is incorrect because&nbsp;Amazon Data Lifecycle Manager is primarily used to manage the lifecycle of your AWS resources and not to allow certain traffic to go through.</p> <p>Option 2 is incorrect because configuring the Network Access Control List (Network ACL) is not necessary in this scenario as it was specified that you were able to connect to other EC2 instances. In addition, Network ACL is much suitable to control the traffic that goes in and out of your entire VPC and not just on one EC2 instance.</p> <p>Option 3 is incorrect because this is relevant to RDP and not SSH.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html</a></p> <p>&nbsp;</p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567066, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are unable to connect to your new EC2 instance via SSH from your home computer, which you have recently deployed. However, you were able to successfully access other existing instances in your VPC without any issues.\xa0 \xa0 Which of the following should you check and possibly correct to restore connectivity?', 'id': 9831750, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SWF', 'prompt': {'relatedLectureIds': '', 'answers': ['For a distributed session management for your mobile application.', 'Managing a multi-step and multi-decision checkout process of an e-commerce mobile app.', 'Orchestrating the execution of distributed business processes.', 'For applications that require a message queue.', 'For web applications that require content delivery networks.'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>A leading media company has an application hosted in an EBS-backed EC2 instance which uses Simple Workflow Service (SWF) to handle its sequential background jobs. The application works well in production and your manager asked you to also implement the same solution to other areas of their business.\xa0 \xa0</p><p>In which other scenarios can you use both Simple Workflow Service (SWF) and Amazon EC2 as a solution? (Choose 2)</p>', 'explanation': '<p>You can use a combination of EC2 and SWF for the following scenarios:</p> <ol> <li>Managing a multi-step and multi-decision checkout process of an e-commerce mobile app.</li> <li>Orchestrating the execution of distributed business processes</li> </ol> <p>Amazon Simple Workflow Service (SWF) is a web service that makes it easy to coordinate work across distributed application components. Amazon SWF enables applications for a range of use cases, including media processing, web application back-ends, business process workflows, and analytics pipelines, to be designed as a coordination of tasks. Tasks represent invocations of various processing steps in an application which can be performed by executable code, web service calls, human actions, and scripts.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/amazonswf/latest/developerguide/images/swf-overview-actors.png" alt="" width="500" height="409" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect as Elasticache is the best option for distributed session management.</p> <p>Option 4 is incorrect as SQS is the best service to use as a&nbsp;message queue.</p> <p>Option 5 is incorrect as CloudFront is the best option for applications that require a global content delivery network.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/swf/">https://aws.amazon.com/swf/</a></p> <p><a href="https://aws.amazon.com/ec2/">https://aws.amazon.com/ec2/</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this Amazon SWF Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-amazon-swf/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-amazon-swf/</a></p>'}, 'correct_response': ['b', 'c'], 'original_assessment_id': 2567068, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'A leading media company has an application hosted in an EBS-backed EC2 instance which uses Simple Workflow Service (SWF) to handle its sequential background jobs. The application works well in production and your manager asked you to also implement the same solution to other areas of their business.\xa0 \xa0In which other scenarios can you use both Simple Workflow Service (SWF) and Amazon EC2 as a solution? (Choose 2)', 'id': 9831752, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['Enable SSE on an S3 bucket to make use of AES-256 encryption', 'Store the data in encrypted EBS snapshots', 'Encrypt the data locally using your own encryption keys, then copy the data to Amazon S3 over HTTPS endpoints', 'Store the data on EBS volumes with encryption enabled instead of using Amazon S3', '<p>Use AWS Shield to protect your data at rest</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are a new Solutions Architect in a large insurance firm. To maintain compliance with HIPPA laws, all data being backed up or stored on Amazon S3 needs to be encrypted at rest. In this scenario, what is the best method of encryption for your data, assuming S3 is being used for storing financial-related data? (Choose 2)</p>', 'explanation': '<p>Data protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit by using SSL or by using client-side encryption. You have the following options for protecting data at rest in Amazon S3.</p> <ul> <li><strong>Use Server-Side Encryption</strong>&nbsp;&ndash; You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects.</li> <li><strong>Use Client-Side Encryption</strong>&nbsp;&ndash; You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</li> </ul> <p>&nbsp;<img style="display: block; margin-left: auto; margin-right: auto;" src="https://media.amazonwebservices.com/blog/2014/s3_sse_customer_key_2.png" /></p> <p>&nbsp;</p> <p>Hence, Options 1 and 3 are the correct answers:</p> <ul> <li>Enable SSE on an S3 bucket to make use of AES-256 encryption</li> <li>Encrypt the data locally using your own encryption keys, then copy the data to Amazon S3 over HTTPS endpoints. This refers to using a Server-Side Encryption with Customer-Provided Keys (SSE-C).&nbsp;</li> </ul> <p>&nbsp;</p> <p>Options 2 and 4 are incorrect because all these options are for protecting your data in your EBS volumes. Note that an S3 bucket does not use EBS volumes to store your data.</p> <p>Option 5 is incorrect because AWS Shield is mainly used to protect your entire VPC against DDoS attacks.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2567072, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are a new Solutions Architect in a large insurance firm. To maintain compliance with HIPPA laws, all data being backed up or stored on Amazon S3 needs to be encrypted at rest. In this scenario, what is the best method of encryption for your data, assuming S3 is being used for storing financial-related data? (Choose 2)', 'id': 9831756, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Auto Scaling', 'prompt': {'relatedLectureIds': '', 'answers': ['Do nothing. You can start directly launching EC2 instances in the Auto Scaling group with the same launch configuration.', 'Create a new launch configuration.', 'Create a new target group.', 'Create a new target group and launch configuration.'], 'feedbacks': ['', '', '', ''], 'question': 'A tech company is currently using Auto Scaling for their web application. A new AMI now needs to be used for launching a fleet of EC2 instances. <br><br>Which of the following changes needs to be done?', 'explanation': '<p>For this scenario, you have to create a new launch configuration. Remember that&nbsp;you can\'t modify a launch configuration after you\'ve created it.</p> <p>A&nbsp;<em>launch configuration</em>&nbsp;is a template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you\'ve launched an EC2 instance before, you specified the same information in order to launch the instance.</p> <p>You can specify your launch configuration with multiple Auto Scaling groups. However, you can only specify one launch configuration for an Auto Scaling group at a time, and you can\'t modify a launch configuration after you\'ve created it. Therefore, if you want to change the launch configuration for an Auto Scaling group, you must create a launch configuration and then update your Auto Scaling group with the new launch configuration.</p> <p>Option 1 is incorrect because what you are trying to achieve is change the AMI being used by your fleet of EC2 instances. Therefore, you need to change the launch configuration to update what your instances are using.</p> <p>Options 3 and 4 are incorrect because you only want to change the AMI being used by your instances, and not the instances themselves. Therefore, you should be updating your launch configuration, not the target group.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/LaunchConfiguration.html">http://docs.aws.amazon.com/autoscaling/latest/userguide/LaunchConfiguration.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567074, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'A tech company is currently using Auto Scaling for their web application. A new AMI now needs to be used for launching a fleet of EC2 instances. Which of the following changes needs to be done?', 'id': 9831758, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['The total volume of data and number of objects you can store are unlimited.', 'The largest object that can be uploaded in a single PUT is 5 TB.', '<p>S3 is an object storage service that provides file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage.</p>', 'You can only store ZIP or TAR files in S3.', '<p>The largest object that can be uploaded in a single PUT is 5 GB.</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working as a Cloud Engineer for a top aerospace engineering firm. One of your tasks is to set up a document storage system using S3 for all of the engineering files. In Amazon S3, which of the following statements are true? (Choose 2)</p>', 'explanation': '<p>The correct answers are:&nbsp;</p> <ul> <li>The total volume of data and number of objects you can store are unlimited.</li> <li>The largest object that can be uploaded in a single PUT is 5 GB.</li> </ul> <p>&nbsp;</p> <p>Option 2 is incorrect as the largest object that can be uploaded in a single PUT is 5 GB and not 5 TB. Remember that the upload limit depends on whether you upload an object using a single PUT operation or via Multipart Upload. The largest object that can be uploaded in a single PUT is 5 GB. Please take note the phrase "... in a single PUT". If you are using the multipart upload API, then the limit is 5 TB.</p> <p>Option 3 is incorrect because although S3 is indeed an object storage service, it does not provide file system access semantics. EFS provides this feature but not S3.</p> <p>Option 4 is incorrect as you can store virtually any kind of data in any format in S3.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/s3/faqs/">https://aws.amazon.com/s3/faqs/</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['a', 'e'], 'original_assessment_id': 2567086, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working as a Cloud Engineer for a top aerospace engineering firm. One of your tasks is to set up a document storage system using S3 for all of the engineering files. In Amazon S3, which of the following statements are true? (Choose 2)', 'id': 9831768, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Route 53', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Configure an Active-Active Failover with Weighted routing policy.\xa0 </p>', '<p>Configure an Active-Passive Failover with Weighted Records.\xa0 </p>', '<p>Configure an Active-Active Failover with One Primary and One Secondary Resource.\xa0 </p>', '<p>Configure an Active-Passive Failover with Multiple Primary and Secondary Resources.\xa0 </p>'], 'feedbacks': ['', '', '', ''], 'question': "<p>You are setting up the cloud architecture for an international money transfer service to be deployed in AWS which will have thousands of users around the globe. The service should be available 24/7 to avoid any business disruption and should be resilient enough to handle the outage of an entire AWS region. To meet this requirement, you have deployed your AWS resources to multiple AWS Regions. You need to use Route 53 and configure it to set all of your resources to be available all the time as much as possible. When a resource becomes unavailable, your Route 53 should detect that it's unhealthy and stop including it when responding to queries.\xa0 \xa0</p><p>Which of the following is the most fault tolerant routing configuration that you should use in this scenario?\xa0 </p>", 'explanation': '<p>You can use Route 53 health checking to configure active-active and active-passive failover configurations. You configure active-active failover using any routing policy (or combination of routing policies) other than failover, and you configure active-passive failover using the failover routing policy.</p> <p><strong>Active-Active Failover </strong></p> <p>Use this failover configuration when you want all of your resources to be available the majority of the time. When a resource becomes unavailable, Route 53 can detect that it\'s unhealthy and stop including it when responding to queries.</p> <p>In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. Route 53 can respond to a DNS query using any healthy record.</p> <p><strong>Active-Passive Failover </strong></p> <p>Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only the healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.</p> <p>Options 2 and 4 are incorrect because an Active-Passive Failover is mainly used when you want a primary resource or group of resources to be available most of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. In this scenario, all of your resources should be available all the time as much as possible which is why you have to use an Active-Active Failover instead.</p> <p>Option 3 is incorrect because you cannot set up an Active-Active Failover with One Primary and One Secondary Resource. Remember that an Active-Active Failover uses all available resources all the time without a primary nor a secondary resource.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html ">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html </a></p> <p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html ">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html </a></p> <p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567076, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': "You are setting up the cloud architecture for an international money transfer service to be deployed in AWS which will have thousands of users around the globe. The service should be available 24/7 to avoid any business disruption and should be resilient enough to handle the outage of an entire AWS region. To meet this requirement, you have deployed your AWS resources to multiple AWS Regions. You need to use Route 53 and configure it to set all of your resources to be available all the time as much as possible. When a resource becomes unavailable, your Route 53 should detect that it's unhealthy and stop including it when responding to queries.\xa0 \xa0Which of the following is the most fault tolerant routing configuration that you should use in this scenario?", 'id': 9831760, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Auto Scaling', 'prompt': {'relatedLectureIds': '', 'answers': ['You already have 20 on-demand instances running in your entire VPC.', '<p>The maximum size of your Auto Scaling group is set to twenty.</p>', '<p>The scale down policy of your Auto Scaling group is too high.</p>', '<p>The scale up policy of your Auto Scaling group, which is based on the average CPU Utilization metric, is not yet reached.</p>', '<p>You are using <em>burstable</em> instances which have the ability to sustain high CPU performance of more than 40 minutes, which in effect, suspends your scale-up policy.</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a global game development company. They have a web application currently running on twenty EC2 instances as part of an Auto Scaling group. All twenty instances have been running at a maximum of 100% CPU Utilization for the past 40 minutes however, the Auto Scaling group has not added any additional EC2 instances to the group.\xa0 \xa0</p><p>What could be the root cause of this issue? (Choose 2)</p>', 'explanation': '<p>You are limited to running up to a total of 20 On-Demand instances across the instance family, purchasing 20 Reserved Instances, and requesting Spot Instances per your dynamic Spot limit per region.</p> <p>If the maximum size of your Auto Scaling group has already been reached, then it would not create any new EC2 instance.</p> <p>Hence, the correct answers are Options 1 and 2:</p> <p style="padding-left: 30px;">&nbsp;- You already have 20 on-demand instances running in your entire VPC.</p> <p style="padding-left: 30px;">&nbsp;- The maximum size of your Auto Scaling group is set to twenty.</p> <p style="padding-left: 30px;">&nbsp;</p> <p><img src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/T3-unltd-when-to-use.png" alt="" width="700" height="223" />&nbsp;</p> <p>Option 3 is incorrect because the scenario depicts that the number of running instances is limited to 20 when you scale up, therefore, the scale down policy has nothing to do with this.</p> <p>Option 4 is incorrect because your thresholds should have been reached since all instances are showing 100% utilization. What is preventing you from scaling is the maximum size you set for the auto scaling group.</p> <p>Option 5 is incorrect because a burstable instance, which runs at higher CPU utilization for a prolonged period, does not affect the scale-up policy of the Auto Scaling group. Burstable instances provide additional compute capacity and cost-benefit for your EC2 instances hence, this is not a possible culprit in this scenario.&nbsp;</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/ec2/faqs/">https://aws.amazon.com/ec2/faqs/</a></p> <p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a', 'b'], 'original_assessment_id': 2567078, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working as a Solutions Architect for a global game development company. They have a web application currently running on twenty EC2 instances as part of an Auto Scaling group. All twenty instances have been running at a maximum of 100% CPU Utilization for the past 40 minutes however, the Auto Scaling group has not added any additional EC2 instances to the group.\xa0 \xa0What could be the root cause of this issue? (Choose 2)', 'id': 9831762, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'ECS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>EKS</p>', '<p>EFS</p>', '<p>ECS</p>', '<p>EBS</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A new online banking platform has been re-designed to have a microservices architecture in which complex applications are decomposed into smaller, independent services. The new platform is using Docker considering that application containers are optimal for running small, decoupled services. </p>\n\n<p>Which service can you use to migrate this new platform to AWS?</p>', 'explanation': '<p>Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. You can host your cluster on a serverless infrastructure that is managed by Amazon ECS by launching your services or tasks using the Fargate launch type. For more control, you can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/images/overview-fargate.png" alt="" width="700" height="785" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because Amazon EKS runs the Kubernetes management infrastructure and not Docker.</p> <p>Option 2 is incorrect because Amazon EFS is a file system for Linux-based workloads for use with AWS Cloud services and on-premises resources.</p> <p>Option 4 is incorrect because Amazon EBS is primarily used to provide persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-container-service-amazon-ecs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-container-service-amazon-ecs/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567080, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'A new online banking platform has been re-designed to have a microservices architecture in which complex applications are decomposed into smaller, independent services. The new platform is using Docker considering that application containers are optimal for running small, decoupled services. \n\nWhich service can you use to migrate this new platform to AWS?', 'id': 9831764, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EMR', 'prompt': {'relatedLectureIds': '', 'answers': ['Amazon DynamoDB for storing and EC2 for analyzing the logs.', 'Amazon EC2 with EBS volumes for storing and analyzing the log files.', 'Amazon S3 for storing the ELB log files and an EC2 instance for analyzing the log files using a custom-built application.', 'Amazon S3 for storing ELB log files and Amazon EMR for analyzing the log files.'], 'feedbacks': ['', '', '', ''], 'question': 'You are working for a large telecommunications company where you need to run analytics against all combined log files from your Application Load Balancer as part of the regulatory requirements. <br><br>Which AWS services can be used together to collect logs and then easily perform log analysis?', 'explanation': '<p>In this scenario, it is best to use a combination of Amazon S3 and Amazon EMR:&nbsp;Amazon S3 for storing ELB log files and Amazon EMR for analyzing the log files. Access logging in the ELB is stored in Amazon S3 which means that options 3 and 4 are both valid answers. However, log analysis can be automatically provided by Amazon EMR, which is more economical than building a custom-built log analysis application and hosting it in EC2. Hence, option 4 is the best answer between the two.&nbsp;&nbsp;</p> <p>Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. You can disable access logging at any time.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/emr/latest/ManagementGuide/images/cluster-node-types.png" /></p> <p>&nbsp;</p> <p>Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. It securely and reliably handles a broad set of big data use cases, including log analysis, web indexing, data transformations (ETL), machine learning, financial analysis, scientific simulation, and bioinformatics. You can also run other popular distributed frameworks such as Apache Spark, HBase, Presto, and Flink in Amazon EMR, and interact with data in other AWS data stores such as Amazon S3 and Amazon DynamoDB.&nbsp;</p> <p>Option 1 is incorrect because DynamoDB is a noSQL database solution of AWS. It would be inefficient to store logs in DynamoDB while using EC2 to analyze them.</p> <p>Option 2 is incorrect because using EC2 with EBS would be costly, and EBS might not provide the most durable storage for your logs, unlike S3.</p> <p>Option 3 is incorrect because using EC2 to analyze logs would be inefficient and expensive since you will have to program the analyzer yourself.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/emr/">https://aws.amazon.com/emr/</a></p> <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this Amazon EMR Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-emr/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-emr/</span></a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567082, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working for a large telecommunications company where you need to run analytics against all combined log files from your Application Load Balancer as part of the regulatory requirements. Which AWS services can be used together to collect logs and then easily perform log analysis?', 'id': 9831766, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Elastic Beanstalk', 'prompt': {'relatedLectureIds': '', 'answers': ['AWS Elastic Beanstalk', '<p>AWS CloudFront\xa0 </p>', '<p>AWS CloudFormation</p>', '<p>AWS CodeCommit\xa0 </p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>Your company is in a hurry of deploying their new web application written in NodeJS to AWS. As the Solutions Architect of the company, you were assigned to do the deployment without worrying about the underlying infrastructure that runs the application. Which service will you use to easily deploy and manage your new web application in AWS?\xa0 </p>', 'explanation': '<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p> <p>Option 2 is incorrect because AWS CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. It does not provide any deployment capability for your custom applications unlike Elastic Beanstalk.</p> <p>Option 3 is incorrect because although the CloudFormation service provides deployment capabilities, you will still have to design a custom template that contains the required AWS resources for your application needs. Hence, this will require more time to complete instead of just directly using Elastic Beanstalk.</p> <p>Option 4 is incorrect because although you can upload your NodeJS code in AWS CloudCommit, this service is just a fully-managed source control service that hosts secure Git-based repositiories and hence, it does not provide a way to deploy or manage your applications in AWS.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-beanstalk/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-beanstalk/</span></a></p> <p>&nbsp;</p> <p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567088, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'Your company is in a hurry of deploying their new web application written in NodeJS to AWS. As the Solutions Architect of the company, you were assigned to do the deployment without worrying about the underlying infrastructure that runs the application. Which service will you use to easily deploy and manage your new web application in AWS?', 'id': 9831770, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ElastiCache', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>It securely delivers data to customers globally with low latency and high transfer speeds.</p>', '<p>It provides an in-memory cache that delivers up to 10x performance improvement from milliseconds to microseconds or even at millions of requests per second.</p>', '<p>By caching database query results.</p>', '<p>It reduces the load on your database by routing read queries from your applications to the Read Replica.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>Your web application is relying entirely on slower disk-based databases, causing it to perform slowly. To improve its performance, you integrated an in-memory data store to your web application using ElastiCache. How does Amazon ElastiCache improve database performance?</p>', 'explanation': '<p>ElastiCache improves the performance of your database through caching query results.</p> <p>The primary purpose of an in-memory key-value store is to provide ultra-fast (submillisecond latency) and inexpensive access to copies of data. Most data stores have areas of data that are frequently accessed but seldom updated.&nbsp; Additionally, querying a database is always slower and more expensive than locating a key in a key-value pair cache. Some database queries are especially expensive to perform, for example, queries that involve joins across multiple tables or queries with intensive calculations.</p> <p>By caching such query results, you pay the price of the query once and then are able to quickly retrieve the data multiple times without having to re-execute the query.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/images/ElastiCache-Caching.png" width="750" height="368" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because this option describes what CloudFront does and not ElastiCache.</p> <p>Option 2 is incorrect because&nbsp;this option describes what Amazon DynamoDB Accelerator (DAX) does and not ElastiCache.&nbsp;Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB.</p> <p>Option 4 is incorrect because&nbsp;this option describes what an RDS Read Replica does and not ElastiCache.&nbsp;Amazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWS Region.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/elasticache/ ">https://aws.amazon.com/elasticache/</a></p> <p><a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-use-cases.html">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-use-cases.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon Elasticache Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/</span></a>&nbsp;</p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567090, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'Your web application is relying entirely on slower disk-based databases, causing it to perform slowly. To improve its performance, you integrated an in-memory data store to your web application using ElastiCache. How does Amazon ElastiCache improve database performance?', 'id': 9831772, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'relatedLectureIds': '', 'answers': ['The Classic Load Balancer listener is not set to port 80.', 'The security group of the EC2 instances does not allow HTTP traffic.', 'Cross-Zone Load Balancing is disabled.', 'The Classic Load Balancer listener is not set to port 22.'], 'feedbacks': ['', '', '', ''], 'question': '<p>In your VPC, you have a Classic Load Balancer distributing traffic to 2 running EC2 instances in <code>ap-southeast-1a</code> AZ and 8 EC2 instances in <code>ap-southeast-1b</code> AZ. However, you noticed that half of your incoming traffic goes to <code>ap-southeast-1a</code> AZ which over-utilize its 2 instances but underutilize the other 8 instances in the other AZ.\xa0 </p><p><br></p><p>What could be the most likely cause of this problem? </p>', 'explanation': '<p>Cross-zone load balancing reduces the need to maintain equivalent numbers of instances in each enabled Availability Zone, and improves your application\'s ability to handle the loss of one or more instances.&nbsp;</p> <p>When you create a Classic Load Balancer, the default for cross-zone load balancing depends on how you create the load balancer. With the API or CLI, cross-zone load balancing is disabled by default. With the AWS Management Console, the option to enable cross-zone load balancing is selected by default. After you create a Classic Load Balancer, you can enable or disable cross-zone load balancing at any time.</p> <p>The following diagrams demonstrate the effect of cross-zone load balancing. There are two enabled Availability Zones, with 2 targets in Availability Zone A and 8 targets in Availability Zone B. Clients send requests, and Amazon Route&nbsp;53 responds to each request with the IP address of one of the load balancer nodes. This distributes traffic such that each load balancer node receives 50% of the traffic from the clients. Each load balancer node distributes its share of the traffic across the registered targets in its scope.</p> <p>If cross-zone load balancing is <strong>enabled</strong>, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets.</p> <p>&nbsp;</p> <div><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/images/cross_zone_load_balancing_enabled.png" alt="When cross-zone load balancing is enabled" /></div> <p>&nbsp;</p> <p>If cross-zone load balancing is <strong>disabled</strong>, each of the 2 targets in Availability Zone A receives 25% of the traffic and each of the 8 targets in Availability Zone B receives 6.25% of the traffic. This is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone.</p> <p>&nbsp;</p> <div><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/images/cross_zone_load_balancing_disabled.png" alt="When cross-zone load balancing is disabled" /></div> <p>&nbsp;</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-crosszone-lb.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#cross-zone-load-balancing</a></p> <p><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-crosszone-lb.html">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-crosszone-lb.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567092, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'In your VPC, you have a Classic Load Balancer distributing traffic to 2 running EC2 instances in ap-southeast-1a AZ and 8 EC2 instances in ap-southeast-1b AZ. However, you noticed that half of your incoming traffic goes to ap-southeast-1a AZ which over-utilize its 2 instances but underutilize the other 8 instances in the other AZ.\xa0 What could be the most likely cause of this problem?', 'id': 9831774, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'relatedLectureIds': '', 'answers': ['It provides elasticity to your Amazon RDS database.', 'Allows both read and write operations on the read replica to complement the primary database.', 'Improves performance of the primary database by taking workload from it.', 'Automatic failover in the case of Availability Zone service failures.', 'It enhances the read performance of your primary database.'], 'feedbacks': ['', '', '', '', ''], 'question': 'You are trying to convince a team to use Amazon RDS Read Replica for your multi-tier web application. What are two benefits of using read replicas? (Choose 2)', 'explanation': '<p>Amazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.</p> <p>You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, and PostgreSQL as well as Amazon Aurora.</p> <p>Option 2 is incorrect as the Read Replica only offers read operations.</p> <p>Option 4 is incorrect as this is a benefit of Multi-AZ and not of a Read Replica.</p> <p>Option 5 is incorrect because Read Replicas does not do anything to upgrade or increase the read throughput on the primary DB instance per se, but it provides a way for your application to fetch data from replicas. In this way, it improves the overall performance of your entire database-tier (and not just the primary DB instance).</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/rds/details/read-replicas/">https://aws.amazon.com/rds/details/read-replicas/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p> <p>&nbsp;</p> <p><strong>Additional tutorial -&nbsp;How do I make my RDS MySQL read replica writable?</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/j5da6d2TIPc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2567094, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are trying to convince a team to use Amazon RDS Read Replica for your multi-tier web application. What are two benefits of using read replicas? (Choose 2)', 'id': 9831776, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Lambda', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>You only specified one subnet in your Lambda function configuration. That single subnet runs out of available IP addresses and there is no other subnet or Availability Zone which can handle the peak load.</p>', '<p>Your VPC does not have a NAT gateway.</p>', '<p>Your VPC does not have sufficient subnet ENIs or subnet IPs.</p>', '<p>The associated security group of your function does not allow outbound connections.</p>', '<p>The attached IAM execution role of your function does not have the necessary permissions to access the resources of your VPC.</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You have a VPC that has a CIDR block of <code>10.31.0.0/27</code> which is connected to your on-premises data center. There was a requirement to create a Lambda function that will process massive amounts of cryptocurrency transactions every minute and then store the results to EFS. After you set up the serverless architecture and connected Lambda function to your VPC, you noticed that there is an increase in invocation errors with EC2 error types such as <code>EC2ThrottledException</code> on certain times of the day. </p><p>Which of the following are the possible causes of this issue? (Choose 2)</p>', 'explanation': '<p>You can configure a function to connect to a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access private resources during execution.</p> <p>AWS Lambda runs your function code securely within a VPC by default. However, to enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within your private VPC.</p> <p>Lambda functions cannot connect directly to a VPC with dedicated instance tenancy. To connect to resources in a dedicated VPC, peer it to a second VPC with default tenancy.</p> <p>&nbsp;</p> <center><iframe src="https://www.youtube.com/embed/JcRKdEP94jM" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></center> <p>&nbsp;</p> <p>Your Lambda function automatically scales based on the number of events it processes.&nbsp;If your Lambda function accesses a VPC, you must make sure that your VPC has sufficient ENI capacity to support the scale requirements of your Lambda function.&nbsp;It is also recommended that you specify at least one subnet in each Availability Zone in your Lambda function configuration.&nbsp;</p> <p>By specifying subnets in each of the Availability Zones, your Lambda function can run in another Availability Zone if one goes down or runs out of IP addresses. If your VPC does not have sufficient ENIs or subnet IPs, your Lambda function will not scale as requests increase, and you will see an increase in invocation errors with EC2 error types like <code>EC2ThrottledException</code>. For asynchronous invocation, if you see an increase in errors without corresponding CloudWatch Logs, invoke the Lambda function synchronously in the console to get the error responses. Hence, the correct answers for this scenario are Options 1 and 3.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-04-03_03-53-45-3550f2ecbfad29ea60f49fd5ea8d2809.gif" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect because an issue in the NAT Gateway is unlikely to cause a request throttling issue or produce an&nbsp;<code>EC2ThrottledException</code> error in Lambda. Take note that the scenario says that the issue is happening only on certain times of the day, which means that the issue is only intermittent and the function works at other times. We can also conclude that an availability issue is not an issue since the application is already using a highly available NAT Gateway and not just a NAT instance.</p> <p>Option 4 is incorrect because if the associated security group does not allow outbound connections then the Lambda function will not work at all in the first place. Remember that the scenario says that the issue only happens intermittently. In addition, Internet traffic restrictions do not usually produce&nbsp;<code>EC2ThrottledException</code> errors.</p> <p>Option 5 is incorrect because just as what is explained in Options 2 and 4 above, the issue is intermittent and thus, the IAM execution role of the function does have the necessary permissions to access the resources of the VPC since it works at those specific times. In case that the issue is indeed caused by a permission problem, then an <code>EC2AccessDeniedException</code> error would most likely be returned and not an <code>EC2ThrottledException</code> error.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/lambda/latest/dg/vpc.html">https://docs.aws.amazon.com/lambda/latest/dg/vpc.html</a></p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/">https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this AWS Lambda Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/</span></a></p>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 8843412, '_class': 'assessment', 'updated': '2019-05-15T01:23:26Z', 'created': '2019-05-15T01:23:26Z', 'question_plain': 'You have a VPC that has a CIDR block of 10.31.0.0/27 which is connected to your on-premises data center. There was a requirement to create a Lambda function that will process massive amounts of cryptocurrency transactions every minute and then store the results to EFS. After you set up the serverless architecture and connected Lambda function to your VPC, you noticed that there is an increase in invocation errors with EC2 error types such as EC2ThrottledException on certain times of the day. Which of the following are the possible causes of this issue? (Choose 2)', 'id': 9831796, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['EC2 instances in a private subnet can communicate with the Internet only if they have an Elastic IP.', 'Each subnet maps to a single Availability Zone.', 'The allowed block size in VPC is between a /16 netmask (65,536 IP addresses) and /27 netmask (16 IP addresses).', 'Every subnet that you create is automatically associated with the main route table for the VPC. ', 'Each subnet spans to 2 Availability Zones.'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are a Solutions Architect working for an aerospace engineering company which recently adopted a hybrid cloud infrastructure with AWS. One of your tasks is to launch a VPC with both public and private subnets for their EC2 instances as well as their database instances respectively.\xa0 \xa0</p><p>Which of the following statements are true regarding Amazon VPC subnets? (Choose 2)</p>', 'explanation': '<p>A VPC spans all the Availability Zones in the region. After creating a VPC, you can add one or more subnets in each Availability Zone. When you create a subnet, you specify the CIDR block for the subnet, which is a subset of the VPC CIDR block. Each subnet must reside entirely within one Availability Zone and cannot span zones. Availability Zones are distinct locations that are engineered to be isolated from failures in other Availability Zones. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single location.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/userguide/images/subnets-diagram.png" alt="" width="619" height="732" /></p> <p>&nbsp;</p> <p>Below are the important points you have to remember about subnets:</p> <ul> <li>-Each subnet maps to a single Availability Zone.</li> <li>-Every subnet that you create is automatically associated with the main route table for the VPC.</li> <li>-If a subnet\'s traffic is routed to an Internet gateway, the subnet is known as a public subnet.&nbsp;</li> </ul> <p>&nbsp;</p> <p>Option 1 is incorrect because&nbsp;EC2 instances in a private subnet can communicate with the Internet not just by having an Elastic IP, but also with a public IP address.</p> <p>Option 3 is incorrect because the&nbsp;allowed block size in VPC is between a /16 netmask (65,536 IP addresses) and /28 netmask (16 IP addresses) and not&nbsp;/27 netmask. For you to easily remember this,&nbsp;/27 netmask is equivalent to exactly 27 IP addresses but keep in mind that the limit is until /28 netmask.</p> <p>Option 5 is incorrect because each subnet must reside entirely within one Availability Zone and cannot span zones.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['b', 'd'], 'original_assessment_id': 2567096, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are a Solutions Architect working for an aerospace engineering company which recently adopted a hybrid cloud infrastructure with AWS. One of your tasks is to launch a VPC with both public and private subnets for their EC2 instances as well as their database instances respectively.\xa0 \xa0Which of the following statements are true regarding Amazon VPC subnets? (Choose 2)', 'id': 9831778, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'DynamoDB', 'prompt': {'relatedLectureIds': '', 'answers': ['AWS RDS', 'DynamoDB', 'Amazon ElastiCache', 'Redshift'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a tech company where you are instructed to build a web architecture using On-Demand EC2 instances and a database in AWS. However, due to budget constraints, the company instructed you to choose a database service in which they no longer need to worry about database management tasks such as hardware or software provisioning, setup, configuration, scaling and backups.<br><br>Which database service in AWS is best to use in this scenario?</p>', 'explanation': '<p>Basically, a database service in which you no longer need to worry about database management tasks such as hardware or software provisioning, setup and configuration is called a&nbsp;fully managed database. This means that AWS<em> fully manages</em> all of the database management tasks and the underlying host server.&nbsp;The main differentiator here is the keyword "<strong>scaling</strong>" in the question. In RDS, you still have to manually scale up your resources and create Read Replicas&nbsp;to improve scalability while in DynamoDB, this is automatically done.&nbsp;</p> <p>DynamoDB is the best option to use in this scenario. It is a fully managed non-relational database service &ndash; you simply create a database table, set your target utilization for Auto Scaling, and let the service handle the rest. You no longer need to worry about database management tasks such as hardware or software provisioning, setup and configuration, software patching, operating a reliable, distributed database cluster, or partitioning data over multiple instances as you scale. DynamoDB also lets you backup and restore all your tables for data archival, helping you meet your corporate and governmental regulatory requirements.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/streams-endpoints.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because&nbsp;AWS RDS is just a "managed" service and not "fully managed". This means that you still have to handle the backups and other administrative tasks such as when the automated&nbsp;OS patching will take place.</p> <p>Option 3 is incorrect because although ElastiCache is fully managed, it is not a database service but an In-Memory Data Store.</p> <p>Option 4 is incorrect because although Redshift is fully managed, it is not a database service but a&nbsp;Data Warehouse.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/dynamodb/">https://aws.amazon.com/dynamodb/</a></p> <p><a href="https://aws.amazon.com/products/databases/">https://aws.amazon.com/products/databases/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567098, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are working as a Solutions Architect for a tech company where you are instructed to build a web architecture using On-Demand EC2 instances and a database in AWS. However, due to budget constraints, the company instructed you to choose a database service in which they no longer need to worry about database management tasks such as hardware or software provisioning, setup, configuration, scaling and backups.Which database service in AWS is best to use in this scenario?', 'id': 9831780, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'relatedLectureIds': '', 'answers': ['Deploy eight EC2 instances in one Availability Zone behind an Amazon Elastic Load Balancer.', 'Deploy four EC2 instances in one region and four in another region behind an Amazon Elastic Load Balancer.', 'Deploy four EC2 instances in one Availability Zone and four in another availability zone in the same region behind an Amazon Elastic Load Balancer.', 'Deploy two EC2 instances in four regions behind an Amazon Elastic Load Balancer.'], 'feedbacks': ['', '', '', ''], 'question': 'You are a Solutions Architect for a major TV network. They have a web application running on eight Amazon EC2 instances, consuming about 55% of resources on each instance. You are using Auto Scaling to make sure that eight instances are running at all times. The number of requests that this application processes are consistent and do not experience spikes. Your manager instructed you to ensure high availability of this web application at all times to avoid any loss of revenue. You want the load to be distributed evenly between all instances. You also want to use the same Amazon Machine Image (AMI) for all EC2 instances. <br><br>How will you be able to achieve this?', 'explanation': '<p>The best option to take is to deploy four EC2 instances in one Availability Zone and four in another availability zone in the same region behind an Amazon Elastic Load Balancer. In this way, if one availability zone goes down, there is still another available zone that can accomodate traffic.</p> <p>Option 1 is incorrect because this architecture is not highly available. If that Availability Zone goes down, then your web application will be unreachable.</p> <p>Options 2 and 4 are incorrect because the ELB is designed to only run in one region and not across multiple regions.&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/elasticloadbalancing/">https://aws.amazon.com/elasticloadbalancing/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567100, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are a Solutions Architect for a major TV network. They have a web application running on eight Amazon EC2 instances, consuming about 55% of resources on each instance. You are using Auto Scaling to make sure that eight instances are running at all times. The number of requests that this application processes are consistent and do not experience spikes. Your manager instructed you to ensure high availability of this web application at all times to avoid any loss of revenue. You want the load to be distributed evenly between all instances. You also want to use the same Amazon Machine Image (AMI) for all EC2 instances. How will you be able to achieve this?', 'id': 9831782, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Set up a small EC2 instance and a security group which only allows access on port 22 via your IP address</p>', '<p>Set up a large EC2 instance and a security group which only allows access on port 22 via your IP address</p>', '<p>Set up a large EC2 instance and a security group which only allows access on port 22</p>', '<p>Set up a small EC2 instance and a security group which only allows access on port 22</p>'], 'feedbacks': ['', '', '', ''], 'question': "<p>Your IT Manager instructed you to set up a bastion host in the cheapest, most secure way, and that you should be the only person that can access it via SSH.\xa0 \xa0</p><p>Which of the following steps would satisfy your IT Manager's request?</p>", 'explanation': '<p>A&nbsp;<em>bastion host is</em>&nbsp;a server whose purpose is to provide access to a private network from an external network, such as the Internet. Because of its exposure to potential attack, a&nbsp;<em>bastion host</em>&nbsp;must minimize the chances of penetration.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://dmhnzl5mp9mj6.cloudfront.net/bigdata_awsblog/images/EMR_Subnet_Image_4.jpg" alt="" width="444" height="343" /></p> <p>&nbsp;</p> <p>To create a bastion host, you can create a new EC2 instance which should only have a security group from a particular IP address for maximum security. Since the cost is also considered in the question, you should choose a small instance for your host. By default,&nbsp;t2.micro instance is used by AWS but you can change these settings during deployment.</p> <p>Option 2 is incorrect because you don\'t need to provision a large EC2 instance to run a single bastion host. At the same time, you are looking for the cheapest solution possible.</p> <p>Options 3 and 4 are incorrect because you did not set your specific IP address to the security group rules, which possibly means that you publicly allow traffic from all sources in your security group. This is wrong as you should only be the one to have access to the bastion host.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html">https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html</a></p> <p><a href="https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/">https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567102, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': "Your IT Manager instructed you to set up a bastion host in the cheapest, most secure way, and that you should be the only person that can access it via SSH.\xa0 \xa0Which of the following steps would satisfy your IT Manager's request?", 'id': 9831784, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>You will be billed when your On-Demand instance is in <code>pending</code> state.</p>', '<p>You will be billed when your Spot instance is preparing to stop with a <code>stopping</code> state.</p>', '<p>You will be billed when your On-Demand instance is preparing to hibernate with a <code>stopping</code> state.</p>', '<p>You will be billed when your Reserved instance is in <code>terminated</code> state.</p>', '<p>You will not be billed for any instance usage while an instance is not in the <code>running</code> state.</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>In Amazon EC2, you can manage your instances from the moment you launch them up to their termination. You can flexibly control your computing costs by changing the EC2 instance state. Which of the following statements is true regarding EC2 billing? (Choose 2)</p>', 'explanation': '<p>By working with Amazon EC2 to manage your instances from the moment you launch them through their termination, you ensure that your customers have the best possible experience with the applications or sites that you host on your instances.&nbsp;The following illustration represents the transitions between instance states. Notice that you can\'t stop and start an instance store-backed instance:</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_lifecycle.png" alt="" width="650" height="314" /></p> <p>&nbsp;</p> <p>Below are the valid EC2 lifecycle instance states:&nbsp;</p> <p style="padding-left: 30px;"><strong><code>pending</code></strong>&nbsp;-&nbsp;The instance is preparing to enter the running state. An instance enters the pending state when it launches for the first time, or when it is restarted after being in the stopped state.</p> <p style="padding-left: 30px;"><strong><code>running</code></strong> - The instance is running and ready for use.</p> <p style="padding-left: 30px;"><strong><code>stopping</code></strong> - The instance is preparing to be stopped. Take note that you will not billed if it is preparing to stop however, you will still be billed if it is just preparing to hibernate.<br /><br /><strong><code>stopped</code></strong> - The instance is shut down and cannot be used. The instance can be restarted at any time.</p> <p style="padding-left: 30px;"><strong><code>shutting-down</code></strong>&nbsp;- The instance is preparing to be terminated.</p> <p style="padding-left: 30px;"><strong><code>terminated</code></strong>&nbsp;- The instance has been permanently deleted and cannot be restarted. Take note that Reserved Instances that applied to terminated instances are <strong><em>still billed</em> </strong>until the end of their term according to their payment option.<br />&nbsp;</p> <p>Option 1 is incorrect because you will not be billed if your instance is in&nbsp;<strong><code>pending</code></strong> state.</p> <p>Option 2 is incorrect because&nbsp;you will not be billed if your instance is preparing to stop with a&nbsp;<strong><code>stopping</code></strong> state.</p> <p>Option 3 is correct because when the instance state is&nbsp;<strong><code>stopping</code></strong>, you will not billed if it is preparing to stop however, you <strong>will still be billed</strong> if it is just preparing to hibernate.</p> <p>Option 4 is correct because&nbsp;Reserved Instances that applied to terminated instances are still billed until the end of their term according to their payment option. I actually raised a pull-request to Amazon team about the billing conditions for Reserved Instances, which has been approved and reflected on your official AWS Documentation:&nbsp;<a href="https://github.com/awsdocs/amazon-ec2-user-guide/pull/45">https://github.com/awsdocs/amazon-ec2-user-guide/pull/45</a></p> <p>Option 5 is incorrect because&nbsp;the statement is not entirely true. You can still be billed if your&nbsp;instance is preparing to hibernate with a&nbsp;<strong><code>stopping&nbsp;</code></strong>state.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://github.com/awsdocs/amazon-ec2-user-guide/pull/45">https://github.com/awsdocs/amazon-ec2-user-guide/pull/45</a></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c', 'd'], 'original_assessment_id': 2567104, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'In Amazon EC2, you can manage your instances from the moment you launch them up to their termination. You can flexibly control your computing costs by changing the EC2 instance state. Which of the following statements is true regarding EC2 billing? (Choose 2)', 'id': 9831786, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Cloudformation', 'prompt': {'relatedLectureIds': '', 'answers': ['$2.50 per template per month', 'The length of time it takes to build the architecture with CloudFormation', 'It depends on the region where you will deploy.', 'CloudFormation templates are free but you are charged for the underlying resources it builds.'], 'feedbacks': ['', '', '', ''], 'question': 'You are a new Solutions Architect in your department and you have created 7 CloudFormation templates. Each template has been defined for a specific purpose. <br><br>What determines the cost of using these new CloudFormation templates?', 'explanation': '<p>There is no additional charge for AWS CloudFormation. You pay for AWS resources (such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation in the same manner as if you created them manually. You only pay for what you use, as you use it; there are no minimum fees and no required upfront commitments.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/create-stack-diagram.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect. There is no cost for creating CloudFormation templates. Costs are calculated from the AWS resources that are provisioned from that CloudFormation template.</p> <p>Option 2 is incorrect. There is no cost for the time it takes to execute CloudFormation templates. Costs are calculated from the AWS resources that are provisioned from that CloudFormation template.</p> <p>Option 3 is incorrect. Costs per region are not calculated based on the CloudFormation template, but rather on the regions where resources are provisioned during the building of the environment using the CloudFormation template.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/cloudformation/pricing/ ">https://aws.amazon.com/cloudformation/pricing/ </a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudFormation Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567106, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': 'You are a new Solutions Architect in your department and you have created 7 CloudFormation templates. Each template has been defined for a specific purpose. What determines the cost of using these new CloudFormation templates?', 'id': 9831788, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Security Group Inbound Rule: Protocol – TCP. Port Range – 22, Source 175.45.116.100/32', 'Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 175.45.116.100/32', 'Network ACL Inbound Rule: Protocol – UDP, Port Range – 22, Source 175.45.116.100/32', 'Network ACL Inbound Rule: Protocol – TCP, Port Range-22, Source 175.45.116.100/0'], 'feedbacks': ['', '', '', ''], 'question': "You are working for a large financial firm and you are instructed to set up a Linux bastion host. It will allow access to the Amazon EC2 instances running in their VPC. For security purposes, only the clients connecting from the corporate external public IP address 175.45.116.100 should have SSH access to the host. <br><br>Which is the best option that can meet the customer's requirement?", 'explanation': '<p>A bastion host is a special purpose computer on a network specifically designed and configured to withstand attacks. The computer generally hosts a single application, for example a proxy server, and all other services are removed or limited to reduce the threat to the computer.&nbsp;</p> <p>When setting up a bastion host in AWS, you should only allow&nbsp;the individual IP of the client and not the entire network. Therefore, in the&nbsp;<strong>Source,&nbsp;</strong>&nbsp;the proper CIDR notation should be used. The <strong>/32</strong> denotes one IP address and the <strong>/0</strong> refers to the entire network.</p> <p>Option 2 is incorrect since the SSH protocol uses TCP and port 22, and not UDP.</p> <p>Option 3 is incorrect since the SSH protocol uses TCP and port 22, and not UDP. Aside from that, network ACLs act as a firewall for your whole VPC subnet, while security groups operate on an instance level. Since you are securing an EC2 instance, you should be using security groups.</p> <p>Option 4 is incorrect as it allowed the entire network instead of a single IP to gain access to the host.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567108, '_class': 'assessment', 'updated': '2019-05-15T01:23:25Z', 'created': '2019-05-15T01:23:25Z', 'question_plain': "You are working for a large financial firm and you are instructed to set up a Linux bastion host. It will allow access to the Amazon EC2 instances running in their VPC. For security purposes, only the clients connecting from the corporate external public IP address 175.45.116.100 should have SSH access to the host. Which is the best option that can meet the customer's requirement?", 'id': 9831790, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFront', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use CloudFront Signed Cookies to ensure that only their client can access the files.</p>', '<p>Use CloudFront signed URLs to ensure that only their client can access the files.</p>', '<p>Use S3 pre-signed URLs to ensure that only their client can access the files. Remove permission to use Amazon S3 URLs to read the files for anyone else.</p>', '<p>Create an origin access identity (OAI) and give it permission to read the files in the bucket.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>Your customer has clients all across the globe that access product files stored in several S3 buckets, which are behind each of their own CloudFront web distributions. They currently want to deliver their content to a specific client, and they need to make sure that only that client can access the data. Currently, all of their clients can access their S3 buckets directly using an S3 URL or through their CloudFront distribution. </p><p>Which of the following options should you implement to meet the above requirements?</p>', 'explanation': '<p>Many companies that distribute content over the Internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee. To securely serve this private content by using CloudFront, you can do the following:</p> <ul> <li>-Require that your users access your private content by using special CloudFront signed URLs or signed cookies.</li> <li>-Require that your users access your Amazon S3 content by using CloudFront URLs, not Amazon S3 URLs. Requiring CloudFront URLs isn\'t necessary, but it is recommended to prevent users from bypassing the restrictions that you specify in signed URLs or signed cookies.</li> </ul> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/images/PrivateContent_TwoParts.png" /></p> <p>&nbsp;</p> <p>All objects and buckets by default are private. The presigned URLs are useful if you want your user/customer to be able to upload a specific object to your bucket, but you don\'t require them to have AWS security credentials or permissions. You can generate a presigned URL programmatically using the AWS SDK for Java or the AWS SDK for .NET. If you are using Microsoft Visual Studio, you can also use AWS Explorer to generate a presigned object URL without writing any code. Anyone who receives a valid presigned URL can then programmatically upload an object.</p> <p>Option 3 is correct because&nbsp;using a&nbsp;presigned URL to your S3 bucket will prevent other users from accessing your private data which is intended only for a certain client.</p> <p>Option 1 is incorrect because the signed cookies feature is primarily used if you want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers\' area of website. In addition, this solution is not complete since the users can bypass&nbsp;the restrictions by simply using the direct S3 URLs.</p> <p>Option 2 is incorrect because although this solution is valid, the users can still bypass the restrictions in CloudFront by simply connecting to the direct S3 URLs.</p> <p>Option 4 is incorrect because an Origin Access Identity (OAI) will require your client to access the files only by using the CloudFront URL and not through a direct S3 URL. This can be a possible solution if it mentions the use of Signed URL or Signed Cookies.</p> <p>&nbsp;</p> <p><strong>References</strong>:</p> <p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon CloudFront cheat sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</span></a></p> <p>&nbsp;&nbsp;</p> <p><strong>S3 Pre-signed URLs vs CloudFront Signed URLs vs Origin Access Identity (OAI)</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 8158254, '_class': 'assessment', 'updated': '2019-05-15T01:23:26Z', 'created': '2019-05-15T01:23:26Z', 'question_plain': 'Your customer has clients all across the globe that access product files stored in several S3 buckets, which are behind each of their own CloudFront web distributions. They currently want to deliver their content to a specific client, and they need to make sure that only that client can access the data. Currently, all of their clients can access their S3 buckets directly using an S3 URL or through their CloudFront distribution. Which of the following options should you implement to meet the above requirements?', 'id': 9831792, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SNI', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use a wildcard certificate to handle multiple sub-domains and different domains.</p>', '<p>Add a Subject Alternative Name (SAN) for each additional domain to your certificate.</p>', '<p>Create a new CloudFront web distribution and configure it to serve HTTPS requests using dedicated IP addresses in order to associate your alternate domain names with a dedicated IP address in each CloudFront edge location.</p>', '<p>Upload all SSL certificates of the domains in the ALB using the console and bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client using Server Name Indication (SNI).</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A travel company has a suite of web applications hosted in an Auto Scaling group of On-Demand EC2 instances behind an Application Load Balancer that handles traffic from various web domains such as <code>i-love-manila.com</code>, <code>i-love-boracay.com</code>, <code>i-love-cebu.com</code> and many others. To improve security and lessen the overall cost, you are instructed to secure the system by allowing multiple domains to serve SSL traffic without the need to reauthenticate and reprovision your certificate everytime you add a new domain. This migration from HTTP to HTTPS will help improve their SEO and Google search ranking. </p><p>Which of the following is the most cost-effective solution to meet the above requirement?</p>', 'explanation': '<p>SNI Custom SSL relies on the SNI extension of the Transport Layer Security protocol, which allows multiple domains to serve SSL traffic over the same IP address by including the hostname which the viewers are trying to connect to.&nbsp;</p> <p>You can host multiple TLS secured applications, each with its own TLS certificate, behind a single load balancer. In order to use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client. These features are provided at no additional charge.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/10/05/2017-10-05-00.51.24.gif" width="700" height="482" /></p> <p>&nbsp;</p> <p>To meet the requirements in the scenario,&nbsp;you can upload all SSL certificates of the domains in the ALB using the console and bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client using Server Name Indication (SNI). Hence, Option 4 is correct.</p> <p>Option 1 is incorrect because&nbsp;a wildcard certificate can only handle multiple sub-domains but not different domains.&nbsp;</p> <p>Option 2 is incorrect because although using&nbsp;Subject Alternative Name (SAN) is correct, you will still have to reauthenticate and reprovision your certificate every time you add a new domain. One of the requirements in the scenario is that you should not have to reauthenticate and reprovision your certificate hence, this solution is incorrect.</p> <p>Option 3 is incorrect because although it is valid to use&nbsp;dedicated IP addresses to meet this requirement, this solution is not cost-effective. Remember that if you configure CloudFront to serve HTTPS requests using dedicated IP addresses, you incur an additional monthly charge. The charge begins when you associate your SSL/TLS certificate with&nbsp;your CloudFront distribution. You can just simply upload the certificates to the ALB and use SNI to handle multiple domains in a cost-effective manner.</p> <p>&nbsp;</p> <p><strong>References:</strong>&nbsp;</p> <p><a href="https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/">https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/</a></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-https-dedicated-ip-or-sni.html#cnames-https-dedicated-ip">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-https-dedicated-ip-or-sni.html#cnames-https-dedicated-ip</a></p> <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</a></span></p> <p>&nbsp;</p> <p><strong>SNI Custom SSL vs Dedicated IP Custom SSL:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-sni-custom-ssl-vs-dedicated-ip-custom-ssl/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-sni-custom-ssl-vs-dedicated-ip-custom-ssl/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 8160124, '_class': 'assessment', 'updated': '2019-05-15T01:23:26Z', 'created': '2019-05-15T01:23:26Z', 'question_plain': 'A travel company has a suite of web applications hosted in an Auto Scaling group of On-Demand EC2 instances behind an Application Load Balancer that handles traffic from various web domains such as i-love-manila.com, i-love-boracay.com, i-love-cebu.com and many others. To improve security and lessen the overall cost, you are instructed to secure the system by allowing multiple domains to serve SSL traffic without the need to reauthenticate and reprovision your certificate everytime you add a new domain. This migration from HTTP to HTTPS will help improve their SEO and Google search ranking. Which of the following is the most cost-effective solution to meet the above requirement?', 'id': 9831794, 'related_lectures': [], 'assessment_type': 'multiple-choice'}]}, 'type': 'practice-test', 'title': 'AWS Certified Solutions Architect Associate Practice Test 3'}, {'quiz_data': {'next': None, 'count': 65, 'previous': None, 'results': [{'section': 'EFS', 'prompt': {'answers': ['<p>EFS</p>', '<p>EBS</p>', '<p>S3</p>', '<p>Glacier</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A data analytics company has been building its new generation big data and analytics platform on their AWS cloud infrastructure. They need a storage service that provides the scale and performance that their big data applications require such as high throughput to compute nodes coupled with read-after-write consistency and low-latency file operations. In addition, their data needs to be stored redundantly across multiple AZs and allows concurrent connections from multiple EC2 instances hosted on multiple AZs.\xa0 \xa0</p><p>Which of the following AWS storage services will you use to meet this requirement?</p>', 'explanation': '<p>In this question, you should take note of the two keywords/phrases: "file operation" and "allows concurrent connections from multiple EC2 instances". There are various AWS storage options that you can choose but whenever these criteria show up, always consider using EFS instead of using EBS Volumes which is mainly used as a "block" storage and can only have one connection to one EC2 instance at a time.&nbsp;Amazon EFS provides the scale and performance required for big data applications that require high throughput to compute nodes coupled with read-after-write consistency and low-latency file operations.</p> <p>Amazon EFS is a fully-managed service that makes it easy to set up and scale file storage in the Amazon Cloud. With a few clicks in the AWS Management Console, you can create file systems that are accessible to Amazon EC2 instances via a file system interface (using standard operating system file I/O APIs) and supports full file system access semantics (such as strong consistency and file locking).&nbsp;</p> <p>Amazon EFS file systems can automatically scale from gigabytes to petabytes of data without needing to provision storage. Tens, hundreds, or even thousands of Amazon EC2 instances can access an Amazon EFS file system at the same time, and Amazon EFS provides consistent performance to each Amazon EC2 instance. Amazon EFS is designed to be highly durable and highly available.&nbsp;</p> <p>Option 2 is incorrect because EBS does not allow concurrent connections from multiple EC2 instances hosted on multiple AZs and it does not store data redundantly across multiple AZs by default, unlike EFS.</p> <p>Option 3 is incorrect because although S3 can handle concurrent connections from multiple EC2 instances, it does not have the ability&nbsp;to provide low-latency file operations, which is required in this scenario.</p> <p>Option 4 is incorrect&nbsp;because Glacier is an archiving storage solution and is not applicable in this scenario.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/efs/latest/ug/performance.html">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p> <p><a href="https://aws.amazon.com/efs/faq/">https://aws.amazon.com/efs/faq/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EFS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-efs/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-efs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 vs EBS vs EFS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3-vs-ebs-vs-efs/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3-vs-ebs-vs-efs/</a></p> <p>&nbsp;</p> <p><strong>Here\'s a short video tutorial on Amazon EFS:</strong></p> <p><iframe src="https://www.youtube.com/embed/AvgAozsfCrY" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567118, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'A data analytics company has been building its new generation big data and analytics platform on their AWS cloud infrastructure. They need a storage service that provides the scale and performance that their big data applications require such as high throughput to compute nodes coupled with read-after-write consistency and low-latency file operations. In addition, their data needs to be stored redundantly across multiple AZs and allows concurrent connections from multiple EC2 instances hosted on multiple AZs.\xa0 \xa0Which of the following AWS storage services will you use to meet this requirement?', 'id': 10337380, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['Enable AWS CloudTrail to audit all Amazon S3 bucket access.', 'Enable server access logging for all required Amazon S3 buckets.', 'Enable the Requester Pays option to track access via AWS Billing.', 'Enable Amazon S3 Event Notifications for PUT and POST.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You are employed by a large electronics company that uses Amazon Simple Storage Service. For reporting purposes, they want to track and log every request access to their S3 buckets including the requester, bucket name, request time, request action, response status, and error code information. They also use this information for their internal security and access audits. <br><br>Which is the best solution among the following options that can satisfy the company requirement?', 'explanation': '<p>For this scenario, you can use CloudTrail and the Server Access Logging feature of Amazon S3. However, the question mentioned that it needs detailed information about every access request sent to the S3 bucket such as requestor, bucket name, request time, request action, response status, and error code information. Cloudtrail can only log the API calls and provides less information compared with the Server Access Logging feature in S3. Hence, the correct answer is Option 2.</p> <p>Option 3 is incorrect because this action&nbsp;refers to AWS billing and not for logging.</p> <p>Option 4 is incorrect because we are looking for a logging solution and not event notification.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567120, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are employed by a large electronics company that uses Amazon Simple Storage Service. For reporting purposes, they want to track and log every request access to their S3 buckets including the requester, bucket name, request time, request action, response status, and error code information. They also use this information for their internal security and access audits. Which is the best solution among the following options that can satisfy the company requirement?', 'id': 10337382, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Storage Gateway', 'prompt': {'answers': ['Amazon EC2', 'Amazon Storage Gateway', '<p>Amazon Elastic Block Storage</p>', 'Amazon SQS'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a tech company which currently has an on-premises infrastructure. They are currently running low on storage and want to have the ability to extend their storage using AWS cloud. <br><br>Which AWS service can help you achieve this requirement?</p>', 'explanation': '<p>AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure. You can use the service to store data in the AWS Cloud for scalable and cost-effective storage that helps maintain data security.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/storagegateway/latest/userguide/images/aws-storage-gateway-stored-diagram.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect since EC2 is a compute service, not a storage service.</p> <p>Option 3 is incorrect since EBS is primarily used as a storage of your EC2 instances.</p> <p>Option 4 is incorrect since SQS is a message queuing service, and does not extend your on-premises storage capacity.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html">http://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-storage-gateway/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-storage-gateway/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567122, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are working for a tech company which currently has an on-premises infrastructure. They are currently running low on storage and want to have the ability to extend their storage using AWS cloud. Which AWS service can help you achieve this requirement?', 'id': 10337384, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'answers': ['<p>Create a new inbound rule in the security group of the EC2 instance with the following details: </p><p>Protocol: TCP </p><p>Port Range: 20 - 21 </p><p>Source: <code>175.45.116.100/32</code></p>', '<p>Create a new inbound rule in the security group of the EC2 instance with the following details:</p><p>Protocol: UDP </p><p>Port Range: 20 - 21 </p><p>Source: <code>175.45.116.100/32</code></p>', '<p>Create a new Network ACL inbound rule in the subnet of the EC2 instance with the following details: </p><p>Protocol: TCP </p><p>Port Range: 20 - 21 </p><p>Source: <code>175.45.116.100/0</code></p><p>Allow/Deny: ALLOW</p>', '<p>Create a new Network ACL inbound rule in the subnet of the EC2 instance with the following details: </p><p>Protocol: UDP </p><p>Port Range: 20 - 21 </p><p>Source: <code>175.45.116.100/0</code> </p><p>Allow/Deny: ALLOW</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You recently launched a new FTP server using an On-Demand EC2 instance in a newly created VPC with default settings. The server should not be accessible publicly but only through your IP address <code>175.45.116.100</code> and nowhere else. </p><p>Which of the following is the most suitable way to implement this requirement?</p>', 'explanation': '<p>The FTP protocol uses TCP via ports 21 and 22. This should be configured in your security groups or in your Network ACL inbound rules. As&nbsp;required by the scenario, you should only allow&nbsp;the individual IP of the client and not the entire network. Therefore, in the&nbsp;<strong>Source,&nbsp;</strong>&nbsp;the proper CIDR notation should be used. The <strong>/32</strong> denotes one IP address and the <strong>/0</strong> refers to the entire network.</p> <p>Notice that the scenario says that you launched the EC2 instances in a newly created VPC with default settings. Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic. Hence, you actually don\'t need to explicitly add inbound rules to your Network ACL to allow inbound traffic, if your VPC has a default setting.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/userguide/images/security-diagram.png" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect because although the configuration of the Security Group is valid, the provided Protocol is incorrect. Take note that FTP uses TCP and not UDP.</p> <p>Option 3 is incorrect because although setting up an inbound Network ACL is valid,&nbsp;the source is invalid since it must be an IPv4 or IPv6 CIDR block.&nbsp;In the provided IP address, the&nbsp;<strong>/0</strong> refers to the entire network and not a specific IP address. In addition, the scenario says that the newly created VPC has default settings and by default, the Network ACL allows all traffic. This means that there is actually no need to configure your Network ACL.</p> <p>Option 4 is incorrect because, just like Option 3, the source is also invalid.&nbsp;Take note that FTP uses TCP and not UDP, which is one of the reasons why this option is wrong.&nbsp;In addition, the scenario says that the newly created VPC has default settings and by default, the Network ACL allows all traffic. This means that there is actually no need to configure your Network ACL.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567124, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You recently launched a new FTP server using an On-Demand EC2 instance in a newly created VPC with default settings. The server should not be accessible publicly but only through your IP address 175.45.116.100 and nowhere else. Which of the following is the most suitable way to implement this requirement?', 'id': 10337386, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['http://169.254.169.254/latest/meta-data/public-ipv4', 'http://169.255.169.255/latest/meta-data/public-ipv4', 'http://254.169.254.169/metadata/public-ipv4', 'http://255.169.255.169/latest/public-ipv4'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'The IT Operations team of your company wants to retrieve all of the Public IP addresses assigned to a running EC2 instance via the Instance metadata. <br><br>Which of the following URLs will you use?', 'explanation': '<p>http://169.254.169.254/latest/meta-data/ is the URL that you can use to retrieve the Instance Metadata of your EC2 instance, including the public-hostname, public-ipv4, public-keys et cetera.</p> <p>This can be helpful when you\'re writing scripts to run from your instance as it enables you to access the local IP address of your instance from the instance metadata to manage a connection to an external application. Remember that you are not billed for HTTP requests used to retrieve instance metadata and user data.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567126, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'The IT Operations team of your company wants to retrieve all of the Public IP addresses assigned to a running EC2 instance via the Instance metadata. Which of the following URLs will you use?', 'id': 10337388, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Amazon MQ', 'prompt': {'answers': ['<p>Amazon SNS</p>', '<p>Amazon MQ</p>', '<p>Amazon SQS</p>', '<p>Amazon SWF</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A customer is transitioning their ActiveMQ messaging broker service onto the AWS cloud in which they require an alternative asynchronous service that supports NMS and MQTT messaging protocol. The customer does not have the time and resources needed to recreate their messaging service in the cloud. The service has to be highly available and should require almost no management overhead. </p><p>Which of the following is the most suitable service to use to meet the above requirement?</p>', 'explanation': '<p>Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Connecting your current applications to Amazon MQ is easy because it uses industry-standard APIs and protocols for messaging, including JMS, NMS, AMQP, STOMP, MQTT, and WebSocket. Using standards means that in most cases, there&rsquo;s no need to rewrite any messaging code when you migrate to AWS.</p> <p>Amazon MQ, Amazon SQS, and Amazon SNS are messaging services that are suitable for anyone from startups to enterprises. If you\'re using messaging with existing applications and want to move your messaging&nbsp;service to the cloud quickly and easily, it is&nbsp;recommended that you consider Amazon MQ. It supports industry-standard APIs and protocols so you can switch from any standards-based message broker to Amazon MQ without rewriting the messaging code in your applications. Hence, Option 2 is the correct answer.</p> <p>If you are building brand new applications in the cloud, then it is highly recommended that you consider Amazon SQS and Amazon SNS. Amazon SQS and SNS are lightweight, fully managed message queue and topic services that scale almost infinitely and provide simple, easy-to-use APIs. You can use Amazon SQS and SNS to decouple and scale microservices, distributed systems, and serverless applications, and improve reliability.</p> <p>&nbsp;</p> <p><strong><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/images/amazon-mq-architecture-active-standby-deployment.png" /></strong></p> <p>&nbsp;</p> <p>Option 1 is incorrect because SNS is more suitable as a&nbsp;pub/sub messaging service instead of&nbsp;a message broker service.&nbsp;</p> <p>Option 3 is incorrect because although Amazon SQS is a&nbsp;fully managed message queuing service, it does not support an extensive&nbsp;list of industry-standard messaging APIs and protocol, unlike Amazon MQ. Moreover, using Amazon SQS requires you to do additional changes in the messaging code of applications to make it compatible.</p> <p>Option 4 is incorrect because SWF is a fully-managed state tracker and task coordinator service and not a messaging service, unlike Amazon MQ, AmazonSQS, and Amazon SNS.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/amazon-mq/">https://aws.amazon.com/amazon-mq/</a></p> <p><a href="https://aws.amazon.com/messaging/">https://aws.amazon.com/messaging/</a></p> <p><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html#sqs-difference-from-amazon-mq-sns">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html#sqs-difference-from-amazon-mq-sns</a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567142, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'A customer is transitioning their ActiveMQ messaging broker service onto the AWS cloud in which they require an alternative asynchronous service that supports NMS and MQTT messaging protocol. The customer does not have the time and resources needed to recreate their messaging service in the cloud. The service has to be highly available and should require almost no management overhead. Which of the following is the most suitable service to use to meet the above requirement?', 'id': 10337404, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFormation', 'prompt': {'answers': ['AWS Elastic Beanstalk', 'AWS SQS', '<p>AWS CloudFormation</p>', 'AWS SNS'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>The company you are working for has a set of AWS resources hosted in ap-northeast-1 region. You have been requested by your IT Manager to create a shell script which could create duplicate resources in another region in the event that ap-northeast-1 region fails.<br><br>Which of the following AWS services could help fulfill this task?</p>', 'explanation': '<p>AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS.</p> <p>You can create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you. With this, you can deploy an exact copy of your AWS architecture, along with all of the AWS resources which are hosted in one region to another.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudFormation Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567128, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'The company you are working for has a set of AWS resources hosted in ap-northeast-1 region. You have been requested by your IT Manager to create a shell script which could create duplicate resources in another region in the event that ap-northeast-1 region fails.Which of the following AWS services could help fulfill this task?', 'id': 10337390, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Auto Scaling', 'prompt': {'answers': ['Just change the instance type to <code>t2.2xlarge</code> in the current launch configuration', 'Create another Auto Scaling Group and attach the new instance type.', 'Create a new launch configuration with the new instance type and update the Auto Scaling Group.', 'Change the instance type of each EC2 instance manually.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You have an Auto Scaling group which is configured to launch new <code>t2.micro</code> EC2 instances when there is a significant load increase in the application. To cope with the demand, you now need to replace those instances with a larger <code>t2.2xlarge</code> instance type. How would you implement this change?', 'explanation': '<p>You can only specify one launch configuration for an Auto Scaling group at a time, and you can\'t modify a launch configuration after you\'ve created it.&nbsp;Therefore, if you want to change the launch configuration for an Auto Scaling group, you must create a launch configuration and then update your Auto Scaling group with the new launch configuration.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567130, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You have an Auto Scaling group which is configured to launch new t2.micro EC2 instances when there is a significant load increase in the application. To cope with the demand, you now need to replace those instances with a larger t2.2xlarge instance type. How would you implement this change?', 'id': 10337392, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudWatch', 'prompt': {'answers': ['<p>Install AWS SDK in the EC2 instances. Create a script that will trigger the Auto Scaling event if there is a high memory usage.</p>', '<p>Install CloudWatch monitoring scripts in the instances. Send custom metrics to CloudWatch which will trigger your Auto Scaling group to scale up.</p>', '<p>Enable detailed monitoring on the instances.</p>', '<p>Modify the scaling policy to increase the threshold to scale up the number of instances.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>An auto-scaling group of Linux EC2 instances is created with basic monitoring enabled in CloudWatch. You noticed that your application is slow so you asked one of your engineers to check all of your EC2 instances. After checking your instances, you noticed that the auto scaling group is not launching more instances as it should be, even though the servers already have high memory usage. </p><p>What is the best solution that will fix this issue?</p>', 'explanation': '<p>The Amazon CloudWatch Monitoring Scripts for Amazon Elastic Compute Cloud (Amazon EC2) Linux-based instances demonstrate how to produce and consume Amazon CloudWatch custom metrics. These sample Perl scripts comprise a fully functional example that reports memory, swap, and disk space utilization metrics for a Linux instance.</p> <p>Option 2 is correct because CloudWatch does not monitor EC2 memory usage as well as disk space utilization. You would have to send custom metrics to CloudWatch.</p> <p>Option 1 is incorrect because AWS SDK is a set of programming tools that allow you to create applications that run using Amazon cloud services. You would have to program the alert which is not the best strategy for this scenario.</p> <p>Option 3 is incorrect because detailed monitoring does not provide metrics for memory usage. Cloudwatch does not monitor memory usage in its default set of EC2 metrics and detailed monitoring just provides higher frequency of metrics (1-minute frequency).</p> <p>Option 4 is incorrect because you are already maxing out your usage, which should in effect cause an auto-scaling event.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html</a> <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 and CloudWatch Cheat Sheets:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</a></span></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567132, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'An auto-scaling group of Linux EC2 instances is created with basic monitoring enabled in CloudWatch. You noticed that your application is slow so you asked one of your engineers to check all of your EC2 instances. After checking your instances, you noticed that the auto scaling group is not launching more instances as it should be, even though the servers already have high memory usage. What is the best solution that will fix this issue?', 'id': 10337394, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EMR', 'prompt': {'answers': ['Amazon S3 for storing the application log files and Amazon Elastic MapReduce for processing the log files.', 'Amazon Glacier for storing the application log files and Spot EC2 Instances for processing them.', 'A single On-Demand Amazon EC2 instance for both storing and processing the log files', 'Amazon RedShift to store the logs and Amazon Lambda for running custom log analysis scripts'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You have a set of linux servers running on multiple On-Demand EC2 Instances. The Audit team wants to collect and process the application log files generated from these servers for their report.  <br><br>Which of the following services is the best to use in this case?', 'explanation': '<p>Amazon EMR is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data. By using these frameworks and related open-source projects such as Apache Hive and Apache Pig, you can process data for analytics purposes and business intelligence workloads. Additionally, you can use Amazon EMR to transform and move large amounts of data into and out of other AWS data stores and databases such as Amazon Simple Storage Service (Amazon S3) and Amazon DynamoDB.</p> <p>Option 2 is wrong as Amazon Glacier is used for data archive only.</p> <p>Option 3 is wrong as an EC2 instance is not a recommended storage service. In addition, Amazon EC2 does not have a built-in data processing engine to process large amounts of data.</p> <p>Option 4 is wrong as Amazon RedShift&nbsp;is mainly used as a data warehouse service.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon EMR Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-emr/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-emr/</span></a></p> <p>&nbsp;</p> <p><strong>Here is an in-depth tutorial on Amazon EMR:</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/jylp2atrZjc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'}, 'correct_response': ['a'], 'original_assessment_id': 2567134, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You have a set of linux servers running on multiple On-Demand EC2 Instances. The Audit team wants to collect and process the application log files generated from these servers for their report.  Which of the following services is the best to use in this case?', 'id': 10337396, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['A running EC2 Instance', '<p>A stopped On-Demand EC2 Instance</p>', 'EBS Volumes attached to stopped EC2 Instances', 'Using an Amazon VPC', 'Public Data Set'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>To save costs, your manager instructed you to analyze and review the setup of your AWS cloud infrastructure. You should also provide an estimate of how much your company will pay for all of the AWS resources that they are using. In this scenario, which of the following will incur costs? (Choose 2)</p>', 'explanation': '<p>Billing commences when Amazon EC2 initiates the boot sequence of an AMI instance. Billing ends when the instance terminates, which could occur through a web services command, by running "shutdown -h", or through instance failure. When you stop an instance, AWS shuts it down but don\'t charge hourly usage for a stopped instance or data transfer fees, but AWS does charge for the storage of any Amazon EBS volumes. Hence, options 1 and 3 are the right answers and conversely, options 2 and 6 are incorrect as there is no charge for a terminated EC2 instance&nbsp;that you have shut down.</p> <p>Option 4 is incorrect because there are no additional charges for creating and using the VPC itself. Usage charges for other Amazon Web Services, including Amazon EC2, still apply at published rates for those resources, including data transfer charges.</p> <p>Option 5 is incorrect due to the fact that Amazon stores the data sets at no charge to the community and, as with all AWS services, you pay only for the compute and storage you use for your own applications.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a></p> <p><a href="https://aws.amazon.com/vpc/faqs">https://aws.amazon.com/vpc/faqs</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2567136, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'To save costs, your manager instructed you to analyze and review the setup of your AWS cloud infrastructure. You should also provide an estimate of how much your company will pay for all of the AWS resources that they are using. In this scenario, which of the following will incur costs? (Choose 2)', 'id': 10337398, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Step Functions', 'prompt': {'answers': ['<p>SWF</p>', '<p>AWS Lambda</p>', '<p>AWS Step Functions</p>', '<p>AWS Batch</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A financial company instructed you to automate the recurring tasks in your department such as patch management, infrastructure selection, and data synchronization to improve their current processes. You need to have a service which can coordinate multiple AWS services into serverless workflows.\xa0 \xa0</p><p>Which of the following is the most cost-effective service to use in this scenario?</p>', 'explanation': '<p>AWS Step Functions provides serverless orchestration for modern applications. Orchestration centrally manages a workflow by breaking it into multiple steps, adding flow logic, and tracking the inputs and outputs between the steps. As your applications execute, Step Functions maintains application state, tracking exactly which workflow step your application is in, and stores an event log of data that is passed between application components. That means that if networks fail or components hang, your application can pick up right where it left off.</p> <p>Application development is faster and more intuitive with Step Functions, because you can define and manage the workflow of your application independently from its business logic. Making changes to one does not affect the other. You can easily update and modify workflows in one place, without having to struggle with managing, monitoring and maintaining multiple point-to-point integrations. Step Functions frees your functions and containers from excess code, so your applications are faster to write, more resilient, and easier to maintain.</p> <p>Option 1 is incorrect because SWF is a fully-managed state tracker and task coordinator service. It does not provide serverless orchestration to multiple AWS resources.</p> <p>Option 2 is incorrect because although Lambda is used for serverless computing, it does not provide a direct way to coordinate multiple AWS services into serverless workflows.</p> <p>Option 4 is incorrect because AWS Batch is primarily used to efficiently run hundreds of thousands of batch computing jobs in AWS.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://aws.amazon.com/step-functions/features/">https://aws.amazon.com/step-functions/features/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Step Functions Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-step-functions/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-step-functions/</span></a></strong></p> <p>&nbsp;</p> <p><strong>Amazon Simple Workflow (SWF) vs AWS Step Functions vs Amazon SQS:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-swf-vs-aws-step-functions-vs-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-swf-vs-aws-step-functions-vs-amazon-sqs/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567138, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'A financial company instructed you to automate the recurring tasks in your department such as patch management, infrastructure selection, and data synchronization to improve their current processes. You need to have a service which can coordinate multiple AWS services into serverless workflows.\xa0 \xa0Which of the following is the most cost-effective service to use in this scenario?', 'id': 10337400, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EIP', 'prompt': {'answers': ['Deploy a NAT instance into the public subnet.', '<p>Assign an Elastic IP address to the fifth\xa0instance.</p>', '<p>Configure a publicly routable IP Address in the host OS of the fifth instance.</p>', 'Modify the routing table for the public subnet.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'The game development company that you are working for has an Amazon VPC with a public subnet. It has 4 EC2 instances that are deployed in the public subnet. These 4 instances can successfully communicate with other hosts on the Internet. You launch a fifth instance in the same public subnet, using the same AMI and security group configuration that you used for the others. However, this new instance cannot be accessed from the internet unlike the other instance. <br><br>What should you do to enable access to the fifth instance over the Internet?', 'explanation': '<p>An&nbsp;<em>Elastic IP address</em>&nbsp;is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.</p> <p>An Elastic IP address is a public IPv4 address, which is reachable from the Internet. If your instance does not have a public IPv4 address, you can associate an Elastic IP address with your instance to enable communication with the Internet; for example, to connect to your instance from your local computer.</p> <p>Option 1 is incorrect because it is already mentioned that your instances are in a public subnet. You only have to configure a NAT instance when your instances are on a private subnet.</p> <p>Option 2 is the correct answer because you need to either add a public address or add an EIP for this EC2 instance for it to be able to access the internet.</p> <p>Option 3 is incorrect because the public IP address has to be configured in the Elastic Network Interface (ENI) of the EC2 instance and not on its Operating System (OS).</p> <p>Option 4 is incorrect because if the routing table was wrong then you would have an issue with the other 4 instances.</p> <p>&nbsp;</p> <p>References:&nbsp;</p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html</a></p> <p>&nbsp;</p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567140, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'The game development company that you are working for has an Amazon VPC with a public subnet. It has 4 EC2 instances that are deployed in the public subnet. These 4 instances can successfully communicate with other hosts on the Internet. You launch a fifth instance in the same public subnet, using the same AMI and security group configuration that you used for the others. However, this new instance cannot be accessed from the internet unlike the other instance. What should you do to enable access to the fifth instance over the Internet?', 'id': 10337402, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFormation', 'prompt': {'answers': ['<p>Provides highly durable and scalable data storage</p>', 'A storage location for the code of your application', '<p>Enables modeling, provisioning, and version-controlling of your entire AWS infrastructure</p>', 'Allows you to model your entire infrastructure in a text file', '<p>Using CloudFormation itself is free, including the AWS resources that have been created.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': "<p>A technology company is building a new cryptocurrency trading platform that allows buying and selling of Bitcoin, Ethereum, XRP, Ripple and many others. You were hired as a Cloud Engineer to build the required infrastructure needed for this new trading platform. On your first week at work, you started to create CloudFormation YAML scripts that defines all of the needed AWS resources for the application. Your manager was shocked that you haven't created the EC2 instances, S3 buckets and other AWS resources straight away. He does not understand the text-based scripts that you have done and was disappointed that you are just slacking off at your job.\xa0 </p><p>In this scenario, what are the benefits of using the Amazon CloudFormation service that you should tell your manager to clarify his concerns? (Choose 2)</p>", 'explanation': '<p>AWS CloudFormation provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This file serves as the single source of truth for your cloud environment. AWS CloudFormation is available at no additional charge, and you pay only for the AWS resources needed to run your applications.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/create-stack-diagram.png" /></p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/cloudformation/">https://aws.amazon.com/cloudformation/</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudFormation Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/</span></a></p>'}, 'correct_response': ['c', 'd'], 'original_assessment_id': 2567146, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': "A technology company is building a new cryptocurrency trading platform that allows buying and selling of Bitcoin, Ethereum, XRP, Ripple and many others. You were hired as a Cloud Engineer to build the required infrastructure needed for this new trading platform. On your first week at work, you started to create CloudFormation YAML scripts that defines all of the needed AWS resources for the application. Your manager was shocked that you haven't created the EC2 instances, S3 buckets and other AWS resources straight away. He does not understand the text-based scripts that you have done and was disappointed that you are just slacking off at your job.\xa0 In this scenario, what are the benefits of using the Amazon CloudFormation service that you should tell your manager to clarify his concerns? (Choose 2)", 'id': 10337406, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'CloudWatch', 'prompt': {'answers': ['First, look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance.', 'First, look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create an alarm in Amazon SNS for that custom metric which invokes an action to restart the EC2 instance.', 'First, look at the existing Flow logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance.', 'First, look at the existing Flow logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which calls a Lambda function that invokes an action to restart the EC2 instance.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You have a web application hosted in AWS cloud where the application logs are sent to Amazon CloudWatch. Lately, the web application has recently been encountering some errors which can be resolved simply by restarting the instance. <br><br>What will you do to automatically restart the EC2 instances whenever the same application error occurs?', 'explanation': '<p>In this scenario, you can&nbsp;look at the&nbsp;existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance.&nbsp;</p> <p>You can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances using Amazon CloudWatch alarm actions.&nbsp;You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.</p> <p>Option 2 is incorrect because you can\'t create an alarm in Amazon SNS.</p> <p>Options 3 and 4 are incorrect because Flow Logs are used in VPC and not on specific EC2 instance.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567148, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You have a web application hosted in AWS cloud where the application logs are sent to Amazon CloudWatch. Lately, the web application has recently been encountering some errors which can be resolved simply by restarting the instance. What will you do to automatically restart the EC2 instances whenever the same application error occurs?', 'id': 10337408, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Redshift Spectrum', 'prompt': {'answers': ['<p>S3 Select, Amazon Neptune, DynamoDB DAX\xa0 </p>', '<p>Amazon X-Ray, Amazon Neptune, DynamoDB\xa0 </p>', '<p>Amazon Glue, Glacier Select, Amazon Redshift\xa0 </p>', '<p>S3 Select, Amazon Athena, Amazon Redshift Spectrum\xa0 </p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A real-time data analytics application is using AWS Lambda to process data and store results in JSON format to an S3 bucket. To speed up the existing workflow, you have to use a service where you can run sophisticated Big Data analytics on your data without moving them into a separate analytics system.\xa0 \xa0</p><p>Which of the following group of services can you use to meet this requirement?\xa0 </p>', 'explanation': '<p>Amazon S3 allows you to run sophisticated Big Data analytics on your data without moving the data into a separate analytics system. In AWS, there is a suite of tools that make analyzing and processing large amounts of data in the cloud faster, including ways to optimize and integrate existing workflows with Amazon S3:</p> <p><strong>1. S3 Select </strong></p> <p>Amazon S3 Select is designed to help analyze and process data within an object in Amazon S3 buckets, faster and cheaper. It works by providing the ability to retrieve a subset of data from an object in Amazon S3 using simple SQL expressions. Your applications no longer have to use compute resources to scan and filter the data from an object, potentially increasing query performance by up to 400%, and reducing query costs as much as 80%. You simply change your application to use SELECT instead of GET to take advantage of S3 Select.</p> <p><strong>2. Amazon Athena </strong></p> <p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL expressions. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries you run. Athena is easy to use. Simply point to your data in Amazon S3, define the schema, and start querying using standard SQL expressions. Most results are delivered within seconds. With Athena, there&rsquo;s no need for complex ETL jobs to prepare your data for analysis. This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.</p> <p><strong>3. Amazon Redshift Spectrum </strong></p> <p>Amazon Redshift also includes Redshift Spectrum, allowing you to directly run SQL queries against exabytes of unstructured data in Amazon S3. No loading or transformation is required, and you can use open data formats, including Avro, CSV, Grok, ORC, Parquet, RCFile, RegexSerDe, SequenceFile, TextFile, and TSV. Redshift Spectrum automatically scales query compute capacity based on the data being retrieved, so queries against Amazon S3 run fast, regardless of data set size.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/s3/features/#Query_in_Place">https://aws.amazon.com/s3/features/#Query_in_Place</a></p> <p>&nbsp;</p> <p><strong>Check out these AWS Cheat Sheets:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-athena/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-athena/</span></a></strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/</span></a></strong></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567150, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'A real-time data analytics application is using AWS Lambda to process data and store results in JSON format to an S3 bucket. To speed up the existing workflow, you have to use a service where you can run sophisticated Big Data analytics on your data without moving them into a separate analytics system.\xa0 \xa0Which of the following group of services can you use to meet this requirement?', 'id': 10337410, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['By using IAM.', '<p>By using a CloudWatch metric.</p>', 'By using a Curl or Get Command to get the latest metadata information from http://169.254.169.254/latest/meta-data/', 'By using a Curl or Get Command to get the latest user data information from http://169.254.169.254/latest/user-data/'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': "You have a web application hosted in an On-Demand EC2 instance in your VPC. You are creating a shell script that needs the instance's public and private IP addresses. <br><br>What is the best way to get the instance's associated IP addresses which your shell script can use?", 'explanation': '<p>Instance metadata is data about your EC2 instance that you can use to configure or manage the running instance.&nbsp;Because your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI. This can be helpful when you\'re writing scripts to run from your instance. For example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application.</p> <p>To view the private IPv4 address, public IPv4 address, and all other categories of instance metadata from within a running instance, use the following URL:</p> <pre><code>http://169.254.169.254/latest/meta-data/</code></pre> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567152, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': "You have a web application hosted in an On-Demand EC2 instance in your VPC. You are creating a shell script that needs the instance's public and private IP addresses. What is the best way to get the instance's associated IP addresses which your shell script can use?", 'id': 10337412, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['<p>Spot volumes provide the lowest cost per gigabyte of all EBS volume types and are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important.</p>', '<p>Provisioned IOPS volumes offer storage with consistent and low-latency performance, and are designed for I/O intensive applications such as large relational or NoSQL databases. </p>', '<p>Magnetic volumes provide the lowest cost per gigabyte of all EBS volume types and are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important.</p>', '<p>Reduced Redundancy Storage volumes offer consistent and low-latency performance, and are designed for I/O intensive applications such as large relational or NoSQL databases. </p>', '<p>Single root I/O virtualization (SR-IOV) volumes are suitable for a broad range of workloads, including small to medium sized databases, development and test environments, and boot volumes. </p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are the technical lead of the Cloud Infrastructure team in your company and you were consulted by a software developer regarding the required AWS resources of the web application that he is building. He knows that an Instance Store only provides ephemeral storage where the data is automatically deleted when the instance is terminated. To ensure that the data of his web application persists, the app should be launched in an EC2 instance that has a durable, block-level storage volume attached. He knows that they need to use an EBS volume, but they are not sure what type they need to use.\xa0 </p><p>In this scenario, which of the following is true about Amazon EBS volume types and their respective usage? (Choose 2)</p>', 'explanation': '<p>Amazon EBS provides three volume types to best meet the needs of your workloads: General Purpose (SSD), Provisioned IOPS (SSD), and Magnetic.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/overview_getting_started.png" alt="" width="409" height="338" /></p> <p>&nbsp;</p> <p>General Purpose (SSD) is the new, SSD-backed, general purpose EBS volume type that we recommend as the default choice for customers. General Purpose (SSD) volumes are suitable for a broad range of workloads, including small to medium sized databases, development, and test environments, and boot volumes.</p> <p>Provisioned IOPS (SSD) volumes offer storage with consistent and low-latency performance and are designed for I/O intensive applications such as large relational or NoSQL databases. Magnetic volumes provide the lowest cost per gigabyte of all EBS volume types.</p> <p>Magnetic volumes are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;&nbsp;</strong></p> <p><a href="https://aws.amazon.com/ec2/instance-types/">https://aws.amazon.com/ec2/instance-types/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['b', 'c'], 'original_assessment_id': 2567154, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are the technical lead of the Cloud Infrastructure team in your company and you were consulted by a software developer regarding the required AWS resources of the web application that he is building. He knows that an Instance Store only provides ephemeral storage where the data is automatically deleted when the instance is terminated. To ensure that the data of his web application persists, the app should be launched in an EC2 instance that has a durable, block-level storage volume attached. He knows that they need to use an EBS volume, but they are not sure what type they need to use.\xa0 In this scenario, which of the following is true about Amazon EBS volume types and their respective usage? (Choose 2)', 'id': 10337414, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'ARN', 'prompt': {'answers': ['<p>AWS Resource ID</p>', '<p>AWS Service Namespaces</p>', 'Amazon Resource Name', '<p>Tags</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A company is using hundreds of AWS resources in multiple AWS regions. They require a way to uniquely identify all of their AWS resources that will allow them to specify a resource unambiguously across all of AWS, such as in IAM policies, Amazon Relational Database Service (Amazon RDS) tags, and API calls.\xa0 \xa0</p><p>Which of the following is the most suitable option to use in this scenario?\xa0 </p>', 'explanation': '<p>Amazon Resource Names (ARNs) uniquely identify AWS resources. We require an ARN when you need to specify a resource unambiguously across all of AWS, such as in IAM policies, Amazon Relational Database Service (Amazon RDS) tags, and API calls.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/mobile/sdkforxamarin/developerguide/images/edit-permissions-dynamodb.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because&nbsp;an AWS Resource ID is primarily used to find your resources in the Amazon EC2 console only and not your entire VPC or AWS account.</p> <p>Option 2 is incorrect because AWS Service Namespaces only helps you identify an AWS service and not a unique resource.&nbsp;For example, the namespace for Amazon S3 is&nbsp;<strong><code class="code">s3</code></strong>, and the namespace for Amazon EC2 is&nbsp;<strong><code class="code">ec2</code></strong>.</p> <p>Option 4 is incorrect because&nbsp;although Tags&nbsp;can enable you to categorize your AWS resources by purpose, owner, or environment, it is still limited because you cannot tag all of your AWS resources. Take note that you cannot tag Egress-only internet gateway, VPC flow log, VPC endpoint, and many others. Amazon Resource Names (ARNs) uniquely identify all of your AWS resources which is a more suitable option for this scenario.</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html">https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/resource-ids.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/resource-ids.html</a></p> <p>&nbsp;</p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567156, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'A company is using hundreds of AWS resources in multiple AWS regions. They require a way to uniquely identify all of their AWS resources that will allow them to specify a resource unambiguously across all of AWS, such as in IAM policies, Amazon Relational Database Service (Amazon RDS) tags, and API calls.\xa0 \xa0Which of the following is the most suitable option to use in this scenario?', 'id': 10337416, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudWatch', 'prompt': {'answers': ['AWS Cloudtrail', 'AWS Cloudwatch', 'AWS SWF', 'AWS SQS'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You are a Solutions Architect for a large London-based software company. You are assigned to improve the performance and current processes of supporting the AWS resources in your VPC. Upon checking, you noticed that the Operations team does not have an automated way to monitor and resolve issues with their on-demand EC2 instances.<br><br>What can be used to automatically monitor your EC2 instances and notify the Operations team for any incidents?', 'explanation': '<p>Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms.</p> <p>Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. You can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. You can use these insights to react and keep your application running smoothly.</p> <p>Option 1 is incorrect as CloudTrail is mainly used for logging and not for monitoring.</p> <p>Options 3 and 4 are incorrect as SWF and SQS are used for creating distributed application with decoupled components and not for monitoring.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/cloudwatch/faqs/">https://aws.amazon.com/cloudwatch/faqs/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567158, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are a Solutions Architect for a large London-based software company. You are assigned to improve the performance and current processes of supporting the AWS resources in your VPC. Upon checking, you noticed that the Operations team does not have an automated way to monitor and resolve issues with their on-demand EC2 instances.What can be used to automatically monitor your EC2 instances and notify the Operations team for any incidents?', 'id': 10337418, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFront', 'prompt': {'answers': ['<p>Use AWS CloudFront with website as the custom origin.</p>', 'For better read throughput, use AWS Storage Gateway to distribute the content across multiple regions.', "Use Amazon ElastiCache for the website's in-memory data store or cache.", 'Deploy the website to all regions in different VPCs for faster processing.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are responsible for running a global news website hosted in a fleet of EC2 Instances. Lately, the load on the website has increased which resulted to slower response time for the site visitors. This issue impacts the revenue of the company as some readers tend to leave the site if it does not load after 10 seconds.\xa0 \xa0</p><p>Which of the below services in AWS can be used to solve this problem? (Choose 2)</p>', 'explanation': '<p>The global news website has a problem with latency considering that there are a lot of readers of the site from all parts of the globe. In this scenario, you can use a&nbsp;content delivery network (CDN) which is a geographically distributed group of servers which work together to provide fast delivery of Internet content. And since this is a news website, most of its data are read-only, which can be cached to improve the read throughput and avoid the repetitive requests from the server.</p> <p>In AWS, Amazon CloudFront is the global content delivery network (CDN) service that you can use and for web caching,&nbsp;Amazon ElastiCache is the suitable service. Hence, the answers here are options 1 and 3.</p> <p>Option 2 is incorrect as&nbsp;AWS Storage Gateway is used for storage.</p> <p>Option 4 is incorrect as this would be costly and totally unnecessary considering that you can use Amazon CloudFront and ElastiCache to improve the performance of the website.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/elasticache/">https://aws.amazon.com/elasticache/</a></p> <p><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</span></a></p>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2567160, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are responsible for running a global news website hosted in a fleet of EC2 Instances. Lately, the load on the website has increased which resulted to slower response time for the site visitors. This issue impacts the revenue of the company as some readers tend to leave the site if it does not load after 10 seconds.\xa0 \xa0Which of the below services in AWS can be used to solve this problem? (Choose 2)', 'id': 10337420, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EBS', 'prompt': {'answers': ['400', '500', '600', '800'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You are creating a Provisioned IOPS volume in AWS. The size of the volume is 10 GiB.<br><br>Which of the following is the correct value that should be put for the IOPS of the volume?', 'explanation': '<p>Provisioned IOPS SSD (<code class="code">io1</code>) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike&nbsp;<code class="code">gp2</code>, which uses a bucket and credit model to calculate performance, an&nbsp;<code class="code">io1</code>&nbsp;volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers within 10 percent of the provisioned IOPS performance 99.9 percent of the time over a given year.</p> <p>An&nbsp;<code class="code">io1</code>&nbsp;volume can range in size from 4 GiB to 16 TiB. You can provision from 100 IOPS up to 64,000 IOPS per volume on&nbsp;<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html#ec2-nitro-instances">Nitro system</a>&nbsp;instance families and up to 32,000 on other instance families. The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1.</p> <p>For example, a 100 GiB volume can be provisioned with up to 5,000 IOPS. On a supported instance type, any volume 1,280 GiB in size or greater allows provisioning up to the 64,000 IOPS maximum (50 &times; 1,280 GiB = 64,000).</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/io1_throughput.png" alt="" width="700" height="253" /></p> <p>&nbsp;</p> <p>An&nbsp;<code class="code">io1</code>&nbsp;volume provisioned with up to 32,000 IOPS supports a maximum I/O size of 256 KiB and yields as much as 500 MiB/s of throughput. With the I/O size at the maximum, peak throughput is reached at 2,000 IOPS. A volume provisioned with more than 32,000 IOPS (up to the cap of 64,000 IOPS) supports a maximum I/O size of 16 KiB and yields as much as 1,000 MiB/s of throughput.</p> <p>Therefore, for instance, a 10 GiB volume can be provisioned with up to <strong>500</strong> IOPS. Any volume 640 GiB in size or greater allows provisioning up to a maximum of 32,000 IOPS (50 &times; 640 GiB = 32,000).</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567162, '_class': 'assessment', 'updated': '2019-06-16T05:30:06Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are creating a Provisioned IOPS volume in AWS. The size of the volume is 10 GiB.Which of the following is the correct value that should be put for the IOPS of the volume?', 'id': 10337422, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudTrail', 'prompt': {'answers': ['DynamoDB', 'A RDS instance', 'Amazon Redshift', 'Amazon S3'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are managing an online platform which allows people to easily buy, sell, spend, and manage their cryptocurrency. To meet the strict IT audit requirements, each of the API calls on all of your AWS resources should be properly captured and recorded. You used CloudTrail in your VPC to help you in the compliance, operational auditing, and risk auditing of your AWS account.\xa0 \xa0</p><p>In this scenario, where does CloudTrail store all of the logs that it creates?\xa0 </p>', 'explanation': '<p>CloudTrail is enabled on your AWS account when you create it. When activity occurs in your AWS account, that activity is recorded in a CloudTrail event. You can easily view events in the CloudTrail console by going to&nbsp;<strong>Event history</strong>.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://media.amazonwebservices.com/blog/2014/cloudtrail_flow_9.png" width="750" /></p> <p>&nbsp;</p> <p>Event history allows you to view, search, and download the past 90 days of supported activity in your AWS account. In addition, you can create a CloudTrail trail to further archive, analyze, and respond to changes in your AWS resources. A trail is a configuration that enables delivery of events to an Amazon S3 bucket that you specify. You can also deliver and analyze events in a trail with Amazon CloudWatch Logs and Amazon CloudWatch Events. You can create a trail with the CloudTrail console, the AWS CLI, or the CloudTrail API.</p> <p><span style="font-weight: 400;">The rest of the answers are incorrect. Options 1 and 2 are for database, option 3 is used for data warehouse that scales horizontally and allows you to store terabytes and petabytes of data.</span></p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p> <p><a href="https://aws.amazon.com/cloudtrail/" target="_blank" rel="noopener">https://aws.amazon.com/cloudtrail/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudTrail Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567176, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are managing an online platform which allows people to easily buy, sell, spend, and manage their cryptocurrency. To meet the strict IT audit requirements, each of the API calls on all of your AWS resources should be properly captured and recorded. You used CloudTrail in your VPC to help you in the compliance, operational auditing, and risk auditing of your AWS account.\xa0 \xa0In this scenario, where does CloudTrail store all of the logs that it creates?', 'id': 10337436, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'answers': ['<p>2</p>', '<p>3</p>', '<p>4</p>', '<p>6</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>Your company has a two-tier environment in their on-premises data center which is composed of an application tier and database tier. You are instructed to migrate their environment to the AWS cloud, and to design the subnets in their VPC with the following requirements: </p><p>a) There is an application load balancer that would distribute the incoming traffic among the servers in the application tier. </p><p>b) The application tier and the database tier must not be accessible from the public Internet. The application tier should only accept traffic coming from the load balancer. </p><p>c) The database tier contains very sensitive data. It must not share the same subnet with other AWS resources and its custom route table with other instances in the environment. </p><p>d) The environment must be highly available and scalable to handle a surge of incoming traffic over the Internet. </p><p><br></p><p>How many subnets should you create to meet the above requirements?</p>', 'explanation': '<p>In the given scenario, it is evident that only the load balancer is accessible from the public. Therefore, a public subnet is required. Take note that if you have more than one private subnet in the same Availability Zone that contains instances that need to be registered with the load balancer, you only need to create one public subnet. You only need one public subnet per Availability Zone; you can add the private instances in all the private subnets that reside in that particular Availability Zone.</p> <p>Since the application tier and database tier should not be accessible from the Internet, they should both be in a private subnet of the VPC. The issue here is that the database servers should not be in the same subnet as the application servers. Therefore, they will each have their own private subnet. So in total that makes 3 subnets already.</p> <p>The catch though is found on the final requirement. The environment should be highly available and in the event&nbsp;that one Availability Zone goes down, there is another AZ available to handle the incoming traffic. Redundancy would solve the matter by deploying the environment in&nbsp;two Availability Zones. For the database tier, you can set up a master-slave replication between the two instances across the two AZs.</p> <p>Hence, the correct answer is 6 subnets.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html</a></p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/">https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567164, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'Your company has a two-tier environment in their on-premises data center which is composed of an application tier and database tier. You are instructed to migrate their environment to the AWS cloud, and to design the subnets in their VPC with the following requirements: a) There is an application load balancer that would distribute the incoming traffic among the servers in the application tier. b) The application tier and the database tier must not be accessible from the public Internet. The application tier should only accept traffic coming from the load balancer. c) The database tier contains very sensitive data. It must not share the same subnet with other AWS resources and its custom route table with other instances in the environment. d) The environment must be highly available and scalable to handle a surge of incoming traffic over the Internet. How many subnets should you create to meet the above requirements?', 'id': 10337424, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['Copy the S3 bucket to an EBS-backed EC2 instance.', 'Create a Lifecycle Policy to regularly backup the S3 bucket to Amazon Glacier.', 'Use AWS Storage Gateway to keep a backup of the data.', 'Do nothing since the S3 bucket can withstand an outage in one of the Availability Zones and even regional service failures.', 'Enable Cross-Region Replication.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': 'You are working for a major financial firm in Wall Street where you are tasked to design an application architecture for their online trading platform which should have high availability and fault tolerance. The application is using an Amazon S3 bucket located in the us-east-1 region to store large amounts of intraday financial data. <br><br>To avoid any costly service disruptions, what will you do to ensure that the stored financial data in the S3 bucket would not be affected even if there is an outage in one of the Availability Zones or a regional service failure in us-east-1?', 'explanation': '<p>In this scenario, you need to enable Cross-Region Replication to ensure that your S3 bucket would not be affected even if there is an outage in one of the Availability Zones or a regional service failure in us-east-1. When you upload your data in S3, your objects are redundantly stored on multiple devices across multiple facilities within the region only, where you created the bucket. Hence, if there is an outage on the entire region, your S3 bucket will be unavailable if you do not enable Cross-Region Replication, which should make your data available to another region.</p> <p>Note that an Availability Zone (AZ) is more related with Amazon EC2 instances rather than Amazon S3 so if there is any outage in the AZ, the S3 bucket is usually not affected but only the EC2 instances deployed on that zone.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/s3/faqs/">https://aws.amazon.com/s3/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['e'], 'original_assessment_id': 2567166, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are working for a major financial firm in Wall Street where you are tasked to design an application architecture for their online trading platform which should have high availability and fault tolerance. The application is using an Amazon S3 bucket located in the us-east-1 region to store large amounts of intraday financial data. To avoid any costly service disruptions, what will you do to ensure that the stored financial data in the S3 bucket would not be affected even if there is an outage in one of the Availability Zones or a regional service failure in us-east-1?', 'id': 10337426, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB Auto Scaling', 'prompt': {'answers': ['<p>Integrate an Application Load Balancer with your DynamoDB table.\xa0 </p>', '<p>Add the DynamoDB table to an Auto Scaling Group.\xa0 </p>', '<p>Use DynamoDB Auto Scaling\xa0 </p>', '<p>Create an SQS queue in front of the DynamoDB table.\xa0 </p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A popular augmented reality (AR) mobile game is heavily using a RESTful API which is hosted in AWS. The API uses Amazon API Gateway and a DynamoDB table with a preconfigured read and write capacity. Based on your systems monitoring, the DynamoDB table begins to throttle requests during high peak loads which causes the slow performance of the game.\xa0 </p><p>Which of the following can you do to improve the performance of your app?\xa0 </p>', 'explanation': '<p>DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don\'t pay for unused provisioned capacity.</p> <p>Option 3 is the best answer. DynamoDB Auto Scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf.</p> <p>Option 1 is incorrect because an Application Load Balancer is not suitable to be used with DynamoDB and in addition, this will not increase the throughput of your DynamoDB table.</p> <p>Option 2 is incorrect because you usually put EC2 instances on an Auto Scaling Group, and not a DynamoDB table.</p> <p>Option 4 is incorrect because this is not a design principle for high throughput DynamoDB table. Using SQS is for handling queuing and polling the request. This will not increase the throughput of DynamoDB which is required in this situation.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567168, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'A popular augmented reality (AR) mobile game is heavily using a RESTful API which is hosted in AWS. The API uses Amazon API Gateway and a DynamoDB table with a preconfigured read and write capacity. Based on your systems monitoring, the DynamoDB table begins to throttle requests during high peak loads which causes the slow performance of the game.\xa0 Which of the following can you do to improve the performance of your app?', 'id': 10337428, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Redshift', 'prompt': {'answers': ['<p>Create a scheduled job that will automatically take the snapshot of your Redshift Cluster and store it to an S3 bucket. Restore the snapshot in case of an AWS region outage.</p>', '<p>Do nothing because Amazon Redshift is a highly available, fully-managed data warehouse which can withstand an outage of an entire AWS region.</p>', '<p>Use Automated snapshots of your Redshift Cluster.</p>', '<p>Enable Cross-Region Snapshots Copy in your Amazon Redshift Cluster.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A data analytics company, which uses machine learning to collect and analyze consumer data, is using Redshift cluster as their data warehouse. You are instructed to implement a disaster recovery plan for their systems to ensure business continuity even in the event of an AWS region outage.\xa0 \xa0</p><p>Which of the following is the best approach to meet this requirement?</p>', 'explanation': '<p>You can configure Amazon Redshift to copy snapshots for a cluster to another region. To configure cross-region snapshot copy, you need to enable this copy feature for each cluster and configure where to copy snapshots and how long to keep copied automated snapshots in the destination region. When cross-region copy is enabled for a cluster, all new manual and automatic snapshots are copied to the specified region.&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/redshift/latest/mgmt/images/rs-mgmt-restore-table-from-snapshot.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because&nbsp;although this option is possible, this entails a lot of manual work and hence, not the best option. You should configure cross-region snapshot copy instead.</p> <p>Option 2 is incorrect because although Amazon Redshift is a fully-managed data warehouse, you will still need to configure cross-region snapshot copy to ensure that your data is properly replicated to another region.</p> <p>Option&nbsp;3 is incorrect because using automated snapshots is not enough and will not be available in case the entire AWS region is down.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html">https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-redshift/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567206, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'A data analytics company, which uses machine learning to collect and analyze consumer data, is using Redshift cluster as their data warehouse. You are instructed to implement a disaster recovery plan for their systems to ensure business continuity even in the event of an AWS region outage.\xa0 \xa0Which of the following is the best approach to meet this requirement?', 'id': 10337464, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudWatch', 'prompt': {'answers': ['<p>Install the unified CloudWatch Logs agent in each instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>', '<p>Install AWS SDK in each instance and create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and use CloudWatch Logs Insights to analyze the log data of all instances.</p>', '<p>Install the AWS Systems Manager Agent (SSM Agent) in each instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>', '<p>Install AWS Inspector Agent in each instance which will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all instances.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>There is a new compliance rule in your company that audits every Windows and Linux EC2 instances each month to view any performance issues. They have more than a hundred EC2 instances running in production, and each must have a logging function that collects various system details regarding that instance. The SysOps team will periodically review these logs and analyze their contents using AWS Analytics tools, and the result will need to be retained in an S3 bucket. </p><p>In this scenario, what is the most efficient way to collect and analyze logs from the instances with minimal effort?</p>', 'explanation': '<p>To collect logs from your Amazon EC2 instances and on-premises servers into CloudWatch Logs, AWS offers both a new unified CloudWatch agent, and an older CloudWatch Logs agent. It is recommended to use the unified CloudWatch agent which has the following advantages:</p> <div class="itemizedlist"> <p style="padding-left: 30px;">&nbsp;- You can collect both logs and advanced metrics with the installation and configuration of just one agent.</p> <p style="padding-left: 30px;">&nbsp;- The unified agent enables the collection of logs from servers running Windows Server.</p> <p style="padding-left: 30px;">&nbsp;- If you are using the agent to collect CloudWatch metrics, the unified agent also enables the collection of additional system metrics, for in-guest visibility.</p> <p style="padding-left: 30px;">&nbsp;- The unified agent provides better performance.</p> <p style="padding-left: 30px;">&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/product-marketing/cloudwatch/Product%20Page%20Diagrams/LogsInsights-workflow.0d4b31f451e7cc4dc05e2b40240bcd0d178f1283.png" width="800" height="236" /></p> <p>&nbsp;</p> <p>CloudWatch Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you quickly and effectively respond to operational issues. If an issue occurs, you can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.</p> <p>CloudWatch Logs Insights includes a purpose-built query language with a few simple but powerful commands. CloudWatch Logs Insights provides sample queries, command descriptions, query autocompletion, and log field discovery to help you get started quickly. Sample queries are included for several types of AWS service logs.</p> <p>Option 2 is incorrect. Although this is a valid solution, this entails a lot of effort to implement as you have to allocate time to install the AWS SDK to each instance&nbsp;and develop a custom monitoring solution. Remember that the question is specifically looking for a solution that can be implemented with&nbsp;minimal effort. In addition, it is unnecessary and not cost-efficient to enable detailed monitoring in CloudWatch in order to meet the requirements of this scenario since this can be done using CloudWatch Logs.</p> </div> <p>Option 3 is incorrect as although this is also a valid solution, it is more efficient to use CloudWatch agent than an SSM agent. Manually connecting to an instance to view log files and troubleshoot an issue with SSM Agent is time-consuming hence, for more efficient instance monitoring, you can use the CloudWatch Agent instead to send the log data to Amazon CloudWatch Logs.</p> <p>Option 4 is incorrect because AWS Inspector is simply a security assessments&nbsp;service which only helps you in checking for unintended network accessibility of your EC2 instances and for vulnerabilities on those EC2 instances. Furthermore, setting up an Amazon CloudWatch dashboard is not suitable since its&nbsp;primarily used&nbsp;for scenarios where you have to monitor your resources in a single view, even those resources that are spread across different AWS Regions. It is better to use&nbsp;CloudWatch Logs Insights instead since it enables you to interactively search and analyze your log data.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html ">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p> <p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Additional learning material:</strong> CloudWatch Logs Insights Customer Use Case</p> <p><iframe src="https://www.youtube.com/embed/RnN1o4Zdego" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p> <p>&nbsp;</p> <p><strong>CloudWatch Agent vs &nbsp;SSM Agent vs Custom Daemon Scripts:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567110, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'There is a new compliance rule in your company that audits every Windows and Linux EC2 instances each month to view any performance issues. They have more than a hundred EC2 instances running in production, and each must have a logging function that collects various system details regarding that instance. The SysOps team will periodically review these logs and analyze their contents using AWS Analytics tools, and the result will need to be retained in an S3 bucket. In this scenario, what is the most efficient way to collect and analyze logs from the instances with minimal effort?', 'id': 10337372, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Highly Available Network Design', 'prompt': {'answers': ['Deploy an Auto Scaling group with 2 instances in each of 3 Availability Zones behind an Application Load Balancer.', 'Deploy an Auto Scaling group with 2 instances in each of 2 Availability Zones behind an Application Load Balancer.', 'Deploy  an Auto Scaling group with 4 instances in one Availability Zone behind an Application Load Balancer.', 'Deploy an Auto Scaling group with 1 instance in each of 4 Availability Zones behind an Application Load Balancer.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You are a Solutions Architect working for a large multinational investment bank. They have a web application that requires a minimum of 4 EC2 instances to run to ensure that it can cater to its users across the globe. You are instructed to ensure fault tolerance of this system. <br><br>Which of the following is the best option?', 'explanation': '<p>Fault Tolerance is the ability of a system to remain in operation even if some of the components used to build the system fail. In AWS, this means that in the event of server fault or system failures, the number of running EC2 instances should not fall below the minimum number of instances required by the system for it to work properly. So if the the application requires a minimum of 4 instances, there should be at least 4 instances running in case there is an outage in one of the Availability Zones or if there are server issues.&nbsp;</p> <p>One of the differences between Fault Tolerance&nbsp;and High Availability is that, the former refers to the minimum number of running instances. For example, you have a system that requires a minimum of 4 running instances and currently has 6 running instances deployed in two Availability Zones. There was a component failure in one of the Availability Zones which knocks out 3 instances. In this case, the system can still be regarded as Highly Available since there are still instances running that can accomodate the requests. However, it is not Fault Tolerant since the required minimum of four instances have not been met.</p> <p>As such, Option 1 is the correct answer because even if there was an outage in one of the Availability Zones, the system still satisfies the requirement of a minimum of 4 running instances.&nbsp;</p> <p>Option 2 is incorrect because if one Availability Zone went out, there will only be 2 running instances available out of the required 4 minimum instances. Although the Auto Scaling group can spin up another 2 instances, the fault tolerance of the web application has already been compromised.</p> <p>Option 3&nbsp;is incorrect because if the Availability Zone went out, there will be no running instance available to accommodate the request.</p> <p>Option 4&nbsp;is incorrect because if one Availability Zone went out, there will only be 3 instances available to accommodate the request.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf">https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf</a></p> <p><a href="https://media.amazonwebservices.com/architecturecenter/AWS_ac_ra_ftha_04.pdf">https://media.amazonwebservices.com/architecturecenter/AWS_ac_ra_ftha_04.pdf</a>&nbsp;</p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567112, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are a Solutions Architect working for a large multinational investment bank. They have a web application that requires a minimum of 4 EC2 instances to run to ensure that it can cater to its users across the globe. You are instructed to ensure fault tolerance of this system. Which of the following is the best option?', 'id': 10337374, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['Use a custom shell script that transfers data from the S3 bucket to Glacier', 'Use Lifecycle Policies', 'Use AWS SQS', 'Use AWS SWF'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are a Cloud Migration Engineer in a media company which uses EC2, ELB, and S3 for its video-sharing portal for filmmakers. They are using a standard S3 storage class to store all high-quality videos that are frequently accessed only during the first three months of posting. What should you do if the company needs to automatically transfer or archive media data from an S3 bucket to Glacier?</p>', 'explanation': '<p>You can create a lifecycle policy in S3 to automatically transfer your data to Glacier.</p> <p>Lifecycle configuration enables you to specify the lifecycle management of objects in a bucket. The configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects.</p> <p>These actions can be classified as follows:</p> <ul> <li><strong>Transition actions</strong>&nbsp;&ndash; In which you define when objects transition to another storage class. For example, you may choose to transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation, or archive objects to the GLACIER storage class one year after creation.</li> <li><strong>Expiration actions</strong>&nbsp;&ndash; In which you specify when the objects expire. Then Amazon S3 deletes the expired objects on your behalf.</li> </ul> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567192, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are a Cloud Migration Engineer in a media company which uses EC2, ELB, and S3 for its video-sharing portal for filmmakers. They are using a standard S3 storage class to store all high-quality videos that are frequently accessed only during the first three months of posting. What should you do if the company needs to automatically transfer or archive media data from an S3 bucket to Glacier?', 'id': 10337450, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Highly Available Network Design', 'prompt': {'answers': ['<p>eu-east-2a with two EC2 instances, eu-east-2b with four EC2 instances, and eu-east-2c with two EC2 instances</p>', 'eu-east-2a with two EC2 instances, eu-east-2b with two EC2 instances, and eu-east-2c with two EC2 instances', 'eu-east-2a with four EC2 instances, eu-east-2b with two EC2 instances, and eu-east-2c with two EC2 instances', 'eu-east-2a with six EC2 instances, eu-east-2b with six EC2 instances, and eu-east-2c with no EC2 instances', 'eu-east-2a with three EC2 instances, eu-east-2b with three EC2 instances, and eu-east-2c with three EC2 instances'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': 'You are working as a  Solutions Architect for a major supermarket store chain. They have an e-commerce application which is running in eu-east-2 region that strictly requires six EC2 instances running at all times. In that region, there are 3 Availability Zones (AZ) - eu-east-2a, eu-east-2b, and eu-east-2c that you can use. <br><br>Which of the following deployments provide 100% fault tolerance if any single AZ in the region becomes unavailable? (Choose 2)', 'explanation': '<p>Fault Tolerance is the ability of a system to remain in operation even if some of the components used to build the system fail. In AWS, this means that in the event of server fault or system failures, the number of running EC2 instance should not fall below the minimum number of instances required by the system for it to work properly. So if the the application requires a minimum of 4 instances, there should be at least 4 instances running in case there is an outage in one of the Availability Zones or server issues.&nbsp;</p> <p><img src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/tutorial_as_elb_architecture.png" alt="" width="471" height="399" /></p> <p>In this scenario, you have to simulate a situation where one Availability Zone became unavailable for each option and check whether it still has 6 running instances. Hence, the correct answers are Options 4 and 5 because even if there is an outage in one of the Availability Zones, there are still 6 running instances:</p> <ol> <li>eu-east-2a with six EC2 instances, eu-east-2b with six EC2 instances, and eu-east-2c with no EC2 instances</li> <li>eu-east-2a with three EC2 instances, eu-east-2b with three EC2 instances, and eu-east-2c with three EC2 instances</li> </ol> <p><strong>Reference:</strong></p> </div> <p><a href="https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf">https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf</a></p>'}, 'correct_response': ['d', 'e'], 'original_assessment_id': 2567114, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are working as a  Solutions Architect for a major supermarket store chain. They have an e-commerce application which is running in eu-east-2 region that strictly requires six EC2 instances running at all times. In that region, there are 3 Availability Zones (AZ) - eu-east-2a, eu-east-2b, and eu-east-2c that you can use. Which of the following deployments provide 100% fault tolerance if any single AZ in the region becomes unavailable? (Choose 2)', 'id': 10337376, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Kinesis', 'prompt': {'answers': ['<p>Create an Amazon Kinesis Data Stream to collect the messages.</p>', '<p>Set up a default Amazon SQS queue to handle the messages.</p>', '<p>Set up an Amazon SNS Topic to handle the messages.</p>', '<p>Create a pipeline using AWS Data Pipeline to handle the messages.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': "<p>You are working as a Solutions Architect for a startup in which you are tasked to develop a custom messaging service that will also be used to train their AI for an automatic response feature which they plan to implement in the future. Based on their research and tests, the service can receive up to thousands of messages a day, and all of these data are to be sent to Amazon EMR for further processing. It is crucial that none of the messages will be lost, no duplicates will be produced and that they are processed in EMR in the same order as their arrival. </p><p>Which of the following options should you implement to meet the startup's requirements?</p>", 'explanation': '<p>Two important requirements that the chosen AWS service should fulfill is that data should not go missing, is durable, and streams data in the sequence of arrival. Kinesis can do the job just fine because of its architecture. A Kinesis data stream is a set of shards that has a sequence of data records, and each data record has a sequence number that is assigned by Kinesis Data Streams. Kinesis can also easily handle the high volume of messages being sent to the service.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png" alt="" width="700" height="315" /><br /><br />Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p> <p>Option 2 is incorrect because although SQS is a valid messaging service, it is not suitable for scenarios where you need to process the data based on the order they were received. Take note that a default&nbsp;queue in SQS&nbsp;is just a standard queue and not a&nbsp;FIFO (First-In-First-Out) queue. In addition,&nbsp;SQS does not guarantee that no duplicates will be sent.</p> <p>Option 3 is incorrect because SNS is a pub-sub messaging service in AWS. SNS might not be capable of handling such a large volume of messages being received and sent at a time. It does not also guarantee that the data will be transmitted in the same order they were received.</p> <p>Option 4 is incorrect because Data pipeline is primarily used as a cloud-based data workflow service that helps you process and move data between different AWS services and on-premises data sources. It is not suitable for collecting data from distributed sources such as users, IoT devices, or clickstreams.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/streams/latest/dev/introduction.html ">https://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></p> <p>For additional information, read the&nbsp;<em><strong>When should I use Amazon Kinesis Data Streams, and when should I use Amazon SQS?</strong></em>&nbsp;section of the Kinesis Data Stream FAQ:</p> <p><a href="https://aws.amazon.com/kinesis/data-streams/faqs/">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567116, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': "You are working as a Solutions Architect for a startup in which you are tasked to develop a custom messaging service that will also be used to train their AI for an automatic response feature which they plan to implement in the future. Based on their research and tests, the service can receive up to thousands of messages a day, and all of these data are to be sent to Amazon EMR for further processing. It is crucial that none of the messages will be lost, no duplicates will be produced and that they are processed in EMR in the same order as their arrival. Which of the following options should you implement to meet the startup's requirements?", 'id': 10337378, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['Reduced Redundancy Storage (RRS)', 'Cross-Region Replication', 'Transfer Acceleration', 'Multipart Upload'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A Fortune 500 company which has numerous offices and customers around the globe has hired you as their Principal Architect. You have staff and customers that upload gigabytes to terabytes of data to a centralized S3 bucket from the regional data centers, across continents, all over the world on a regular basis. At the end of the financial year, there are thousands of data being uploaded to the central S3 bucket which is in ap-southeast-2 (Sydney) region and a lot of employees are starting to complain about the slow upload times. You were instructed by the CTO to resolve this issue as soon as possible to avoid any delays in processing their global end of financial year (EOFY) reports.    </p><p>Which feature in Amazon S3 enables fast, easy, and secure transfer of your files over long distances between your client and your Amazon S3 bucket?</p>', 'explanation': '<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfer of files over long distances between your client and your Amazon S3 bucket. Transfer Acceleration leverages Amazon CloudFront\'s globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/s3/faqs/">https://aws.amazon.com/s3/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567170, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'A Fortune 500 company which has numerous offices and customers around the globe has hired you as their Principal Architect. You have staff and customers that upload gigabytes to terabytes of data to a centralized S3 bucket from the regional data centers, across continents, all over the world on a regular basis. At the end of the financial year, there are thousands of data being uploaded to the central S3 bucket which is in ap-southeast-2 (Sydney) region and a lot of employees are starting to complain about the slow upload times. You were instructed by the CTO to resolve this issue as soon as possible to avoid any delays in processing their global end of financial year (EOFY) reports.    Which feature in Amazon S3 enables fast, easy, and secure transfer of your files over long distances between your client and your Amazon S3 bucket?', 'id': 10337430, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB', 'prompt': {'answers': ['<p>Use CloudWatch Alarms to trigger the Lambda function whenever a new entry is created in the DynamoDB table.</p>', '<p>Invoke the Lambda functions using SNS each time that the ECS Cluster successfully processed financial data.</p>', '<p>Enable DynamoDB Streams to capture table activity and automatically trigger the Lambda function.</p>', '<p>Use Systems Manager Automation to detect new entries in the DynamoDB table then automatically invoke the Lambda function for processing.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A leading IT consulting company has an application which processes a large stream of financial data by an Amazon ECS Cluster then stores the result to a DynamoDB table. You have to design a solution to detect new entries in the DynamoDB table then automatically trigger a Lambda function to run some tests to verify the processed data. </p><p>What solution can be easily implemented to alert the Lambda function of new entries while requiring minimal configuration change to your architecture?</p>', 'explanation': '<p>Amazon DynamoDB is integrated with AWS Lambda so that you can create&nbsp;<em>triggers</em>&mdash;pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.</p> <p>If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table\'s stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/StreamsAndTriggers.png" /></p> <p>You can create a Lambda function which can perform a specific action that you specify, such as sending a notification or initiating a workflow. For instance, you can set up a Lambda function to simply copy each stream record to persistent storage, such as EFS or S3, to create a permanent audit trail of write activity in your table.</p> <p>Suppose you have a mobile gaming app that writes to a&nbsp;<code class="code">TutorialsDojoCourses</code>&nbsp;table. Whenever the&nbsp;<code class="code">TopCourse</code>&nbsp;attribute of the&nbsp;<code class="code">TutorialsDojoScores</code>&nbsp;table is updated, a corresponding stream record is written to the table\'s stream. This event could then trigger a Lambda function that posts a congratulatory message on a social media network. (The function would simply ignore any stream records that are not updates to&nbsp;<code class="code">TutorialsDojoCourses</code> or that do not modify the&nbsp;<code class="code">TopCourse</code> attribute.)</p> <p>Hence, Option 3 is the correct answer because the requirement can be met with minimal configuration change using DynamoDB streams which can automatically trigger Lambda functions whenever there is a new entry.</p> <p>Option 1 is incorrect because CloudWatch Alarms only monitor service metrics, not changes in DynamoDB table data.</p> <p>Option 2 is incorrect because you don\'t need to create an SNS topic just to invoke Lambda functions. You can enable DynamoDB streams instead to meet the requirement with less configuration.</p> <p>Option 4 is incorrect because the Systems Manager Automation service is primarily used to simplify common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. It does not have the capability to detect new entries in a DynamoDB table.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html</a></p> <p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon DynamoDB cheat sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567172, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'A leading IT consulting company has an application which processes a large stream of financial data by an Amazon ECS Cluster then stores the result to a DynamoDB table. You have to design a solution to detect new entries in the DynamoDB table then automatically trigger a Lambda function to run some tests to verify the processed data. What solution can be easily implemented to alert the Lambda function of new entries while requiring minimal configuration change to your architecture?', 'id': 10337432, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SWF', 'prompt': {'answers': ['Alter the retention period in Amazon SQS.', 'Alter the visibility timeout of SQS.', 'Replace Amazon SQS and instead, use Amazon Simple Workflow service.', 'Change the message size in SQS.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You have a web-based order processing system which is currently using a queue in Amazon SQS. The support team noticed that there are a lot of cases where an order was processed twice. This issue has caused a lot of trouble in your processing and made your customers very unhappy. Your IT Manager has asked you to ensure that this issue does not happen again. <br><br>What can you do to prevent this from happening again in the future?', 'explanation': '<p>The main issue here is that the order management system produces duplicate orders at times. Since the company is using SQS, there is a possibility that a message can have a duplicate in case an EC2 instance failed to delete the already processed message.&nbsp;To prevent this issue from happening, you have to use Amazon Simple Workflow service instead of SQS.</p> <p>For standard queues, the visibility timeout isn\'t a guarantee against receiving a message twice. Hence, Option 2 is incorrect. To avoid duplicate SQS messages, it is better to design your applications to be&nbsp;<em>idempotent</em>&nbsp;(they should not be affected adversely when processing the same message more than once).</p> <p>Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud. If your app\'s steps take more than 500 milliseconds to complete, you need to track the state of processing, and you need to recover or retry if a task fails.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/swf/">https://aws.amazon.com/swf/</a></p> <p><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p> <p>&nbsp;&nbsp;&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p> <p>&nbsp;</p> <p><strong>Amazon Simple Workflow (SWF) vs AWS Step Functions vs Amazon SQS:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-swf-vs-aws-step-functions-vs-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-swf-vs-aws-step-functions-vs-amazon-sqs/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567174, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You have a web-based order processing system which is currently using a queue in Amazon SQS. The support team noticed that there are a lot of cases where an order was processed twice. This issue has caused a lot of trouble in your processing and made your customers very unhappy. Your IT Manager has asked you to ensure that this issue does not happen again. What can you do to prevent this from happening again in the future?', 'id': 10337434, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'answers': ['<p>CAA</p>', 'CNAME', 'TXT', 'MX'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Junior Solutions Architect where you are responsible in enhancing the availability and durability of the database instances in your VPC.\xa0 Your company has a Multi-AZ RDS instance in the ap-northeast-1 region. If a storage volume on the primary instance fails in a Multi-AZ deployment, Amazon RDS automatically initiates a failover to the up-to-date standby instance.\xa0 \xa0</p><p>In case of a failover, which record in Route 53 is changed?</p>', 'explanation': '<p>Failover is automatically handled by Amazon RDS so that you can resume database operations as quickly as possible without administrative intervention. When failing over, Amazon RDS simply flips the canonical name record (CNAME) in Route53 for your DB instance to point at the standby, which in turn is promoted to become the new primary.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/rds/faqs/">https://aws.amazon.com/rds/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567178, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are working as a Junior Solutions Architect where you are responsible in enhancing the availability and durability of the database instances in your VPC.\xa0 Your company has a Multi-AZ RDS instance in the ap-northeast-1 region. If a storage volume on the primary instance fails in a Multi-AZ deployment, Amazon RDS automatically initiates a failover to the up-to-date standby instance.\xa0 \xa0In case of a failover, which record in Route 53 is changed?', 'id': 10337438, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Highly Available Network Design', 'prompt': {'answers': ['Amazon DynamoDB', 'Amazon Elastic Compute Cloud (EC2)', 'Amazon Elastic Load Balancing', 'Amazon Simple Notification Service (SNS)', 'Amazon Simple Storage Service (S3)', '<p>Amazon Certificate Manager </p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', '', ''], 'question': '<p>You are working as an IT Consultant for a transportation agency of the government where you were hired to design and build their online portal. There would be thousands of contracts, permits, and other financial documents that would be submitted to and processed by the portal 24 hours a day, 7 days a week, which is why you have to ensure the reliability of your cloud architecture in case of any infrastructure issues. </p><p>Which AWS services should you use to build a fault-tolerant and highly available architecture? (Choose 2)</p>', 'explanation': '<p>EC2 instances placed in different Availability Zones are both logically and physically separated, and they provide an easy-to-use model for deploying your application across data centers for both high availability and reliability.&nbsp;</p> <p>Elastic Load Balancers (ELB) allow you to spread the load across multiple Availability Zones and Amazon EC2 Auto Scaling groups for redundancy and decoupling of services. It&nbsp;provides high availability such that if one of its Availability Zones failed, it can direct the request to another healthy Availability Zone to avoid any downtime. Hence, Options 2 and 3 are the correct answers.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/tutorial_as_elb_architecture.png" /></p> <p class="p1">&nbsp;</p> <p class="p1">The scenario says that you are building an online portal of a government agency. Although it is true that S3 and DynamoDB are both fault-tolerant and highly available, the requirement says that you have to select two services where you can build a highly available and fault-tolerant architecture.</p> <p class="p1">Since the government agency will be having an online portal that will accept and process various documents, then we could conclude that they need a server to host the API and the public facing website. S3 can be used as a data storage as well as a static website, but since the requirement says that it is for an&nbsp;online portal which should be <em><strong>dynamic</strong></em>, then using S3 might not be the best option since it needs to accept documents and generate output, which can\'t be handled by a static website hosted in S3.</p> <p class="p1">DynamoDB&nbsp;can certainly be used as well, but this service is only limited to provide a NoSQL&nbsp;database. Using a combination of S3 and DynamoDB&nbsp;may be valid but this has certain restrictions unlike EC2 + ELB where you can support more database types, dynamic apps and many others. And even without Auto Scaling, you can still attach multiple EC2 instances to your ELB&nbsp;to ensure high availability. Don\'t forget as well that EC2 Spot Fleet has an automatic scaling feature as well, which removes the need for an Auto Scaling / Launch configuration.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://d0.awsstatic.com/whitepapers/aws-web-hosting-best-practices.pdf">https://d0.awsstatic.com/whitepapers/aws-web-hosting-best-practices.pdf</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet-automatic-scaling.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet-automatic-scaling.html</a></p>'}, 'correct_response': ['b', 'c'], 'original_assessment_id': 2567182, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are working as an IT Consultant for a transportation agency of the government where you were hired to design and build their online portal. There would be thousands of contracts, permits, and other financial documents that would be submitted to and processed by the portal 24 hours a day, 7 days a week, which is why you have to ensure the reliability of your cloud architecture in case of any infrastructure issues. Which AWS services should you use to build a fault-tolerant and highly available architecture? (Choose 2)', 'id': 10337440, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Lambda', 'prompt': {'answers': ['<p>Use a CloudFront web distribution and Route 53 with a latency-based routing policy, in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket.</p>', '<p>Integrate CloudFront with Lambda@Edge in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Amazon Athena and durably store the results to an Amazon S3 bucket.</p>', '<p>Use a CloudFront web distribution and Route 53 with a Geoproximity routing policy in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket.</p>', '<p>Integrate CloudFront with Lambda@Edge in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a leading data analytics company in which you are tasked to process real-time streaming data of your users across the globe. This will enable you to track and analyze globally-distributed user activity on your website and mobile applications, including click stream analysis. Your cloud architecture should process the data in close geographical proximity to your users and to respond to user requests at low latencies. </p><p>Which of the following options is the most ideal solution that you should implement?</p>', 'explanation': '<p>Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency. With Lambda@Edge, you don\'t have to provision or manage infrastructure in multiple locations around the world. You pay only for the compute time you consume - there is no charge when your code is not running.</p> <p>With Lambda@Edge, you can enrich your web applications by making them globally distributed and improving their performance &mdash; all with zero server administration. Lambda@Edge runs your code in response to events generated by the Amazon CloudFront content delivery network (CDN). Just upload your code to AWS Lambda, which takes care of everything required to run and scale your code with high availability at an AWS location closest to your end user.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/products/cloudfront/AWS-Lambda-at-Edge_User-Tracking-Analytics-diagram%20Oct%202018.5cc920f99c9450467b7290c20a8a4eb7c444e915.png" width="700" height="340" /></p> <p>&nbsp;</p> <p>By using Lambda@Edge and Kinesis together, you can process real-time streaming data so that you can track and analyze globally-distributed user activity on your website and mobile applications, including clickstream analysis. Hence, Option 4 is the correct answer in this scenario.</p> <p>Options 1 and 3 are both incorrect because you can only route traffic using Route 53 since it does not have any computing capability. This solution would not be able to process and return the data in close geographical proximity to your users since it is not using Lambda@Edge.</p> <p>Option 2 is incorrect because although using Lambda@Edge&nbsp;is correct, Amazon Athena is just an interactive query service that enables you to easily analyze data in Amazon S3 using standard SQL. Kinesis should be used to process the streaming data in real-time.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p class="p1"><span class="s1"><a href="https://aws.amazon.com/lambda/edge/">https://aws.amazon.com/lambda/edge/</a></span></p> <p class="p1"><span class="s1"><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/global-data-ingestion-with-amazon-cloudfront-and-lambdaedge/">https://aws.amazon.com/blogs/networking-and-content-delivery/global-data-ingestion-with-amazon-cloudfront-and-lambdaedge/</a></span></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567184, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are working as a Solutions Architect for a leading data analytics company in which you are tasked to process real-time streaming data of your users across the globe. This will enable you to track and analyze globally-distributed user activity on your website and mobile applications, including click stream analysis. Your cloud architecture should process the data in close geographical proximity to your users and to respond to user requests at low latencies. Which of the following options is the most ideal solution that you should implement?', 'id': 10337442, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'answers': ['Enable Multi-AZ deployments', 'Enable Amazon RDS Standby Replicas', 'Enable Amazon RDS Read Replicas', 'Use SQS to queue up the requests'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are managing a global news website which is deployed to AWS and is using MySQL RDS. The website has millions of viewers from all over the world which means that the website has read-heavy database workloads. <br><br>In this scenario, which of the following is the best option to use to increase the read throughput on the MySQL database?</p>', 'explanation': '<div> <p>Amazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, and PostgreSQL as well as Amazon Aurora.</p> <p>Option 1 is incorrect because the Multi-AZ deployments feature is mainly used to achieve high availability and failover support for your database.</p> <p>Option 2 is incorrect because a Standby replica is used in&nbsp;Multi-AZ deployments and hence, it is not a solution to&nbsp;reduce read-heavy database workloads.</p> <p>Option 4 is incorrect because although an SQS queue can effectively manage the requests, it won\'t be able to entirely improve the read-throughput of the database by itself.</p> <p>&nbsp;</p> </div> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/rds/details/read-replicas/">https://aws.amazon.com/rds/details/read-replicas/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p> <p>&nbsp;</p> <p><strong>Here is a quick introduction to Amazon RDS:</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/eMzCI7S1P9M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'}, 'correct_response': ['c'], 'original_assessment_id': 2567186, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are managing a global news website which is deployed to AWS and is using MySQL RDS. The website has millions of viewers from all over the world which means that the website has read-heavy database workloads. In this scenario, which of the following is the best option to use to increase the read throughput on the MySQL database?', 'id': 10337444, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudWatch', 'prompt': {'answers': ['The amount of available random access memory.', 'The average number of disk I/O operations per second during the polling period.', 'The percentage of CPU utilization.', 'RDS child processes.', '<p>OS processes</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are a Big Data Engineer who is assigned to handle the online enrollment system database of a prestigious university, which is hosted in RDS. You are required to monitor the database metrics in Amazon CloudWatch to ensure the availability of the enrollment system.\xa0 \xa0</p><p>What are the enhanced monitoring metrics that Amazon CloudWatch provides for Amazon RDS DB instances? (Choose 2)</p>', 'explanation': '<p>Amazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB instance using the console, or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice.</p> <p>CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and Enhanced Monitoring gathers its metrics from an agent on the instance. As a result, you might find differences between the measurements, because the hypervisor layer performs a small amount of work. The differences can be greater if your DB instances use smaller instance classes, because then there are likely more virtual machines (VMs) that are managed by the hypervisor layer on a single physical instance. Enhanced Monitoring metrics are useful when you want to see how different processes or threads on a DB instance use the CPU.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/metrics1.png" /></p> <p>&nbsp;</p> <p>In RDS, the Enhanced Monitoring metrics shown in the Process List view are organized as follows:</p> <div class="itemizedlist"> <p style="padding-left: 30px;"><strong>-RDS child processes</strong>&nbsp;&ndash; Shows a summary of the RDS processes that support the DB instance, for example&nbsp;<code class="code">aurora</code>&nbsp;for Amazon Aurora DB clusters and&nbsp;<code class="code">mysqld</code>&nbsp;for MySQL DB instances. Process threads appear nested beneath the parent process. Process threads show CPU utilization only as other metrics are the same for all threads for the process. The console displays a maximum of 100 processes and threads. The results are a combination of the top CPU consuming and memory consuming processes and threads. If there are more than 50 processes and more than 50 threads, the console displays the top 50 consumers in each category. This display helps you identify which processes are having the greatest impact on performance.</p> <p style="padding-left: 30px;"><strong>-RDS processes</strong>&nbsp;&ndash; Shows a summary of the resources used by the RDS management agent, diagnostics monitoring processes, and other AWS processes that are required to support RDS DB instances.</p> <p style="padding-left: 30px;"><strong>-OS processes</strong>&nbsp;&ndash; Shows a summary of the kernel and system processes, which generally have minimal impact on performance.</p> </div> <p style="padding-left: 30px;">&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/rds-metricscollected.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/rds-metricscollected.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.CloudWatchLogs</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</a></span></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></strong></p>'}, 'correct_response': ['d', 'e'], 'original_assessment_id': 2567188, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are a Big Data Engineer who is assigned to handle the online enrollment system database of a prestigious university, which is hosted in RDS. You are required to monitor the database metrics in Amazon CloudWatch to ensure the availability of the enrollment system.\xa0 \xa0What are the enhanced monitoring metrics that Amazon CloudWatch provides for Amazon RDS DB instances? (Choose 2)', 'id': 10337446, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'VPC', 'prompt': {'answers': ['A bastion host is an EC2 instance in a private subnet of your VPC and is typically accessed using SSH or RDP. Once remote connectivity has been established with the bastion host, it then acts as a ‘jump’ server that allows you to use SSH or RDP to log into other EC2 instances deployed in public subnets.', 'A bastion host is an EC2 instance in a public subnet of your VPC and is typically accessed using SSH or RDP. Once remote connectivity has been established with the bastion host, it then acts as a ‘jump’ server, allowing you to use HTTPS to log into other EC2 instances deployed in private subnets.', 'A bastion host is an EC2 instance in a public subnet of your VPC and is typically accessed using SSH or RDP. Once remote connectivity has been established with a bastion host, it then acts as a ‘jump’ server, allowing you to use SSH or RDP to log into other EC2 instances deployed in private subnets.', "A bastion host is an EC2 instance in a private subnet of your VPC and is typically accessed using SSH or RDP. Once remote connectivity has been established with the bastion host, it then acts as a 'jump' server,  allowing you to use HTTPS to log into other EC2 instances deployed in public subnets."], 'question': 'You are a Solutions Architect working for a large multi-national bank in the Asia-Pacific region. You designed an application architecture that is deployed to AWS, which has four Reserved EC2 instances. To be able to securely and easily manage these instances, you created a bastion host in your VPC. When your CTO found out, he was concerned and asked you about what you have done.<br /><br />How will you describe what a bastion host is to your boss?', 'explanation': '<p>A bastion host is basically an EC2 instance in the public subnet of your VPC and is typically accessed using SSH or RDP,&nbsp;which are used as a jump server to other EC2 instances and other AWS resources within other subnets.&nbsp;A bastion is a special purpose server instance that is designed to be the primary access point from the Internet and acts as a proxy to your other EC2 instances which are preferably deployed in a private subnet.</p><p>&nbsp;</p><p>References:</p><p><a href="https://aws.amazon.com/blogs/security/controlling-network-access-to-ec2-instances-using-a-bastion-server/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/security/controlling-network-access-to-ec2-instances-using-a-bastion-server/</a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567190, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are a Solutions Architect working for a large multi-national bank in the Asia-Pacific region. You designed an application architecture that is deployed to AWS, which has four Reserved EC2 instances. To be able to securely and easily manage these instances, you created a bastion host in your VPC. When your CTO found out, he was concerned and asked you about what you have done.How will you describe what a bastion host is to your boss?', 'id': 10337448, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['Edge locations are used as central control stations for your AWS resources deployed on all regions.', 'An edge location is used as a link when building load balancing between regions', '<p>Availability Zones are collections of data centers that run on physically distinct, independent infrastructure within an AWS region while an Edge location is used to deliver cached content to the closest location to reduce latency.</p>', 'An availability zone is a grouping of AWS resources in a specific region while an edge location is a specific resource within the AWS region'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>Your company recently decided to adopt a hybrid cloud infrastructure with AWS to take advantage of its global infrastructure of Availability Zones, Regions, and Edge Locations. What is the difference between an Availability Zone and an Edge location?</p>', 'explanation': '<p>Availability Zones are collections of data centers that run on physically distinct, independent infrastructure. Availability Zones are engineered to be highly reliable. Common points of failure such as generators and cooling equipment are not shared between Availability Zones. Availability Zones are also physically separate so that even an extreme disaster such as a fire, tornado, or flood will affect only the single Availability Zone where it occurred.</p> <p>Amazon EC2 is hosted in multiple locations worldwide. These locations are composed of regions and Availability Zones. Each&nbsp;<em>region</em>&nbsp;is a separate geographic area. Each region has multiple, isolated locations known as&nbsp;<strong>Availability Zones</strong>.</p> <p>Edge location is used to deliver cached content to the closest location to reduce latency. It delivers your content through a worldwide network of data centers called edge locations.&nbsp;These are primarily used by the Amazon CloudFront service.&nbsp;</p> <p>&nbsp;<img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/aws_regions.png" /></p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://lightsail.aws.amazon.com/ls/docs/en/articles/understanding-regions-and-availability-zones-in-amazon-lightsail">https://lightsail.aws.amazon.com/ls/docs/en/articles/understanding-regions-and-availability-zones-in-amazon-lightsail</a></p> <p><a href="https://aws.amazon.com/blogs/apn/amazon-vpc-for-on-premises-network-engineers-part-one/">https://aws.amazon.com/blogs/apn/amazon-vpc-for-on-premises-network-engineers-part-one/</a></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567194, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'Your company recently decided to adopt a hybrid cloud infrastructure with AWS to take advantage of its global infrastructure of Availability Zones, Regions, and Edge Locations. What is the difference between an Availability Zone and an Edge location?', 'id': 10337452, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFront', 'prompt': {'answers': ['<p>AWS Certificate Manager</p>', '<p>IAM certificate store</p>', '<p>A private S3 bucket with versioning enabled</p>', '<p>An S3 bucket configured with server-side encryption with customer-provided encryption keys (SSE-C) </p>', '<p>CloudFront</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>A web application is hosted in an Auto Scaling group of EC2 instances deployed across multiple Availability Zones in front of an Application Load Balancer. You need to implement an SSL solution for your system to improve its security which is why you requested an SSL/TLS certificate from a third-party certificate authority (CA).\xa0 \xa0</p><p>Where can you safely import the SSL/TLS certificate of your application? (Choose 2)</p>', 'explanation': '<p>If you got your certificate from a third-party CA, import the certificate into ACM or upload it to the IAM certificate store. Hence, Options 1 and 2 are the correct answers.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d2908q01vomqb2.cloudfront.net/22d200f8670dbdb3e253a90eee5098477c95c23d/2016/10/11/image1_numbereddiagram_b.png" alt="" width="750" height="396" /></p> <p>&nbsp;</p> <p>ACM lets you import third-party certificates from the ACM console, as well as programmatically. If ACM is not available in your region, use AWS CLI to upload your third-party certificate to the IAM certificate store.</p> <p>Options 3 and 4 are incorrect as S3 is not a suitable service to store the SSL certificate.</p> <p>Option 5 is incorrect because although you can upload certificates to CloudFront, it doesn\'t mean that you can import SSL certificates on it. You would not be able to export the certificate that you have loaded in CloudFront nor assign them to your EC2 or ELB instances as it would be tied to a single CloudFront distribution.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-procedures.html#cnames-and-https-uploading-certificates">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-procedures.html#cnames-and-https-uploading-certificates</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</span></a></p>'}, 'correct_response': ['a', 'b'], 'original_assessment_id': 2567196, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'A web application is hosted in an Auto Scaling group of EC2 instances deployed across multiple Availability Zones in front of an Application Load Balancer. You need to implement an SSL solution for your system to improve its security which is why you requested an SSL/TLS certificate from a third-party certificate authority (CA).\xa0 \xa0Where can you safely import the SSL/TLS certificate of your application? (Choose 2)', 'id': 10337454, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EC2', 'prompt': {'answers': ['<p>3 instances in eu-west-1a, 3 instances in eu-west-1b, and 3 instances in eu-west-1c</p>', '<p>6 instances in eu-west-1a, 6 instances in eu-west-1b, and 6 instances in eu-west-1c</p>', '<p>2 instances in eu-west-1a, 2 instances in eu-west-1b, and 2 instances in eu-west-1c</p>', '<p>6 instances in eu-west-1a, 6 instances in eu-west-1b, and no instances in eu-west-1c</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A web application requires a minimum of six Amazon Elastic Compute Cloud (EC2) instances running at all times. You are tasked to deploy the application to three availability zones in the EU Ireland region (eu-west-1a, eu-west-1b, and eu-west-1c). </p><p>Which of the following setup is the most cost-effective solution which also maintains the fault-tolerance of your system?</p>', 'explanation': '<p>Basically, fault-tolerance is the ability of a system to remain in operation even in the event that some of its components fail, without any service degradation. In AWS, it can also refer to the minimum number of running EC2 instances or resources which should be running at all times in order for the system to properly operate and serve its consumers. Take note that this is quite different from the concept of High Availability, which is just concerned with having at least one running instance or resource in case of failure.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/documentdb/latest/developerguide/images/RegionsAndAZs.png" /></p> <p>&nbsp;</p> <p><span style="font-weight: 400;">In this scenario, Option 1 is the correct answer because even if there was an outage in one of the Availability Zones, the system still satisfies the requirement of having a minimum of 6 running instances. It is also the most cost-effective solution among other options.</span></p> <p><span style="font-weight: 400;">Option 2 is incorrect because although&nbsp;this solution provides the maximum fault-tolerance for the system, it entails a significant cost to maintain a total of 18 instances across 3 AZs.&nbsp;</span></p> <p>Option 3 is incorrect because if one Availability Zone goes down, there will only be 4 running instances available. Although this is the most cost-effective solution, it does not provide fault-tolerance.</p> <p><span style="font-weight: 400;">Option 4 is incorrect because although it provides fault-tolerance, it is not the most cost-effective solution as compared with Options 1 and 2. This solution has 12 running instances, unlike Option 1 which only has 9 instances.</span></p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html</a></p> <p><a href="https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf">https://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf</a></p> <p>&nbsp;</p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567198, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'A web application requires a minimum of six Amazon Elastic Compute Cloud (EC2) instances running at all times. You are tasked to deploy the application to three availability zones in the EU Ireland region (eu-west-1a, eu-west-1b, and eu-west-1c). Which of the following setup is the most cost-effective solution which also maintains the fault-tolerance of your system?', 'id': 10337456, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFront', 'prompt': {'answers': ['Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI.', 'Create an Identity and Access Management (IAM) user for CloudFront and grant access to the objects in your S3 bucket to that IAM user.', '<p>Create an S3 bucket policy that lists the CloudFront distribution ID as the principal and the target bucket as the Amazon Resource Name (ARN).</p>', 'Add the CloudFront account security group.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You are working for a large global media company with multiple office locations all around the world. You are instructed to build a system to distribute training videos to all employees. Using CloudFront, what method would be used to serve content that is stored in S3, but not publicly accessible from S3 directly?', 'explanation': '<p>When you create or update a distribution in CloudFront, you can add an origin access identity (OAI) and automatically update the bucket policy to give the origin access identity&nbsp;permission to access your bucket. Alternatively, you can choose to manually change the bucket policy or change ACLs, which control permissions on individual objects in your bucket.</p> <p>You can update the Amazon S3 bucket policy using either the AWS Management Console or the Amazon S3 API:</p> <ul> <li>Grant the CloudFront origin access identity the applicable permissions on the bucket.</li> <li>Deny access to anyone that you don\'t want to have access using Amazon S3 URLs.</li> </ul> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html#private-content-granting-permissions-to-oai">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html#private-content-granting-permissions-to-oai</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</span></a></p> <p>&nbsp;</p> <p><strong>S3 Pre-signed URLs vs CloudFront Signed URLs vs Origin Access Identity (OAI)</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567200, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'You are working for a large global media company with multiple office locations all around the world. You are instructed to build a system to distribute training videos to all employees. Using CloudFront, what method would be used to serve content that is stored in S3, but not publicly accessible from S3 directly?', 'id': 10337458, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'AWS Shield', 'prompt': {'answers': ['<p>AWS WAF\xa0 </p>', '<p>AWS Shield\xa0 </p>', '<p>AWS Firewall Manager\xa0 </p>', '<p>Amazon GuardDuty\xa0 </p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a leading financial firm where you are responsible in ensuring that their applications are highly available and safe from common web security vulnerabilities. Which is the most suitable AWS service to use to mitigate Distributed Denial of Service (DDoS) attacks from hitting your back-end EC2 instances?\xa0 \xa0</p>', 'explanation': '<p>AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield - Standard and Advanced.</p> <p>All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your web site or applications. When you use AWS Shield Standard with&nbsp;Amazon CloudFront<a href="https://aws.amazon.com/cloudfront/" target="_blank" rel="noopener">&nbsp;</a>and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/aws-answers/answers-images/non-lb-app-ddos-mitigation.379c24da557a03e2952636c214fd32be3e0d7265.png" width="750" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because&nbsp;AWS WAF&nbsp;is a web application firewall service that helps protect your web apps from common exploits that could affect app availability, compromise security, or consume excessive resources. Although this can help you against DDoS attacks, AWS WAF alone is not enough to fully protect your VPC. You still need to use AWS Shield in this scenario.</p> <p>Option 3 is incorrect because&nbsp;AWS Firewall Manager just simplifies your AWS WAF administration and maintenance tasks across multiple accounts and resources.&nbsp;</p> <p>Option 4 is incorrect because&nbsp;Amazon GuardDuty&nbsp;is an intelligent threat detection service to protect your AWS accounts and workloads. Using this alone will not fully protect your AWS resources against DDoS attacks.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/waf/latest/developerguide/waf-which-to-choose.html">https://docs.aws.amazon.com/waf/latest/developerguide/waf-which-to-choose.html</a></p> <p><a href="https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/">https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this&nbsp;AWS Shield Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-shield/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-shield/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567218, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You are working as a Solutions Architect for a leading financial firm where you are responsible in ensuring that their applications are highly available and safe from common web security vulnerabilities. Which is the most suitable AWS service to use to mitigate Distributed Denial of Service (DDoS) attacks from hitting your back-end EC2 instances?', 'id': 10337476, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['tutorialsdojo.s3-website-ap-southeast-2.amazonaws.com', 'ap-southeast-2.s3-website-tutorialsdojo.amazonaws.com', 'tutorialsdojo.s3-website-ap-southeast-2.amazon.aws.com', 'ap-southeast-2.s3-website-tutorialsdojo.amazon.aws.com'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'Your company wants to host a static website on Amazon S3 using a bucket named "tutorialsdojo" in the Asia Pacific (Sydney) region. What website URL will be assigned to the S3 bucket?', 'explanation': '<p>To host a static website, you configure an Amazon S3 bucket for website hosting, and then upload your website content to the bucket. The website is then available at the AWS Region-specific website endpoint of the bucket, which is in one of the following formats:</p> <p><em><strong>&lt;bucket-name&gt;</strong></em>.s3-website-<em><strong>&lt;AWS-region&gt;</strong></em>.amazonaws.com</p> <p>Hence, the correct answer is option A:</p> <p><a href="http://tutorialsdojo.s3-website-ap-southeast-2.amazonaws.com" target="_blank" rel="noopener">tutorialsdojo.s3-website-ap-southeast-2.amazonaws.com</a></p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567202, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'Your company wants to host a static website on Amazon S3 using a bucket named "tutorialsdojo" in the Asia Pacific (Sydney) region. What website URL will be assigned to the S3 bucket?', 'id': 10337460, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'AWS X-Ray', 'prompt': {'answers': ['<p>VPC Flow Logs</p>', '<p>CloudWatch</p>', '<p>CloudTrail</p>', '<p>AWS X-Ray</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>An application is using a RESTful API hosted in AWS which uses Amazon API Gateway and AWS Lambda. There is a requirement to trace and analyze user requests as they travel through your Amazon API Gateway APIs to the underlying services.\xa0 </p><p>Which of the following is the most suitable service to use to meet this requirement?</p>', 'explanation': '<p>You can use&nbsp;<a href="https://docs.aws.amazon.com/xray/latest/devguide/xray-services-apigateway.html" target="_blank" rel="noopener">AWS X-Ray</a>&nbsp;to trace and analyze user requests as they travel through your Amazon API Gateway APIs to the underlying services. API Gateway supports AWS X-Ray tracing for all API Gateway endpoint types: regional, edge-optimized, and private. You can use AWS X-Ray with Amazon API Gateway in all regions where X-Ray is available.</p> <p>X-Ray gives you an end-to-end view of an entire request, so you can analyze latencies in your APIs and their backend services. You can use an X-Ray service map to view the latency of an entire request and that of the downstream services that are integrated with X-Ray. And you can configure sampling rules to tell X-Ray which requests to record, at what sampling rates, according to criteria that you specify. If you call an API Gateway API from a service that\'s already being traced, API Gateway passes the trace through, even if X-Ray tracing is not enabled on the API.</p> <p>You can enable X-Ray for an API stage by using the API Gateway management console, or by using the API Gateway API or CLI.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/apigateway/latest/developerguide/images/apigateway-xray-traceview-1.png" alt="" width="750" height="326" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because VPC Flow Logs&nbsp;is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your entire VPC. Although it can capture some details about the incoming user requests, it is still better to use AWS X-Ray as it provides a better way to debug and analyze your microservices applications with request tracing so you can find the root cause of your issues and performance.</p> <p>Option 2 is incorrect because CloudWatch is a monitoring and management service. It does not have the capability&nbsp;to trace and analyze user requests as they travel through your Amazon API Gateway APIs.</p> <p>Option 3 is incorrect because CloudTrail is primarily used for&nbsp;API logging of all of your AWS resources.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-xray.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-xray.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS X-Ray Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-x-ray/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-x-ray/</span></a></p> <p>&nbsp;</p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567204, '_class': 'assessment', 'updated': '2019-06-16T05:30:04Z', 'created': '2019-06-16T05:30:04Z', 'question_plain': 'An application is using a RESTful API hosted in AWS which uses Amazon API Gateway and AWS Lambda. There is a requirement to trace and analyze user requests as they travel through your Amazon API Gateway APIs to the underlying services.\xa0 Which of the following is the most suitable service to use to meet this requirement?', 'id': 10337462, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'AMI', 'prompt': {'answers': ['Data is automatically saved as an EBS snapshot.', 'Data is automatically saved as an EBS volume.', 'Data is unavailable until the instance is restarted.', 'Data is automatically deleted.'], 'question': 'You are using an On-Demand EC2 instance to host a legacy web application that uses an Amazon Instance Store-Backed AMI. The web application should be decommissioned as soon as possible and hence, you need to terminate the EC2 instance. <br /><br />When the instance is terminated, what happens to the data on the root volume?', 'explanation': '<p>AMIs are categorized as either&nbsp;<em>backed by Amazon EBS</em>&nbsp;or&nbsp;<em>backed by instance store</em>. The former means that the root device for an instance launched from the AMI is an Amazon EBS volume created from an Amazon EBS snapshot. The latter means that the root device for an instance launched from the AMI is an instance store volume created from a template stored in Amazon S3.</p><p>Option 4 is the correct answer because the data on instance store volumes persist only during the life of the instance which means that if the instance is terminated, the data will be automatically deleted.</p><p>&nbsp;</p><p>References:&nbsp;</p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html</a></p><p>&nbsp;</p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567208, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You are using an On-Demand EC2 instance to host a legacy web application that uses an Amazon Instance Store-Backed AMI. The web application should be decommissioned as soon as possible and hence, you need to terminate the EC2 instance. When the instance is terminated, what happens to the data on the root volume?', 'id': 10337466, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['Amazon S3', 'Amazon Glacier', 'Amazon Import/Export', 'Amazon EC2', 'DynamoDB'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working for a large bank that is developing a web application that receives large amounts of object data. They are using the data to generate a report for their stockbrokers to use on a daily basis. Unfortunately, a recent financial crisis has left the bank short on cash and cannot afford to purchase expensive storage hardware. They had resorted to use AWS instead. <br><br>Which is the best service to use in order to store a virtually unlimited amount of object data without any effort to scale when demand unexpectedly increases?</p>', 'explanation': '<p>In this scenario, you can use Amazon S3 and Amazon Glacier as a storage service. And since we are looking for the best option, we have to consider that the object data being stored by the bank is used on a daily basis as well. Hence, Amazon S3 is the better choice as it provides&nbsp;frequent access to your object data.&nbsp;</p> <p>Amazon S3 is a durable, secure, simple, and fast storage service designed to make web-scale computing easier for developers. Use Amazon S3 if you need low latency or frequent access to your data. Use Amazon Glacier if low storage cost is paramount, and you do not require millisecond access to your data.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567210, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You are working for a large bank that is developing a web application that receives large amounts of object data. They are using the data to generate a report for their stockbrokers to use on a daily basis. Unfortunately, a recent financial crisis has left the bank short on cash and cannot afford to purchase expensive storage hardware. They had resorted to use AWS instead. Which is the best service to use in order to store a virtually unlimited amount of object data without any effort to scale when demand unexpectedly increases?', 'id': 10337468, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Athena', 'prompt': {'answers': ['<p>Create a migration tool to load the CSV export file from S3 to a DynamoDB instance. Once the data has been loaded, run queries using DynamoDB.</p>', '<p>Use mysqldump client utility to load the CSV export file from S3 to a MySQL RDS instance. Run some SQL queries once the data has been loaded to complete your validation.</p>', '<p>To be able to run SQL queries, use AWS Athena to analyze the export data file in S3.</p>', '<p>Use a migration tool to load the CSV export file from S3 to a database which is designed for online analytic processing (OLAP) such as AWS RedShift. Run some queries once the data has been loaded to complete your validation.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>To save cost, a company decided to change their third-party data analytics tool to a cheaper solution. They sent a full data export on a CSV file which contains all of their analytics information. You then save the CSV file to an S3 bucket for storage. Your manager asked you to do some validation on the provided data export.\xa0 </p><p>In this scenario, what is the most cost-effective and easiest way to analyze export data using a standard SQL?</p>', 'explanation': '<p>Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.</p> <p>Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run. Athena scales automatically&mdash;executing queries in parallel&mdash;so results are fast, even with large datasets and complex queries.</p> <p>Athena helps you analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. You can use Athena to run ad-hoc queries using ANSI SQL, without the need to aggregate or load the data into Athena.</p> <p>Hence, the most cost-effective and appropriate answer in this scenario is Option 3: Using AWS Athena.&nbsp;</p> <p>Options 1, 2 and 4 are all incorrect because it is not necessary to set up a database to be able to analyze the CSV export file.&nbsp;You can use a cost-effective option (AWS Athena), which is a serverless service&nbsp;that enables you to pay only for the queries you run.&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon Athena Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-athena/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-athena/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567212, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'To save cost, a company decided to change their third-party data analytics tool to a cheaper solution. They sent a full data export on a CSV file which contains all of their analytics information. You then save the CSV file to an S3 bucket for storage. Your manager asked you to do some validation on the provided data export.\xa0 In this scenario, what is the most cost-effective and easiest way to analyze export data using a standard SQL?', 'id': 10337470, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['The instance will be terminated', 'The instance will be stopped', 'The instance will be restarted', 'This is not possible as only On-Demand instances can be interrupted by Amazon EC2'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You have a prototype web application that uses one Spot EC2 instance. What will happen to the instance by default if it gets interrupted by Amazon EC2 for capacity requirements?', 'explanation': '<div> <p>The main differences are that:</p> <ol> <li>Spot instances typically offer a significant discount off the On-Demand prices</li> <li>Your instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</li> <li>Spot prices adjust gradually based on long term supply and demand for spare EC2 capacity.</li> </ol> <p>You can choose to have your Spot instances terminated, stopped, or hibernated upon interruption. Stop and hibernate options are available for persistent Spot requests and Spot Fleets with the <strong>maintain</strong>&nbsp;option enabled. By default, your instances are terminated hence, option 1 is the correct answer.</p> <p>&nbsp;</p> </div> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/ec2/faqs/">https://aws.amazon.com/ec2/faqs/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p> <p>&nbsp;</p> <p><strong>Here is an in-depth look at Spot Instances:</strong></p> <p><iframe src="https://www.youtube.com/embed/PKvss-RgSjI" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567214, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You have a prototype web application that uses one Spot EC2 instance. What will happen to the instance by default if it gets interrupted by Amazon EC2 for capacity requirements?', 'id': 10337472, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3 Select', 'prompt': {'answers': ['<p>RDS</p>', '<p>Redshift Spectrum</p>', '<p>S3 Select</p>', '<p>AWS Step Functions</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are planning to reduce the amount of data that Amazon S3 transfers to your servers in order to lower your operating costs as well as to lower the latency of retrieving the data. To accomplish this, you need to use simple structured query language (SQL) statements to filter the contents of Amazon S3 objects and retrieve just the subset of data that you need.\xa0 \xa0</p><p>Which of the following services will help you accomplish this requirement?</p>', 'explanation': '<p>With Amazon S3 Select, you can use simple structured query language (SQL) statements to filter the contents of Amazon S3 objects and retrieve just the subset of data that you need. By using Amazon S3 Select to filter this data, you can reduce the amount of data that Amazon S3 transfers, which reduces the cost and latency to retrieve this data.</p> <p>Amazon S3 Select works on objects stored in CSV, JSON, or Apache Parquet format. It also works with objects that are compressed with GZIP or BZIP2 (for CSV and JSON objects only), and server-side encrypted objects. You can specify the format of the results as either CSV or JSON, and you can determine how the records in the result are delimited.</p> <p>Option 1 is incorrect because although RDS is an SQL database where you can perform SQL operations, it is still not valid because you want to apply SQL transactions on S3 itself, and not on the database, which RDS cannot do.</p> <p>Option 2 is incorrect because although Amazon Redshift Spectrum provides a similar in-query functionality like S3 Select, this service is more suitable for querying your Redshift clusters and not your S3 buckets. The Redshift queries are run on your cluster resources against local disk. Redshift Spectrum queries run using per-query scale-out resources against data in S3 which can entail additional costs compared with S3 Select.</p> <p>Option 4 is incorrect because Step functions only&nbsp;let you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html</a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567216, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You are planning to reduce the amount of data that Amazon S3 transfers to your servers in order to lower your operating costs as well as to lower the latency of retrieving the data. To accomplish this, you need to use simple structured query language (SQL) statements to filter the contents of Amazon S3 objects and retrieve just the subset of data that you need.\xa0 \xa0Which of the following services will help you accomplish this requirement?', 'id': 10337474, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SWF', 'prompt': {'answers': ['It is because SWF is waiting human input from an activity task.', 'The workflow has exceeded SWF’s 15-day maximum workflow execution time.', 'The workflow has exceeded SWF’s 14-day maximum workflow execution time.', 'SWF should be restarted.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'A tech company is currently using Amazon Simple Workflow (SWF) service with a default configuration for their order processing system. The system works fine but you noticed that some of the orders seem to be stuck for almost 4 weeks. <br><br>What could be the possible reason for this?', 'explanation': '<p>By default, each workflow execution can run for a maximum of 1 year in Amazon SWF. This means that it is possible that in your workflow, there are some tasks which require manual action that renders it idle. As a result, some orders get stuck for almost 4 weeks.</p> <p>Amazon SWF does not take any special action if a workflow execution is idle for an extended period of time. Idle executions are subject to the timeouts that you configure. For example, if you have set the maximum duration for an execution to be 1 day, then an idle execution will be timed out if it exceeds the 1 day limit. Idle executions are also subject to the Amazon SWF limit on how long an execution can run (1 year).</p> <p>Options 2 and 3 are incorrect as the maximum execution time is 1 year.</p> <p>Option 4 is incorrect as there is no problem with SWF and you can\'t manually restart this service.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/swf/">https://aws.amazon.com/swf/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SWF Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-amazon-swf/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-simple-workflow-amazon-swf/</a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567220, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'A tech company is currently using Amazon Simple Workflow (SWF) service with a default configuration for their order processing system. The system works fine but you noticed that some of the orders seem to be stuck for almost 4 weeks. What could be the possible reason for this?', 'id': 10337478, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['All EBS Volume checks have failed.', 'The EBS Volume has been abruptly stopped.', 'All EBS Volume checks have been completed.', 'The check on the EBS volume is still in progress.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You had recently set up a CloudWatch Alarm that performs status checks on your EBS volume. However, you noticed that the volume check has a status of <code>insufficient-data</code>. What does this status mean?</p>', 'explanation': '<p>Volume status checks are automated tests that run every 5 minutes and return a pass or fail status.</p> <p>If all checks pass, the status of the volume is&nbsp;<strong><code>ok</code></strong>. Option 3 is, therefore, incorrect.</p> <p>If a check fails, the status of the volume is&nbsp;<strong><code>impaired</code></strong>. Option 1 is, therefore, incorrect.</p> <p>If the status is&nbsp;<strong><code>insufficient-data</code></strong>, the checks may still be in progress on the volume. Option 4 is, therefore, correct.</p> <p>There is no status code for option 2, which is also an incorrect choice.</p> <p>You can view the results of volume status checks to identify any impaired volumes and take any necessary actions.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></strong></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567222, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You had recently set up a CloudWatch Alarm that performs status checks on your EBS volume. However, you noticed that the volume check has a status of insufficient-data. What does this status mean?', 'id': 10337480, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Kinesis', 'prompt': {'answers': ['Kinesis', '<p>Redshift Spectrum</p>', '<p>AWS Glue</p>', '<p>Amazon EMR</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A data analytics application requires a service that can collect, process, and analyze clickstream data from various websites in real-time. Which of the following is the most suitable service to use for the application?</p>', 'explanation': '<p>Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and responds instantly instead of having to wait until all your data is collected before the processing can begin.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis_Evolve-from-batch-to-real-time-Analytics.d7ed76be304a30be5720fd159469f157e7c09ede.png" width="750" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect because Redshift Spectrum is primarily used to directly query open data formats stored in Amazon S3 without the need for unnecessary data movement, which enables you to analyze data across your data warehouse and data lake, together, with a single service. It does not provide the ability to process your data in real-time, unlike Kinesis.</p> <p>Option 3 is incorrect because AWS Glue&nbsp;is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.&nbsp;It does not provide the ability to process your data in real-time, unlike Kinesis.</p> <p>Option 4 is incorrect because Amazon EMR is a web service that uses an open-<wbr />source&nbsp;Hadoop framework, to quickly &amp; cost-effectively process vast amounts of data.&nbsp;It does not provide the ability to process your data in real-time, unlike Kinesis.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/kinesis/">https://aws.amazon.com/kinesis/</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon Kinesis Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567224, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'A data analytics application requires a service that can collect, process, and analyze clickstream data from various websites in real-time. Which of the following is the most suitable service to use for the application?', 'id': 10337482, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['In S3, set the permissions of the object to public read during upload.', '<p>Configure the ACL of the S3 bucket to set all objects to be publicly readable and writeable.</p>', 'Configure the S3 bucket policy to set all objects to public read.', 'Create an IAM role to set the objects inside the S3 bucket to public read.', 'Do nothing. Amazon S3 objects are already public by default.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', '', ''], 'question': 'You are working for a media company and you need to configure an Amazon S3 bucket to serve static assets for your public-facing web application. Which methods ensure that all of the objects uploaded to the S3 bucket can be read publicly all over the Internet? (Choose 2)', 'explanation': '<p>By default, all Amazon S3 resources such as buckets, objects, and related subresources&nbsp;are private which means that only the AWS account holder (resource owner) that created it has access to the resource.&nbsp;The resource owner can optionally grant access permissions to others by writing an access policy.&nbsp;In S3, you also set the permissions of the object during upload to make it public.</p> <p>Amazon S3 offers access policy options broadly categorized as resource-based policies and user policies. Access policies you attach to your resources (buckets and objects) are referred to as resource-based policies.</p> <p>For example, bucket policies and access control lists (ACLs) are resource-based policies. You can also attach access policies to users in your account. These are called user policies. You may choose to use resource-based policies, user policies, or some combination of these to manage permissions to your Amazon S3 resources.</p> <p>Option 2 is incorrect as ACLs are primarily used to grant basic read/write permissions to AWS accounts and not suitable for providing public access over the Internet.&nbsp;</p> <p>Option 4 is incorrect. Although with IAM, you can create a user, group, or role that has certain permissions to the S3 bucket, it does not control the individual objects that are hosted in the bucket.</p> <p>Option 5 is incorrect because by default, all the S3 resources are private, so only the AWS account that created the resources can access them.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p> <p>&nbsp;</p> <p><strong>Additional learning material:</strong> How do I configure an S3 bucket policy to Deny all actions unless they meet certain conditions?</p> <p><iframe src="https://www.youtube.com/embed/8ew8MSXBiA4" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2567226, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You are working for a media company and you need to configure an Amazon S3 bucket to serve static assets for your public-facing web application. Which methods ensure that all of the objects uploaded to the S3 bucket can be read publicly all over the Internet? (Choose 2)', 'id': 10337484, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'STS', 'prompt': {'answers': ['Use an IAM policy that references the LDAP identifiers and AWS credentials.', '<p>Use AWS Single Sign-On (SSO) service to enable single sign-on between AWS and your LDAP.</p>', '<p>Develop an on-premises custom identity broker application and use STS to issue short-lived AWS credentials.</p>', 'Use IAM roles to rotate the IAM credentials whenever LDAP credentials are updated.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You have a requirement to integrate the Lightweight Directory Access Protocol (LDAP) directory service of your on-premises data center to your AWS VPC using IAM. The identity store which is currently being used is not compatible with SAML.\xa0 \xa0</p><p>Which of the following provides the most valid approach to implement the integration?</p>', 'explanation': '<p>If your identity store is not compatible with SAML 2.0, then you can build a custom identity broker application to perform a similar function. The broker application authenticates users, requests temporary credentials for users from AWS, and then provides them to the user to access AWS resources.</p> <p>The application verifies that employees are signed into the existing corporate network\'s identity and authentication system, which might use LDAP, Active Directory, or another system. The identity broker application then obtains temporary security credentials for the employees.</p> <p>To get temporary security credentials, the identity broker application calls either&nbsp;<strong><code class="code">AssumeRole</code></strong>&nbsp;or&nbsp;<strong><code class="code">GetFederationToken</code></strong>&nbsp;to obtain temporary security credentials, depending on how you want to manage the policies for users and when the temporary credentials should expire. The call returns temporary security credentials consisting of an AWS access key ID, a secret access key, and a session token. The identity broker application makes these temporary security credentials available to the internal company application. The app can then use the temporary credentials to make calls to AWS directly. The app caches the credentials until they expire, and then requests a new set of temporary credentials.</p> <p>&nbsp;</p> <p><img src="https://docs.aws.amazon.com/IAM/latest/UserGuide/images/enterprise-authentication-with-identity-broker-application.diagram.png" alt="" width="750" height="384" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because using an IAM policy is not enough to integrate your LDAP service to IAM. You need to use SAML, STS or a custom identity broker.</p> <p>Option 2 is incorrect because the scenario did not require SSO and in addition, the identity store that you are using is not SAML-compatible.</p> <p>Option 4 is incorrect because&nbsp;manually rotating the IAM credentials is not an optimal solution to integrate your on-premises and VPC network.&nbsp;You need to use SAML, STS or a custom identity broker.</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p> <p><a href="https://aws.amazon.com/blogs/aws/aws-identity-and-access-management-now-with-identity-federation/">https://aws.amazon.com/blogs/aws/aws-identity-and-access-management-now-with-identity-federation/</a></p> <p>&nbsp;</p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567228, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You have a requirement to integrate the Lightweight Directory Access Protocol (LDAP) directory service of your on-premises data center to your AWS VPC using IAM. The identity store which is currently being used is not compatible with SAML.\xa0 \xa0Which of the following provides the most valid approach to implement the integration?', 'id': 10337486, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'answers': ['<p>EBS</p>', '<p>Storage Gateway</p>', '<p>S3</p>', '<p>EFS</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a financial firm which is building an internal application that processes loans, accruals, and interest rates for their clients. They require a durable storage service that is able to handle future increases in storage capacity and can provide the lowest-latency access to their data. Their web application will be hosted in a single m5ad.24xlarge Reserved EC2 instance which will process and store data to the storage service. </p><p>Which of the following would be the most suitable storage service that you should use to meet this requirement?</p>', 'explanation': '<p>Amazon Web Services (AWS) offers cloud storage services to support a wide range of storage workloads such as Amazon S3, EFS and EBS. Amazon EFS is a file storage service for use with Amazon EC2. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage for up to thousands of Amazon EC2 instances.&nbsp;Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere. Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the <strong>lowest-latency access to data</strong> from a single EC2 instance.&nbsp;You can also increase EBS storage for up to 16TB or add new volumes for additional storage.</p> <p>In this scenario, the company is looking for a storage service which can provide the lowest-latency access to their data which will be fetched by a single m5ad.24xlarge Reserved EC2 instance. This type of workloads can be supported better by using either EFS or EBS but in this case, the latter is the most suitable storage service. As mentioned above, EBS provides the <em>lowest-latency</em> access to the data for your EC2 instance since the volume is directly attached to the instance. In addition, the scenario does not require concurrently-accessible storage since they only have one instance. Hence, the correct answer is Option 1.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-27_08-08-30-95c0c6fa077cd4d2c0dc4dd23c98ef09.png" alt="" width="750" height="562" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect since Storage Gateway is primarily used to extend your on-premises storage to your AWS Cloud.</p> <p>Option 3 is incorrect because although S3 is also highly available and highly scalable, it still does not provide the lowest-latency access to the data, unlike EBS. Remember that S3 does not reside within your VPC by default, which means the data will traverse the public Internet that may result to higher latency. You can set up a&nbsp;VPC Endpoint for S3 yet still, its latency is greater than that of EBS.</p> <p>Option 4 is incorrect because the scenario does not require concurrently-accessible storage since the internal application is only hosted in one instance. Although EFS can provide low latency data access to the EC2 instance as compared with S3, the storage service that can provide the lowest latency access is still EBS.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/ebs/ ">https://aws.amazon.com/ebs/ </a></p> <p><a href="https://aws.amazon.com/efs/faq/">https://aws.amazon.com/efs/faq/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></p> <p>&nbsp;</p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567230, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You are working as a Solutions Architect for a financial firm which is building an internal application that processes loans, accruals, and interest rates for their clients. They require a durable storage service that is able to handle future increases in storage capacity and can provide the lowest-latency access to their data. Their web application will be hosted in a single m5ad.24xlarge Reserved EC2 instance which will process and store data to the storage service. Which of the following would be the most suitable storage service that you should use to meet this requirement?', 'id': 10337488, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'answers': ['The newly created VPC has an invalid CIDR block.', 'Amazon Route53 is not enabled.', 'The DNS resolution and DNS hostname of the VPC configuration should be enabled.', 'The security group of the EC2 instance needs to be modified.'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You launched an EC2 instance in your newly created VPC. You have noticed that the generated instance does not have an associated DNS hostname. <br><br>Which of the following options could be a valid reason for this issue?', 'explanation': '<p>When you launch an EC2 instance into a default VPC, AWS provides it with public and private DNS hostnames that correspond to the public IPv4 and private IPv4 addresses for the instance.</p> <p>However, when you launch an instance into a non-default VPC, AWS provides the instance with a private DNS hostname only. New instances will only be provided with public DNS hostname depending on these two DNS attributes: the&nbsp;<strong>DNS resolution</strong> and <strong>DNS hostnames</strong>, that you have specified for your VPC, and if your instance has a public IPv4 address.</p> <p>In this case, the new EC2 instance does not automatically get a DNS hostname because the <strong>DNS resolution</strong> and <strong>DNS hostnames</strong> attributes are disabled in the newly created VPC.&nbsp;</p> <p>Option 1 is incorrect since it\'s very unlikely that a VPC has an invalid CIDR block because of AWS validation schemes.</p> <p>Option 2 is incorrect since Route 53 does not need to be enabled. Route 53 is the DNS service of AWS, but the VPC is the one that enables assigning of instance hostnames.</p> <p>Option 4 is incorrect since security groups are just firewalls for your instances. They filter traffic based on a set of security group rules.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-dns.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-dns.html</a></p> <p><a href="https://aws.amazon.com/vpc/" target="_blank" rel="noopener">https://aws.amazon.com/vpc/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567232, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You launched an EC2 instance in your newly created VPC. You have noticed that the generated instance does not have an associated DNS hostname. Which of the following options could be a valid reason for this issue?', 'id': 10337490, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'answers': ['Spot Instances', 'Reserved instances', 'Dedicated instances', 'On-Demand instances'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': 'You have a distributed application in AWS that periodically processes large volumes of data across multiple instances. You designed the application to recover gracefully from any instance failures. You are required to launch the application in the most cost-effective way. <br><br>Which type of EC2 instance will meet your requirements?', 'explanation': '<p>You require an EC2 instance that is the most cost-effective among other types. In addition, the application it will host is designed to gracefully recover in case of instance failures.&nbsp;</p> <p>In terms of cost-effectiveness, Spot and Reserved instances&nbsp;are the top options. And since the application can gracefully recover from instance failures, the Spot instance is the best option for this case as it is the cheapest type of EC2 instance. Remember that&nbsp;when you use Spot Instances, there will be interruptions. Amazon EC2 can interrupt your Spot Instance when the Spot price exceeds your maximum price, when the demand for Spot Instances rise, or when the supply of Spot Instances decreases. This makes Option 1 the correct answer.</p> <p>Option 2 is incorrect because although you could also use reserved instances to save costs, it entails a&nbsp;commitment of 1-year or 3-year terms of usage. Since your processes only run periodically, you won\'t be able to maximize the discounted price of using reserved instances.</p> <p>Options 3 and 4 are also incorrect because Dedicated and on-demand instances are not a&nbsp;cost-effective solution to use for your application.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-spot-instances-work.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-spot-instances-work.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p> <p>&nbsp;</p> <p><strong>Here is an in-depth look at Spot Instances:</strong></p> <p><iframe src="https://www.youtube.com/embed/PKvss-RgSjI" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567234, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You have a distributed application in AWS that periodically processes large volumes of data across multiple instances. You designed the application to recover gracefully from any instance failures. You are required to launch the application in the most cost-effective way. Which type of EC2 instance will meet your requirements?', 'id': 10337492, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Cognito', 'prompt': {'answers': ['<p>Add multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users.</p>', '<p>Add a new IAM policy to a user pool in Cognito.</p>', '<p>Integrate Cognito with Amazon SNS Mobile Push to allow additional authentication via SMS.</p>', '<p>Develop a custom application that integrates with Cognito that implements a second layer of authentication.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': "<p>A tech startup has recently received a Series A round of funding to continue building their mobile forex trading application. You are hired to set up their cloud architecture in AWS and to implement a highly available, fault tolerant system. For their database, they are using DynamoDB and for authentication, they have chosen to use Cognito. Since the mobile application contains confidential financial transactions, there is a requirement to add a second authentication method that doesn't rely solely on user name and password.\xa0 \xa0</p><p>How can you implement this in AWS?</p>", 'explanation': '<p>You can add multi-factor authentication (MFA) to a user pool to protect the identity of your users. MFA adds a second authentication method that doesn\'t rely solely on user name and password. You can choose to use SMS text messages, or time-based one-time (TOTP) passwords as second factors in signing in your users. You can also use adaptive authentication with its risk-based model to predict when you might need another authentication factor. It\'s part of the user pool advanced security features, which also include protections against compromised credentials.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/cognito/latest/developerguide/managing-security.html">https://docs.aws.amazon.com/cognito/latest/developerguide/managing-security.html</a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 4114046, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': "A tech startup has recently received a Series A round of funding to continue building their mobile forex trading application. You are hired to set up their cloud architecture in AWS and to implement a highly available, fault tolerant system. For their database, they are using DynamoDB and for authentication, they have chosen to use Cognito. Since the mobile application contains confidential financial transactions, there is a requirement to add a second authentication method that doesn't rely solely on user name and password.\xa0 \xa0How can you implement this in AWS?", 'id': 10337494, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'answers': ['<p>Store the files in S3 then after a month, change the storage class of the bucket to S3-IA using lifecycle policy.</p>', '<p>Store the files in S3 then after a month, change the storage class of the bucket to Intelligent-Tiering using lifecycle policy.</p>', '<p>Store the files in S3 then after a month, change the storage class of the <code>tutorialsdojo-finance</code> prefix to One Zone-IA while the remaining go to Glacier using lifecycle policy.</p>', '<p>Store the files in S3 then after a month, change the storage class of the <code>tutorialsdojo-finance</code> prefix to S3-IA while the remaining go to Glacier using lifecycle policy.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': "<p>There are a few, easily <strong>reproducible</strong> but confidential files that your client wants to store in AWS without worrying about storage capacity. For the first month, all of these files will be accessed frequently but after that, they will rarely be accessed at all. The old files will only be accessed by developers so there is no set retrieval time requirement. However, the files under a specific <code>tutorialsdojo-finance</code> prefix in the S3 bucket will be used for post-processing that requires millisecond retrieval time. </p><p>Given these conditions, which of the following options would be the most cost-effective solution for your client's storage needs?</p>", 'explanation': '<p>Initially, the files will be accessed frequently, and S3 is a durable and highly available storage solution for that. After a month has passed, the files won\'t be accessed frequently anymore, so it is a good idea to use lifecycle policies to move them to a storage&nbsp;class that would have a lower cost for storing them.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonS3/latest/dev/images/SupportedTransitionsWaterfallModel.png" /></p> <p>&nbsp;</p> <p>Since the files are easily reproducible and some of them are needed to be retrieved quickly based on a specific prefix filter (<code>tutorialsdojo-finance</code>), S3-One Zone IA would be a good choice for storing them. The other files that do not contain such prefix would then be moved to Glacier for low cost archival. This setup would also be the most cost-effective for the client. Hence, the correct answer is Option 3.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-04-02_04-26-21-158b6e1eb7ef7be3b91049e88f056f3a.gif" alt="" width="560" height="556" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because although it is valid to move the files to S3-IA, this solution still costs more compared with using a combination of S3-One Zone IA and Glacier.&nbsp;</p> <p>Option 2 is incorrect because while S3 Intelligent-Tiering can automatically move data between two access tiers (frequent access and infrequent access) when access patterns change, it is more suitable for scenarios where you don\'t know the access patterns of your data. It may take some time for S3 Intellgent-Tiering to analyze the access patterns before it moves the data to a cheaper storage class like S3-IA which means you may still end up paying more in the beginning. In addition, you already know the access patterns of the files which means you can directly change the storage class immediately and save cost right away.</p> <p>Option 4 is incorrect because although S3-IA costs less than S3 Standard storage class, it is still more expensive than S3-One Zone IA. Remember that the files are easily reproducible so you can safely move the data to&nbsp;S3-One Zone IA and in case there is an outage, you can simply generate the missing data again.</p> <p>&nbsp;</p> <p><strong>References:</strong>&nbsp;</p> <p><a href="https://aws.amazon.com/blogs/compute/amazon-s3-adds-prefix-and-suffix-filters-for-lambda-function-triggering">https://aws.amazon.com/blogs/compute/amazon-s3-adds-prefix-and-suffix-filters-for-lambda-function-triggering</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-configuration-examples.html ">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-configuration-examples.html </a></p> <p><a href="https://aws.amazon.com/s3/pricing">https://aws.amazon.com/s3/pricing</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a>&nbsp;</p>'}, 'correct_response': ['c'], 'original_assessment_id': 8159556, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': "There are a few, easily reproducible but confidential files that your client wants to store in AWS without worrying about storage capacity. For the first month, all of these files will be accessed frequently but after that, they will rarely be accessed at all. The old files will only be accessed by developers so there is no set retrieval time requirement. However, the files under a specific tutorialsdojo-finance prefix in the S3 bucket will be used for post-processing that requires millisecond retrieval time. Given these conditions, which of the following options would be the most cost-effective solution for your client's storage needs?", 'id': 10337496, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'answers': ['<p>Launch a NAT Gateway in a public subnet. Alternatively, you can also create a NAT instance in one private subnet.</p>', '<p>Launch a NAT instance in the public subnet and add a route from the private subnet to that NAT instance.</p>', '<p>Launch two NAT instances in two separate public subnets and add a route from the private subnet to each NAT instance to make it more fault tolerant.</p>', '<p>Launch two NAT instances in a public subnet and add a route from the private subnet to each NAT instance to make it more fault tolerant.</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Network Engineer for an electronics and communications company in Japan. You are told to implement a NAT instance in your VPC to allow certain EC2 instances to initiate connections to the Internet but restrict any requests coming from the Internet. </p><p>In this scenario, what is the best way to configure a fault-tolerant NAT instance in your VPC?</p>', 'explanation': '<p>You can use a NAT device to enable instances in a private subnet to connect to the Internet (for example, for software updates) or other AWS services, but prevent the Internet from initiating connections with the instances. A NAT device forwards traffic from the instances in the private subnet to the Internet or other AWS services, and then sends the response back to the instances. When traffic goes to the Internet, the source IPv4 address is replaced with the NAT device&rsquo;s address and similarly, when the response traffic goes to those instances, the NAT device translates the address back to those instances&rsquo; private IPv4 addresses.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-11_03-19-40-78dd08371d9ee9ac3df026e3d7571e6e.png" /><br /><br />Option 1 is incorrect because you should not be putting the NAT instances in private subnet as they need to communicate with the Internet. They should be in public subnet.<br /><br />Option 2 is incorrect because you would need at least two NAT instances for fault tolerance.<br /><br />Option 3 is correct because you should place two NAT instances in two separate public subnets, and create route from instances via each NAT instance for achieving fault tolerance.<br /><br />Option 4 is incorrect because if you put both NAT instances in a single public subnet and that subnet becomes unavailable or unreachable to the other instances, the architecture would not be fault tolerant.</p> <p><br /><strong>References</strong>:</p> <p><a href="https://aws.amazon.com/articles/2781451301784570">https://aws.amazon.com/articles/2781451301784570</a></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 8205330, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'You are working as a Network Engineer for an electronics and communications company in Japan. You are told to implement a NAT instance in your VPC to allow certain EC2 instances to initiate connections to the Internet but restrict any requests coming from the Internet. In this scenario, what is the best way to configure a fault-tolerant NAT instance in your VPC?', 'id': 10337498, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'answers': ['<p>Configure Sticky Sessions</p>', '<p>Configure both Cross-Zone Load Balancing and Sticky Sessions</p>', '<p>Configure Connection Draining</p>', '<p>Configure Proxy Protocol</p>'], 'relatedLectureIds': '', 'feedbacks': ['', '', '', ''], 'question': '<p>A bank portal application is hosted in an Auto Scaling group of EC2 instances behind a Classic Load Balancer (CLB). You are required to set up the architecture so that any back-end EC2 instances that you de-register should complete the in-progress requests first before the de-registration process takes effect. Conversely, if a back-end instance fails health checks, the load balancer should not send any new requests to the unhealthy instance but should allow existing requests to complete. </p><p>How will you configure your load balancer to satisfy the above requirement?</p>', 'explanation': '<p>To ensure that a Classic Load Balancer stops sending requests to instances that are de-registering or unhealthy while keeping the existing connections open, use connection draining. This enables the load balancer to complete in-flight requests made to instances that are de-registering or unhealthy. Hence, Option 3 is correct.</p> <p>When you enable connection draining, you can specify a maximum time for the load balancer to keep connections alive before reporting the instance as de-registered. The maximum timeout value can be set between 1 and 3,600 seconds (the default is 300 seconds). When the maximum time limit is reached, the load balancer forcibly closes connections to the de-registering instance.</p> <p>Option 1 is incorrect because the sticky sessions feature&nbsp;is mainly used to ensure that all requests from the user during the session are sent to the same instance.</p> <p>Option 2 is incorrect because configuring both Cross-Zone Load Balancing and Sticky Sessions will still not satisfy the requirement. Cross-Zone load balancing is mainly used to distribute requests evenly across the registered instances in all enabled Availability Zones. You have to enable Connection Draining.</p> <p>Option 4 is incorrect because Proxy Protocol is an Internet protocol used to carry connection information from the source requesting the connection to the destination for which the connection was requested.</p> <p>&nbsp;&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/config-conn-drain.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/config-conn-drain.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</a></span></p>'}, 'correct_response': ['c'], 'original_assessment_id': 8216992, '_class': 'assessment', 'updated': '2019-06-16T05:30:05Z', 'created': '2019-06-16T05:30:05Z', 'question_plain': 'A bank portal application is hosted in an Auto Scaling group of EC2 instances behind a Classic Load Balancer (CLB). You are required to set up the architecture so that any back-end EC2 instances that you de-register should complete the in-progress requests first before the de-registration process takes effect. Conversely, if a back-end instance fails health checks, the load balancer should not send any new requests to the unhealthy instance but should allow existing requests to complete. How will you configure your load balancer to satisfy the above requirement?', 'id': 10337500, 'related_lectures': [], 'assessment_type': 'multiple-choice'}]}, 'type': 'practice-test', 'title': 'AWS Certified Solutions Architect Associate Practice Test 4'}, {'quiz_data': {'next': None, 'count': 65, 'previous': None, 'results': [{'section': 'ElastiCache', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Set up an AWS Systems Manager Session Manager</p>', '<p>Enable the sticky session feature in the Classic Load Balancer</p>', '<p>Use Amazon ElastiCache</p>', '<p>Use the <code>GetSessionToken</code> action in AWS STS for session management</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are designing an online banking application which needs to have a distributed session data management. Currently, the application is hosted on an Auto Scaling group of On-Demand EC2 instances across multiple Availability Zones with a Classic Load Balancer that distributes the load. </p><p>Which of the following options should you do to satisfy the given requirement?</p>', 'explanation': '<p>In this question, the keyword is <strong>distributed </strong>session data management.&nbsp;</p> <p>Sticky session feature of the Classic Load Balancer can also provide session management, however, take note that this feature has its limitations such as, in the event of a failure, you are likely to lose the sessions that were resident on the failed node. In the event that the number of your web servers change when your Auto Scaling kicks in, it&rsquo;s possible that the traffic may be unequally spread across the web servers as active sessions may exist on particular servers. If not mitigated properly, this can hinder the scalability of your applications.&nbsp;Hence, sticky session is not scalable or "<strong>distributed</strong>" as compared with ElastiCache.</p> <p>You can manage HTTP session data from the web servers using an In-Memory Key/Value store such as Redis and Memcached.&nbsp;Redis is an open source, in-memory data structure store used as a database, cache, and message broker. Memcached is an in-memory key-value store for small arbitrary data (strings, objects) from results of database calls, API calls, or page rendering.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/product-marketing/caching-session-management-diagram-v2.c6856e6de83c4222dbc4853d9ff873f5542a86d8.PNG" alt="" width="750" height="356" /></p> <p>In AWS, you can use Amazon ElastiCache which offers fully managed Redis and Memcached service to manage and store session data for your web applications.</p> <p>Option 1 is incorrect because the Session Manager is simply a capability that lets you manage your Amazon EC2 instances through an interactive one-click browser-based shell or through the AWS CLI. This does not act as a distributed session data management.</p> <p>Option 2 is incorrect because although you can use the sticky session feature of the Classic Load Balancer to manage your session&nbsp;data, it is not a "distributed" solution compared to ElastiCache.</p> <p>Option 4 is incorrect because the&nbsp;<code>GetSessionToken</code> is just one of the available actions in STS which returns a set of temporary credentials for an AWS account or IAM user. This is not used for distributed session data management</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/caching/session-management/">https://aws.amazon.com/caching/session-management/</a></p> <p><a href="https://aws.amazon.com/elasticache/">https://aws.amazon.com/elasticache/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Elasticache Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/</span></a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Redis (cluster mode enabled vs disabled) vs Memcached:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-redis-cluster-mode-enabled-vs-disabled-vs-memcached/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-redis-cluster-mode-enabled-vs-disabled-vs-memcached/</span></a></p> <p>&nbsp;</p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567244, '_class': 'assessment', 'updated': '2019-06-22T04:59:35Z', 'created': '2019-06-22T04:59:35Z', 'question_plain': 'You are designing an online banking application which needs to have a distributed session data management. Currently, the application is hosted on an Auto Scaling group of On-Demand EC2 instances across multiple Availability Zones with a Classic Load Balancer that distributes the load. Which of the following options should you do to satisfy the given requirement?', 'id': 10430086, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Kinesis', 'prompt': {'relatedLectureIds': '', 'answers': ['Amazon S3', '<p>Glacier Select</p>', 'Amazon Redshift', '<p>AWS Glue</p>', '<p>Amazon Athena</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>A financial analytics application that collects, processes and analyzes stock data in real-time is using Kinesis Data Streams. The producers continually push data to Kinesis Data Streams while the consumers process the data in real time. In Amazon Kinesis, where can the consumers store their results? (Choose 2)</p>', 'explanation': '<p>In Amazon Kinesis, the producers continually push data to Kinesis Data Streams and the consumers process the data in real time. Consumers (<em>such as a custom application running on Amazon EC2, or an Amazon Kinesis Data Firehose delivery stream</em>) can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.</p> <p>Hence, Options 1 and 3 are the correct answers. The following diagram illustrates the high-level architecture of Kinesis Data Streams:</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/streams/latest/dev/images/architecture.png" width="500" height="215" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect because Glacier Select is not a storage service.&nbsp;It is primarily used to run queries directly on data stored in Amazon Glacier, retrieving only the data you need out of your archives to use for analytics.</p> <p>Option&nbsp;4 is incorrect because AWS Glue&nbsp;is not a storage service. It is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics.</p> <p>Option 5 is incorrect because Amazon Athena&nbsp;is just an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. It is not a storage service where you can store the results processed by the consumers.</p> <p>&nbsp;</p> <p><strong>Reference:</strong>&nbsp;</p> <p><a href="http://docs.aws.amazon.com/streams/latest/dev/key-concepts.html">http://docs.aws.amazon.com/streams/latest/dev/key-concepts.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon Kinesis Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</span></a></p>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2567264, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A financial analytics application that collects, processes and analyzes stock data in real-time is using Kinesis Data Streams. The producers continually push data to Kinesis Data Streams while the consumers process the data in real time. In Amazon Kinesis, where can the consumers store their results? (Choose 2)', 'id': 10430106, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['A bucket name must be unique across all existing bucket names in Amazon S3.', 'By default, an S3 bucket is not owned by the AWS account that created it', 'Bucket names must be at least 5 and no more than 63 characters long.', 'Bucket names can be formatted as an IP address such as 192.168.5.4'], 'feedbacks': ['', '', '', ''], 'question': '<p>An application which uses multiple EBS volumes could not cope with the growing storage requirements needed to store their data. Your IT Manager has instructed you to set up an S3 bucket as a replacement for their EBS volumes.\xa0 \xa0</p><p>Which of the following options is correct regarding the naming convention for the S3 bucket?</p>', 'explanation': '<p>The correct answer is:&nbsp;A bucket name must be unique across all existing bucket names in Amazon S3.</p> <p>A bucket is owned by the AWS account that created it. By default, you can create up to 100 buckets in each of your AWS accounts. There is no limit to the number of objects that can be stored in a bucket and no difference in performance whether you use many buckets or just a few. You can store all of your objects in a single bucket, or you can organize them across several buckets.</p> <p>The rules for DNS-compliant bucket names are as follows:</p> <ul> <li>-Bucket names must be at least 3 and no more than 63 characters long.</li> <li>-Bucket names must be a series of one or more labels. Adjacent labels are separated by a single period (.). Bucket names can contain lowercase letters, numbers, and hyphens. Each label must start and end with a lowercase letter or a number.</li> <li>-Bucket names must not be formatted as an IP address (for example, 192.168.5.4).</li> <li>-When using virtual hosted-style buckets with SSL, the SSL wildcard certificate only matches buckets that do not contain periods. To work around this, use HTTP or write your own certificate verification logic. We recommend that you do not use periods (".") in bucket names.</li> </ul> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567246, '_class': 'assessment', 'updated': '2019-06-22T04:59:35Z', 'created': '2019-06-22T04:59:35Z', 'question_plain': 'An application which uses multiple EBS volumes could not cope with the growing storage requirements needed to store their data. Your IT Manager has instructed you to set up an S3 bucket as a replacement for their EBS volumes.\xa0 \xa0Which of the following options is correct regarding the naming convention for the S3 bucket?', 'id': 10430088, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'AWS Shared Responsibility Model', 'prompt': {'relatedLectureIds': '', 'answers': ['EC2 Instance security', 'Physical network infrastructure', 'User access to the AWS environment via IAM', 'Virtualization infrastructure', '<p>Operating System (OS) patching for Spot and On-Demand EC2 instances</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are a Solutions Architect working for a major investment bank in London. Your IT Manager instructed you to prepare a migration plan from your on-premises application architecture to AWS. During your design process, you considered the current security of your on-premises application.\xa0 \xa0</p><p>Which among the following does AWS provide for you as part of the shared responsibility model? (Choose 2)</p>', 'explanation': '<p>Security and Compliance is a shared responsibility between AWS and the customer. This shared model can help relieve the customer&rsquo;s operational burden as AWS operates, manages, and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the service operates. The customer assumes responsibility and management of the guest operating system (including updates and security patches), other associated application software as well as the configuration of the AWS provided security group firewall.</p><p>Customers should carefully consider the services they choose as their responsibilities vary depending on the services used, the integration of those services into their IT environment, and applicable laws and regulations. The nature of this shared responsibility also provides the flexibility and customer control that permits the deployment. As shown in the chart below, this differentiation of responsibility is commonly referred to as Security &ldquo;of&rdquo; the Cloud versus Security &ldquo;in&rdquo; the Cloud.</p><p>Refer to this diagram for a better understanding of the shared responsibility model.</p><p><img src="https://d1.awsstatic.com/security-center/NewSharedResponsibilityModel.749924fb27d7c6de5cd59376dbaab323f86ce5dd.png" width="570" height="291" /></p><p>&nbsp;</p><p>Resources:</p><p><a href="https://aws.amazon.com/compliance/shared-responsibility-model/">https://aws.amazon.com/compliance/shared-responsibility-model/</a>&nbsp;</p>'}, 'correct_response': ['b', 'd'], 'original_assessment_id': 2567248, '_class': 'assessment', 'updated': '2019-06-22T04:59:35Z', 'created': '2019-06-22T04:59:35Z', 'question_plain': 'You are a Solutions Architect working for a major investment bank in London. Your IT Manager instructed you to prepare a migration plan from your on-premises application architecture to AWS. During your design process, you considered the current security of your on-premises application.\xa0 \xa0Which among the following does AWS provide for you as part of the shared responsibility model? (Choose 2)', 'id': 10430090, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'OpsWorks', 'prompt': {'relatedLectureIds': '', 'answers': ['AWS OpsWorks', 'AWS Elastic Beanstalk', 'AWS CloudFormation', '<p>AWS CodeDeploy</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are setting up a configuration management in your existing cloud architecture where you have to deploy and manage your EC2 instances including the other AWS resources using Chef and Puppet. Which of the following is the most suitable service to use in this scenario?</p>', 'explanation': '<p>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/opsworks/">https://aws.amazon.com/opsworks/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this&nbsp;AWS OpsWorks Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-opsworks/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-opsworks/</span></a></p> <p>&nbsp;</p> <p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567250, '_class': 'assessment', 'updated': '2019-06-22T04:59:35Z', 'created': '2019-06-22T04:59:35Z', 'question_plain': 'You are setting up a configuration management in your existing cloud architecture where you have to deploy and manage your EC2 instances including the other AWS resources using Chef and Puppet. Which of the following is the most suitable service to use in this scenario?', 'id': 10430092, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Glue', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>S3 Select</p>', '<p>Redshift Spectrum</p>', '<p>AWS Step Functions</p>', '<p>AWS Glue</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a data analytics startup that collects clickstream data and stores them in an S3 bucket. You need to launch an AWS Lambda function to trigger your ETL jobs to run as soon as new data becomes available in Amazon S3.\xa0 \xa0</p><p>Which of the following services can you use as an extract, transform, and load (ETL) service in this scenario?</p>', 'explanation': '<p>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You can create and run an ETL job with a few clicks in the AWS Management Console. You simply point AWS Glue to your data stored on AWS, and AWS Glue discovers your data and stores the associated metadata (e.g. table definition and schema) in the AWS Glue Data Catalog. Once cataloged, your data is immediately searchable, queryable, and available for ETL. AWS Glue generates the code to execute your data transformations and data loading processes.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://aws.amazon.com/glue/">https://aws.amazon.com/glue/</a></p> <p>&nbsp;</p> <p><strong>Introduction to AWS Glue:</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/qgWMfNSN9f4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'}, 'correct_response': ['d'], 'original_assessment_id': 2567254, '_class': 'assessment', 'updated': '2019-06-22T04:59:35Z', 'created': '2019-06-22T04:59:35Z', 'question_plain': 'You are working for a data analytics startup that collects clickstream data and stores them in an S3 bucket. You need to launch an AWS Lambda function to trigger your ETL jobs to run as soon as new data becomes available in Amazon S3.\xa0 \xa0Which of the following services can you use as an extract, transform, and load (ETL) service in this scenario?', 'id': 10430096, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Route53', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use a Network Load balancer to divert the traffic between the on-premises and AWS-hosted application.</p>', '<p>Use an Application Elastic Load balancer to divert and proportion the traffic between the on-premises and AWS-hosted application.</p>', '<p>Use Route 53 with Failover routing policy to divert and proportion the traffic between the on-premises and AWS-hosted application.</p>', '<p>Use Route 53 with Weighted routing policy to divert the traffic between the on-premises and AWS-hosted application.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a solutions architect for a large financial company. They have a web application hosted in their on-premises infrastructure which they want to migrate to AWS cloud. Your manager had instructed you to ensure that there is no downtime while the migration process is on-going. In order to achieve this, your team had decided to divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure. Once the migration is over and the application works with no issues, a full diversion to AWS will be implemented.\xa0 \xa0</p><p>Which of the following steps will you do to satisfy this requirement?</p>', 'explanation': '<p>To divert 50% of the traffic to the new application in AWS and the other 50% to the application, you can use Route53 with Weighted routing policy. This will divert the traffic between the on-premises and AWS-hosted application accordingly.</p> <p>Weighted routing lets you associate multiple resources with a single domain name (tutorialsdojo.com) or subdomain name (learn.tutorialsdojo.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software. You can set a specific percentage of how much traffic will be allocated to the resource by specifying the weights.</p> <p>For example, if you want to send a tiny portion of your traffic to one resource and the rest to another resource, you might specify weights of 1 and 255. The resource with a weight of 1 gets 1/256th of the traffic (1/1+255), and the other resource gets 255/256ths (255/1+255).</p> <p>You can gradually change the balance by changing the weights. If you want to stop sending traffic to a resource, you can change the weight for that record to 0.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567256, '_class': 'assessment', 'updated': '2019-06-22T04:59:35Z', 'created': '2019-06-22T04:59:35Z', 'question_plain': 'You are working as a solutions architect for a large financial company. They have a web application hosted in their on-premises infrastructure which they want to migrate to AWS cloud. Your manager had instructed you to ensure that there is no downtime while the migration process is on-going. In order to achieve this, your team had decided to divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure. Once the migration is over and the application works with no issues, a full diversion to AWS will be implemented.\xa0 \xa0Which of the following steps will you do to satisfy this requirement?', 'id': 10430098, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Glacier', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Upload the data to S3 then use a lifecycle policy to transfer data to S3-IA.</p>', '<p>Upload the data to S3 and set a lifecycle policy to transition data to Glacier after <code>0</code> days.</p>', '<p>Upload the data directly to Amazon Glacier through the AWS Management Console.</p>', '<p>Upload the data to S3 then use a lifecycle policy to transfer data to S3 One Zone-IA.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A company has 10 TB of infrequently accessed financial data files that would need to be stored in AWS. These data would be accessed infrequently during specific weeks when they are retrieved for auditing purposes. The retrieval time is not strict as long as it does not exceed 24 hours. </p><p>Which of the following would be a secure, durable, and cost-effective solution for this scenario?</p>', 'explanation': '<p>Glacier is a cost-effective archival solution for large amounts of data. Bulk retrievals are S3 Glacier&rsquo;s lowest-cost retrieval option, enabling you to retrieve large amounts, even petabytes, of data inexpensively in a day. Bulk retrievals typically complete within 5 &ndash; 12 hours. You can specify an absolute or relative time period (including 0 days) after which the specified Amazon S3 objects should be transitioned to Amazon Glacier. Hence, Option 2 is the correct answer.</p> <p>Glacier has a management console which you can use to create and delete vaults. However, you cannot directly upload archives to Glacier by using the management console. To upload data, such as photos, videos, and other documents, you must either use the AWS CLI or write code to make requests, by using either the REST API directly or by using the AWS SDKs.&nbsp;</p> <p>Take note that uploading data to the S3 Console and setting its storage class of "Glacier" is a different story as the proper way to upload data to Glacier is still via its API or CLI. In this way, you can set up your vaults and configure your retrieval options. If you uploaded your data using the S3 console then <span style="text-decoration: underline;">it will be managed via S3</span> even though it is internally, using a Glacier storage class. Hence, you won\'t be able to use Vaults, set your Retrieval Options or purchase Provisioned Capacity for your archives.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonS3/latest/user-guide/images/lifecycle-config-transition.png" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because using Glacier would be a more cost-effective solution than using S3-IA. Since required retrieval periods should not exceed more than a day, Glacier would be the best choice.</p> <p>Option 3 is incorrect because you cannot upload objects to Amazon Glacier directly through the Management Console. To upload data, such as photos, videos, and other documents, you must either use the AWS CLI or write code to make requests, by using either the REST API directly or by using the AWS SDKs.</p> <p>Option 4 is incorrect because with S3 One Zone-IA, the data will only be stored in a single availability zone and thus, this storage solution is not durable. It also costs more compared with Glacier, which is why this option is wrong.</p> <p><br /><strong>References:</strong></p> <p><a href="https://aws.amazon.com/glacier/faqs/">https://aws.amazon.com/glacier/faqs/</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html</a></p> <p><a href="https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html">https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Glacier Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-glacier/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-glacier/</a></span></p> <p>&nbsp;</p> <p><strong>Here is a deep dive on Amazon S3 and Glacier Best Practices:</strong></p> <p><iframe src="https://www.youtube.com/embed/rHeTn9pHNKo" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567258, '_class': 'assessment', 'updated': '2019-06-22T05:17:51Z', 'created': '2019-06-22T04:59:35Z', 'question_plain': 'A company has 10 TB of infrequently accessed financial data files that would need to be stored in AWS. These data would be accessed infrequently during specific weeks when they are retrieved for auditing purposes. The retrieval time is not strict as long as it does not exceed 24 hours. Which of the following would be a secure, durable, and cost-effective solution for this scenario?', 'id': 10430100, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Billing and Cost Management', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>$0.08</p>', '<p>$0.07</p>', '<p>$0.06</p>', '<p>$0.00</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are building a prototype for a cryptocurrency news website of a small startup. The website will be deployed to a Spot EC2 instance and will use Amazon Aurora as its database. You requested a spot instance at a maximum price of $0.04/hr which has been fulfilled immediately and after 90 minutes, the spot price increases to $0.06/hr and then your instance was terminated by AWS.\xa0 \xa0</p><p>In this scenario, what would be the total cost of running your spot instance?\xa0 </p>', 'explanation': '<p>Since the Spot instance has been running for more than an hour, which is past the first instance hour, this means that you will be charged from the time it was launched till the time it was terminated by AWS. The computation for your 90 minute usage would be $0.04 (60 minutes) + $0.02 (30 minutes) = $0.06 hence, option 3 is correct.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/SpotInstance_spotinstancepricinghistory-gwt.png" alt="" width="700" height="425" /></p> <p>&nbsp;</p> <p>Based on the official EC2 FAQ:&nbsp;</p> <p><strong>Q. How will I be charged if my Spot instance is interrupted?</strong></p> <p><em>If your Spot instance is terminated or stopped by Amazon EC2 in the first instance hour, you will not be charged for that usage. However, if you terminate the instance yourself, you will be charged to the nearest second. If the Spot instance is terminated or stopped by Amazon EC2 in any subsequent hour, <span style="text-decoration: underline;">you will be charged for your usage to the nearest second</span>. If you are running on Windows and you terminate the instance yourself, you will be charged for an entire hour.</em></p> <p>Take note that there is one ambiguous <a href="https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/how-spot-instances-work.html">AWS document</a> which says:</p> <p><em>Spot Instances perform exactly like other EC2 instances while running and can be terminated when you no longer need them. If you terminate your instance, you pay for any partial hour used (as you do for On-Demand or Reserved Instances). However, you are not charged for any partial hour of usage if the Spot price goes above your maximum price and Amazon EC2 interrupts your Spot Instance.</em></p> <p>The above paragraph may seem to contradict what the official EC2 FAQ said that you will be charged for your usage to the <strong>nearest second&nbsp;</strong>if the Spot instance is terminated or stopped by Amazon EC2 in any <strong>subsequent hour</strong>. Therefore, please be reminded that the above paragraph is only applicable if the Spot Instance is terminated by Amazon EC2 in the first instance hour only, and not during any subsequent hour (more than 60 minutes) which is what the scenario depicts.</p> <p>I have personally contacted AWS about this issue in their documentation:</p> <p><a href="https://github.com/awsdocs/amazon-ec2-user-guide/pull/65">https://github.com/awsdocs/amazon-ec2-user-guide/pull/65</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/ec2/faqs/">https://aws.amazon.com/ec2/faqs/</a></p> <p><a href="https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/how-spot-instances-work.html">https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-leveraging-ec2-spot-instances/how-spot-instances-work.html</a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567252, '_class': 'assessment', 'updated': '2019-06-22T04:59:35Z', 'created': '2019-06-22T04:59:35Z', 'question_plain': 'You are building a prototype for a cryptocurrency news website of a small startup. The website will be deployed to a Spot EC2 instance and will use Amazon Aurora as its database. You requested a spot instance at a maximum price of $0.04/hr which has been fulfilled immediately and after 90 minutes, the spot price increases to $0.06/hr and then your instance was terminated by AWS.\xa0 \xa0In this scenario, what would be the total cost of running your spot instance?', 'id': 10430094, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Kinesis', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>First, set up an Auto Scaling group of EC2 servers then store the logs on Amazon S3 then finally, use EMR to apply heuristics on the logs.</p>', 'First, send all of the log events to Amazon Kinesis then afterwards, develop a client process to apply heuristics on the logs.', 'First, configure Amazon Cloud Trail to receive custom logs and then use EMR to apply heuristics on the logs.', '<p>First, send all the log events to Amazon SQS then set up an Auto Scaling group of EC2 servers to consume the logs and finally, apply the heuristics.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a multinational telecommunications company. Your IT Manager is willing to consolidate their log streams including the access, application, and security logs in one single system. Once consolidated, the company wants to analyze these logs in real-time based on heuristics. There will be some time in the future where the company will need to validate heuristics, which requires going back to data samples extracted from the last 12 hours.\xa0 </p><p>What is the best approach to meet this requirement?</p>', 'explanation': '<p>In this scenario, you need a service that can collect, process, and analyze data in real-time hence, the right service to use here is Amazon Kinesis.</p> <p>Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.</p> <p>With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/kinesis/">https://aws.amazon.com/kinesis/</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon Kinesis Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567260, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are working for a multinational telecommunications company. Your IT Manager is willing to consolidate their log streams including the access, application, and security logs in one single system. Once consolidated, the company wants to analyze these logs in real-time based on heuristics. There will be some time in the future where the company will need to validate heuristics, which requires going back to data samples extracted from the last 12 hours.\xa0 What is the best approach to meet this requirement?', 'id': 10430102, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EFS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>S3</p>', '<p>EFS</p>', '<p>EBS</p>', '<p>Storage Gateway</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A leading e-commerce company is in need of a storage solution that can be accessed by 1000 Linux servers in multiple availability zones. The service should be able to handle the rapidly changing data at scale while still maintaining high performance. It should also be highly durable and highly available whenever the servers will pull data from it, with little need for management. </p><p>As the Solutions Architect, which of the following services is the most cost-effective choice that you should use to meet the above requirement?</p>', 'explanation': '<p>Amazon Web Services (AWS) offers cloud storage services to support a wide range of storage workloads such as EFS, S3 and EBS. You have to understand when you should use Amazon EFS, Amazon S3 and Amazon Elastic Block Store (EBS) based on the specific workloads. In this scenario, the keywords are&nbsp;<em><strong>rapidly changing data</strong></em> and 1000 Linux servers.</p> <p>Amazon EFS is a file storage service for use with Amazon EC2. Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage for <strong>up to thousands of Amazon EC2 instances</strong>. EFS provides the same level of high availability and high scalability like S3 however, this service is more suitable for scenarios where it is required to have a POSIX-compatible file system or if you are storing rapidly changing data.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/r2018/b/EFS/product-page-diagram-Amazon-EFS-Launch_How-It-Works.cf947858f0ef3557b9fc14077bdf3f65b3f9ff43.png" alt="" width="700" height="193" /></p> <p>&nbsp;</p> <p>Data that must be updated very frequently might be better served by storage solutions that take into account read and write latencies, such as Amazon EBS volumes, Amazon RDS, Amazon DynamoDB, Amazon EFS, or relational databases running on Amazon EC2.&nbsp;</p> <p>Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest-latency access to data from a single EC2 instance.</p> <p>Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere.</p> <p>In this scenario, Option 2 is the best answer. As stated above, Amazon EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage for <strong>up to thousands of Amazon EC2 instances</strong>. EFS provides the performance, durability, high availability, and storage capacity needed by the 1000 Linux servers in the scenario.</p> <p>Option 1 is incorrect because although S3 provides the same level of high availability and high scalability like EFS, this service is not suitable for storing data which are rapidly changing, just as mentioned in the above explanation. It is still more effective to use EFS as it offers strong consistency and file locking which the S3 service lacks.&nbsp;</p> <p>Option 3 is incorrect because an EBS Volume cannot be shared by multiple instances.</p> <p>Option 4 is incorrect because Storage Gateway is primarily used to extend the storage of your on-premises data center to your AWS Cloud.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html">https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html</a></p> <p><a href="https://aws.amazon.com/efs/features/"> https://aws.amazon.com/efs/features/</a></p> <p><a href="https://d1.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf#page=9">https://d1.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf#page=9</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EFS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-efs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-efs/</span></a></p> <p>&nbsp;</p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567262, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A leading e-commerce company is in need of a storage solution that can be accessed by 1000 Linux servers in multiple availability zones. The service should be able to handle the rapidly changing data at scale while still maintaining high performance. It should also be highly durable and highly available whenever the servers will pull data from it, with little need for management. As the Solutions Architect, which of the following services is the most cost-effective choice that you should use to meet the above requirement?', 'id': 10430104, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Security', 'prompt': {'relatedLectureIds': '', 'answers': ["<p>You're using an SSH private key but the corresponding public key is not in the authorized_keys file.</p>", "<p>You don't have permissions for your authorized_keys file.</p>", "<p>You don't have permissions for the <code>.ssh</code> folder.</p>", '<p>The SSH private key that you are using has a file permission of 0777.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A Junior DevOps Engineer deployed a large EBS-backed EC2 instance to host a NodeJS web app in AWS which was developed by an IT contractor. He properly configured the security group and used a key pair named "tutorialsdojokey" which has a <code>tutorialsdojokey.pem</code> private key file. The EC2 instance works as expected and the junior DevOps engineer can connect to it using an SSH connection. The IT contractor was also given the key pair and he has made various changes in the instance as well to the files located in <code>.ssh</code> folder to make the NodeJS app work. After a few weeks, the IT contractor and the junior DevOps engineer cannot connect the EC2 instance anymore, even with a valid private key file. They are constantly getting a "Server refused our key" error even though their private key is valid. </p><p>In this scenario, which one of the following options is not a possible reason for this issue?</p>', 'explanation': '<p>All of the options here are correct except for Option 4 because if the private key that you are using has a file permission of 0777, then it will throw an "Unprotected Private Key File" error and not a "Server refused our key" error.</p> <p>You might be unable to log into an EC2 instance if:</p> <ul> <li>You\'re using an SSH private key but the corresponding public key is not in the authorized_keys file.</li> <li>You don\'t have permissions for your authorized_keys file.</li> <li>You don\'t have permissions for the .ssh folder.</li> <li>Your authorized_keys file or .ssh folder isn\'t named correctly.</li> <li>Your authorized_keys file or .ssh folder was deleted.</li> <li>Your instance was launched without a key, or it was launched with an incorrect key.</li> </ul> <p>To connect to your EC2 instance after receiving the error "Server refused our key," you can update the instance\'s user data to append the specified SSH public key to the authorized_keys file, which sets the appropriate ownership and file permissions for the SSH directory and files contained in it. </p> <p>&nbsp;</p> <p><strong>Reference:</strong> </p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/ec2-server-refused-our-key/">https://aws.amazon.com/premiumsupport/knowledge-center/ec2-server-refused-our-key/</a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567268, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A Junior DevOps Engineer deployed a large EBS-backed EC2 instance to host a NodeJS web app in AWS which was developed by an IT contractor. He properly configured the security group and used a key pair named "tutorialsdojokey" which has a tutorialsdojokey.pem private key file. The EC2 instance works as expected and the junior DevOps engineer can connect to it using an SSH connection. The IT contractor was also given the key pair and he has made various changes in the instance as well to the files located in .ssh folder to make the NodeJS app work. After a few weeks, the IT contractor and the junior DevOps engineer cannot connect the EC2 instance anymore, even with a valid private key file. They are constantly getting a "Server refused our key" error even though their private key is valid. In this scenario, which one of the following options is not a possible reason for this issue?', 'id': 10430110, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['Direct Connect', '<p>Route 53</p>', 'ClassicLink', 'AWS Direct Link'], 'feedbacks': ['', '', '', ''], 'question': "<p>You are implementing a hybrid architecture for your company where you are connecting their Amazon Virtual Private Cloud (VPC) to their on-premises network. Which of the following can be used to create a private connection between the VPC and your company's on-premises network?</p>", 'explanation': '<p>Direct Connect creates a direct, private connection from your on-premises data center to AWS, letting you establish a 1-gigabit or 10-gigabit dedicated network connection using Ethernet fiber-optic cable.&nbsp;&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/connect-vpc/">https://aws.amazon.com/premiumsupport/knowledge-center/connect-vpc/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-direct-connect/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-direct-connect/</span></a></p> <p>&nbsp;</p> <p><strong>Additional tutorial - how to configure a VPN over AWS Direct Connect:</strong></p> <p><iframe src="https://www.youtube.com/embed/dhpTTT6V1So" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p> <p>&nbsp;</p> <p><strong>S3 Transfer Acceleration vs Direct Connect vs VPN vs Snowball vs Snowmobile:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567270, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': "You are implementing a hybrid architecture for your company where you are connecting their Amazon Virtual Private Cloud (VPC) to their on-premises network. Which of the following can be used to create a private connection between the VPC and your company's on-premises network?", 'id': 10430112, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Trusted Advisor', 'prompt': {'relatedLectureIds': '', 'answers': ['AWS Cloudwatch', 'AWS EC2', 'AWS Trusted Advisor', 'AWS SNS'], 'feedbacks': ['', '', '', ''], 'question': 'Your IT Director instructed you to ensure that all of the AWS resources in your VPC don’t go beyond their service limit. <br><br>Which of the following services can help in this task?', 'explanation': '<p>Remember that the AWS Trusted Advisor analyzes your AWS environment and provides best practice recommendations in these five categories: <strong>C</strong>ost Optimization, <strong>P</strong>erformance, <strong>F</strong>ault Tolerance, <strong>S</strong>ecurity, and <strong>S</strong>ervice Limits. You can use a mnemonic, such as CPFSS, to memorize these five categories.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/premiumsupport/trustedadvisor/">https://aws.amazon.com/premiumsupport/trustedadvisor/</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;AWS Trusted Advisor Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-trusted-advisor/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-trusted-advisor/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567272, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'Your IT Director instructed you to ensure that all of the AWS resources in your VPC don’t go beyond their service limit. Which of the following services can help in this task?', 'id': 10430114, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Auto Scaling', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Choose the Availability Zone with the most number of instances, which is the us-west-1a Availability Zone in this scenario.</p>', '<p>Choose the Availability Zone with the least number of instances, which is the us-west-1c Availability Zone in this scenario.</p>', '<p>Select the instances with the most recent launch configuration.</p>', '<p>Select the instances with the oldest launch configuration.</p>', '<p>Select the instance that is closest to the next billing hour.</p>', '<p>Select the instance that is farthest to the next billing hour. </p>'], 'feedbacks': ['', '', '', '', '', ''], 'question': '<p>An online shopping platform is hosted on an Auto Scaling group of On-Demand EC2 instances with a default Auto Scaling termination policy and no instance protection configured. The system is deployed across three Availability Zones in the US West region (us-west-1) with an Application Load Balancer in front to provide high availability and fault tolerance for the shopping platform. The us-west-1a, us-west-1b, and us-west-1c Availability Zones have 10, 8 and 7 running instances respectively. Due to the low number of incoming traffic, the scale-in operation has been triggered.\xa0 \xa0</p><p>Which of the following will the Auto Scaling group do to determine which instance to terminate first in this scenario? (Choose 3)</p>', 'explanation': '<p>The default termination policy is designed to help ensure that your network architecture spans Availability Zones evenly. With the default termination policy, the behavior of the Auto Scaling group is as follows:</p> <p style="padding-left: 30px;">1. If there are instances in multiple Availability Zones, choose the Availability Zone with the most instances and at least one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, choose the Availability Zone with the instances that use the oldest launch configuration.</p> <p style="padding-left: 30px;">2. Determine which unprotected instances in the selected Availability Zone use the oldest launch configuration. If there is one such instance, terminate it.</p> <p style="padding-left: 30px;">3. If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) If there is one such instance, terminate it.</p> <p style="padding-left: 30px;">4. If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random.</p> <p>The following flow diagram illustrates how the default termination policy works:</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/termination-policy-default-flowchart-diagram.png" /> <br /><br /></p> <p><strong>Reference:</strong><br /><br /><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/</span></a></p>'}, 'correct_response': ['a', 'd', 'e'], 'original_assessment_id': 2567266, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'An online shopping platform is hosted on an Auto Scaling group of On-Demand EC2 instances with a default Auto Scaling termination policy and no instance protection configured. The system is deployed across three Availability Zones in the US West region (us-west-1) with an Application Load Balancer in front to provide high availability and fault tolerance for the shopping platform. The us-west-1a, us-west-1b, and us-west-1c Availability Zones have 10, 8 and 7 running instances respectively. Due to the low number of incoming traffic, the scale-in operation has been triggered.\xa0 \xa0Which of the following will the Auto Scaling group do to determine which instance to terminate first in this scenario? (Choose 3)', 'id': 10430108, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Auto Scaling', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Target tracking scaling</p>', '<p>Step scaling</p>', '<p>Simple scaling</p>', '<p>Scheduled Scaling</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>An application is hosted in an Auto Scaling group of EC2 instances. To improve the monitoring process, you have to configure the current capacity to increase or decrease based on a set of scaling adjustments. This should be done by specifying the scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process. </p><p>Which of the following is the most suitable type of scaling policy that you should use?</p>', 'explanation': '<p>With step scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process as well as define how your scalable target should be scaled when a threshold is in breach for a specified number of evaluation periods. Step scaling policies increase or decrease the current capacity of a scalable target based on a set of scaling adjustments, known as step adjustments. The adjustments vary based on the size of the alarm breach. After a scaling activity is started, the policy continues to respond to additional alarms, even while a scaling activity is in progress. Therefore, all alarms that are breached are evaluated by Application Auto Scaling as it receives the alarm messages.</p> <p>When you configure dynamic scaling, you must define how to scale in response to changing demand. For example, you have a web application that currently runs on two instances and you want the CPU utilization of the Auto Scaling group to stay at around 50 percent when the load on the application changes. This gives you extra capacity to handle traffic spikes without maintaining an excessive amount of idle resources. You can configure your Auto Scaling group to scale automatically to meet this need. The policy type determines how the scaling action is performed.</p> <p>&nbsp;</p> <div class="itemizedlist"><img style="display: block; margin-left: auto; margin-right: auto;" src="https://media.amazonwebservices.com/blog/2015/as_create_stepped_group_5.png" /> <p>&nbsp;</p> <p>Amazon EC2 Auto Scaling supports the following types of scaling policies:</p> <p style="padding-left: 30px;"><strong>Target tracking scaling -&nbsp;</strong>Increase or decrease the current capacity of the group based on a target value for a specific metric. This is similar to the way that your thermostat maintains the temperature of your home &ndash; you select a temperature and the thermostat does the rest.</p> <p style="padding-left: 30px;"><strong>Step scaling -&nbsp;</strong>Increase or decrease the current capacity of the group based on a set of scaling adjustments, known as&nbsp;<em>step adjustments</em>, that vary based on the size of the alarm breach.</p> <p style="padding-left: 30px;"><strong>Simple scaling -&nbsp;</strong>Increase or decrease the current capacity of the group based on a single scaling adjustment.</p> </div> <p>If you are scaling based on a utilization metric that increases or decreases proportionally to the number of instances in an Auto Scaling group, then it is recommended that you use target tracking scaling policies. Otherwise, it is better to use step scaling policies instead.</p> <p>Hence, the correct answer in this scenario is Option 2 - Step Scaling.</p> <p>Option 1 is incorrect because the target tracking scaling policy increases or decreases the current capacity of the group based on a <strong>target value for a specific metric</strong>, instead of a set of scaling adjustments.</p> <p>Option 3 is incorrect because the simple scaling policy increases or decreases the current capacity of the group based on a <strong>single</strong> scaling adjustment, instead of a set of scaling adjustments.</p> <p>Option 4 is incorrect because the scheduled scaling policy is based on a schedule that allows you to set your own scaling schedule for <strong>predictable</strong> load changes. This is not considered as one of the types of dynamic scaling.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p> <p><a href="https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html">https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html</a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567274, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'An application is hosted in an Auto Scaling group of EC2 instances. To improve the monitoring process, you have to configure the current capacity to increase or decrease based on a set of scaling adjustments. This should be done by specifying the scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process. Which of the following is the most suitable type of scaling policy that you should use?', 'id': 10430116, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use SSL to encrypt the data while in transit to Amazon S3.</p>', '<p>Implement Amazon S3 server-side encryption with Amazon S3-Managed Encryption Keys.</p>', '<p>Implement Amazon S3 server-side encryption with AWS KMS-Managed Keys (SSE-KMS).</p>', '<p>Implement Amazon S3 server-side encryption with customer-provided keys (SSE-C).</p>', '<p>Encrypt the data on the client-side before sending to Amazon S3 using their own master key.</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a multinational IT consultancy company where you are managing an application hosted in an Auto Scaling group of EC2 instances which stores data in an S3 bucket. You must ensure that the data are encrypted at rest using an encryption key that is both provided and managed by the company. This change should also provide AES-256 encryption to their data to comply with the strict security policy of the company. </p><p>Which of the following actions should you implement to achieve this? (Choose 2)</p>', 'explanation': '<p>Data protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit by using SSL or by using client-side encryption.</p> <p>You have the following options for protecting data at rest in Amazon S3:</p> <p><strong>Use Server-Side Encryption</strong>&nbsp;&ndash; You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects.</p> <div style="padding-left: 30px;">1. Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)</div> <div style="padding-left: 30px;">2. Use Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS)</div> <div style="padding-left: 30px;">3. Use Server-Side Encryption with Customer-Provided Keys (SSE-C)</div> <p>&nbsp;</p> <p><strong>Use Client-Side Encryption</strong>&nbsp;&ndash; You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p> <div style="padding-left: 30px;">1. Use Client-Side Encryption with AWS KMS&ndash;Managed Customer Master Key (CMK)</div> <div style="padding-left: 30px;">2. Use Client-Side Encryption Using a Client-Side Master Key <p>&nbsp;</p> </div> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/security-center/SecurityBlog/bucket_policies_defense_s3.43e6c93a095f2f55b33b30276f4782ab9ec79f47.png" alt="" width="650" height="366" /></p> <p>&nbsp;</p> <p>Hence, the valid actions that you can implement in this scenario are Options 4 and 5.</p> <p>Option 1 is incorrect because&nbsp;the requirement is to only secure the data at rest and not data in transit. Hence, you have to use server-side encryption instead.</p> <p>Option 2 is incorrect because although you can upload the company\'s customer master keys (CMKs), the keys will be managed by KMS and not your company. This does not comply with the security policy mandated by the company.&nbsp;&nbsp;</p> <p>Option 3 is incorrect because the Amazon S3-Managed encryption does not comply with the policy mentioned in the given scenario since the keys are managed by AWS (through Amazon S3) and not by the company. The suitable server-side encryption that you should use here is SSE-C.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['d', 'e'], 'original_assessment_id': 2567276, '_class': 'assessment', 'updated': '2019-06-22T05:43:40Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are working as a Solutions Architect for a multinational IT consultancy company where you are managing an application hosted in an Auto Scaling group of EC2 instances which stores data in an S3 bucket. You must ensure that the data are encrypted at rest using an encryption key that is both provided and managed by the company. This change should also provide AES-256 encryption to their data to comply with the strict security policy of the company. Which of the following actions should you implement to achieve this? (Choose 2)', 'id': 10430118, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['All data moving between the volume and the instance are encrypted.', 'Snapshots are automatically encrypted.', 'Snapshots are not automatically encrypted.', '<p>Only the data in the volume is encrypted and not all the data moving between the volume and the instance.</p>', '<p>The volumes created from the encrypted snapshot are not encrypted.</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>An application is hosted on an EC2 instance with multiple EBS Volumes attached and uses Amazon Neptune as its database. To improve data security, you encrypted all of the EBS volumes attached to the instance to protect the confidential data stored in the volumes.\xa0 </p><p>Which of the following statements are true about encrypted Amazon Elastic Block Store volumes? (Choose 2)</p>', 'explanation': '<p>Amazon Elastic Block Store (Amazon EBS) provides block level storage volumes for use with EC2 instances. EBS volumes are highly available and reliable storage volumes that can be attached to any running instance that is in the same Availability Zone. EBS volumes that are attached to an EC2 instance are exposed as storage volumes that persist independently from the life of the instance.&nbsp;</p> <p>&nbsp;&nbsp;<img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/architecture_storage.png" alt="" width="639" height="360" /></p> <p>&nbsp;</p> <p>When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:</p> <p>&nbsp; &nbsp;- Data at rest inside the volume</p> <p>&nbsp; &nbsp;- All data moving between the volume and the instance</p> <p>&nbsp; &nbsp;- All snapshots created from the volume</p> <p>&nbsp; &nbsp;- All volumes created from those snapshots</p> <p>Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage. You can encrypt both the boot and data volumes of an EC2 instance.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html ">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html </a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['a', 'b'], 'original_assessment_id': 2567278, '_class': 'assessment', 'updated': '2019-06-22T05:28:35Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'An application is hosted on an EC2 instance with multiple EBS Volumes attached and uses Amazon Neptune as its database. To improve data security, you encrypted all of the EBS volumes attached to the instance to protect the confidential data stored in the volumes.\xa0 Which of the following statements are true about encrypted Amazon Elastic Block Store volumes? (Choose 2)', 'id': 10430120, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ["Launch the UAT and production EC2 instances in separate VPC's connected by VPC peering.", 'Create an IAM policy with a condition which allows access to only EC2 instances that are used for production or development.', 'Launch the UAT and production instances in different Availability Zones and use Multi Factor Authentication.', 'Define the tags on the UAT and production servers and add a condition to the IAM policy which allows access to specific tags. '], 'feedbacks': ['', '', '', ''], 'question': 'You have EC2 instances running on your VPC. You have both UAT and production EC2 instances running. You want to ensure that employees who are responsible for the UAT instances don’t have the access to work on the production instances to minimize security risks.<br><br>Which of the following would be the best way to achieve this?', 'explanation': '<p>For this scenario, the best way to achieve this solution is to use a combination of Tags and IAM policies. You can define the tags on the UAT and production EC2 instances and add a condition to the IAM policy which allows access to specific tags.</p> <p>Tags enable you to categorize your AWS resources in different ways, for example, by purpose, owner, or environment. This is useful when you have many resources of the same type &mdash; you can quickly identify a specific resource based on the tags you\'ve assigned to it.</p> <p>By default, IAM users don\'t have permission to create or modify Amazon EC2 resources, or perform tasks using the Amazon EC2 API. (This means that they also can\'t do so using the Amazon EC2 console or CLI.) To allow IAM users to create or modify resources and perform tasks, you must create IAM policies that grant IAM users permission to use the specific resources and API actions they\'ll need, and then attach those policies to the IAM users or groups that require those permissions.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a>&nbsp;</p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-policies-for-amazon-ec2.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-policies-for-amazon-ec2.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567280, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You have EC2 instances running on your VPC. You have both UAT and production EC2 instances running. You want to ensure that employees who are responsible for the UAT instances don’t have the access to work on the production instances to minimize security risks.Which of the following would be the best way to achieve this?', 'id': 10430122, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Security', 'prompt': {'relatedLectureIds': '', 'answers': ['Place the EC2 instances into private subnets', 'Remove the Internet Gateway from the VPC', 'Block the IP addresses in the Network Access Control List', 'Deploy the EC2 instances into private subnets then set up a bastion host'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a top IT Consultancy and one of your clients asked you how to properly secure their AWS infrastructure. They have a VPC with two On-Demand EC2 instances with Elastic IP addresses which recently were under SSH brute force attacks over the Internet. Their IT Security team has identified the IP addresses where these attacks originated.\xa0 </p><p>Which of the following is the quickest way to fix this security vulnerability?\xa0 </p>', 'explanation': '<p>A&nbsp;<em>network access control list (ACL)</em>&nbsp;is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/userguide/images/security-diagram.png" /></p> <p>&nbsp;</p> <p>The following are the basic things that you need to know about network ACLs:</p> <div class="itemizedlist"> <p style="padding-left: 30px;">&nbsp;- Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.</p> <p style="padding-left: 30px;">&nbsp;- You can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</p> <p style="padding-left: 30px;">&nbsp;- Each subnet in your VPC must be associated with a network ACL. If you don\'t explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</p> <p style="padding-left: 30px;">&nbsp;- You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</p> <p style="padding-left: 30px;">&nbsp;- A network ACL contains a numbered list of rules that we evaluate in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL. The highest number that you can use for a rule is 32766. We recommend that you start by creating rules in increments (for example, increments of 10 or 100) so that you can insert new rules where you need to later on.</p> <p style="padding-left: 30px;">&nbsp;- A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic.</p> <p style="padding-left: 30px;">&nbsp;- Network ACLs are stateless; responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).</p> </div> <p>&nbsp;</p> <p>The scenario clearly states that it requires the <strong>quickest</strong> way to fix the security vulnerability. In this situation, you can manually block the offending IP addresses using Network ACLs since the IT Security team already identified the list of offending IP addresses. Alternatively, you can set up a bastion host however, this option entails additional time to properly set up as you have to configure the security configurations of your bastion host. Hence, Option 3 is the best answer since it can quickly resolve the issue by blocking the IP addresses using Network ACL.</p> <p>Option 1 is incorrect because if you deploy the EC2 instance in the private subnet without public or EIP address, it would not be accessible over the Internet, even to you.&nbsp;</p> <p>Option 2 is incorrect because removing the Internet Gateway will also make your EC2 instance inaccessible to you as it will cut down the connection to the Internet.</p> <p>Option 4 is a valid answer, however, setting up a bastion host is not the <em>quickest</em> way to fix the security vulnerability as opposed to using Network ACLs. This entails additional time to properly set up and, in addition, you still have to worry about doing the proper Security Group and NACL configurations of your bastion host. Moreover, the scenario explicitly says that the IT Security team have already identified the list of offending IP addresses. Remember that you primarily use Network ACL if you want to block an IP address on the subnet level. If you used a bastion host, then you will also need to use a Network ACL to block those offending IP addresses in the first place. Hence, this option is incorrect.&nbsp;</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html</a></p> <p>&nbsp;</p> <p><strong>Check out these AWS Security Group and Network ACL Cheat Sheets:</strong></p> <p><a href="https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p> <p><a href="https://tutorialsdojo.com/security-group-vs-nacl/">https://tutorialsdojo.com/security-group-vs-nacl/</a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567282, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are working for a top IT Consultancy and one of your clients asked you how to properly secure their AWS infrastructure. They have a VPC with two On-Demand EC2 instances with Elastic IP addresses which recently were under SSH brute force attacks over the Internet. Their IT Security team has identified the IP addresses where these attacks originated.\xa0 Which of the following is the quickest way to fix this security vulnerability?', 'id': 10430124, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'relatedLectureIds': '', 'answers': ['The instances are using the wrong AMI.', 'The health check configuration is not properly defined.', 'The wrong instance type was used for the EC2 instance.', 'The wrong subnet was used in your VPC'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are assigned to design a highly available architecture in AWS. You have two target groups with three EC2 instances each, which are added to an Application Load Balancer. In the security group of the EC2 instance, you have verified that the port 80 for HTTP is allowed. However, the instances are still showing out of service from the load balancer. </p><p>What could be the root cause of this issue?</p>', 'explanation': '<p>Since the security group is properly configured, the issue may be caused by a wrong&nbsp;health check configuration in the Target Group.&nbsp;</p> <p>Your Application Load Balancer periodically sends requests to its registered targets to test their status. These tests are called&nbsp;<em>health checks</em>. Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target group with which the target is registered. After your target is registered, it must pass one health check to be considered healthy. After each health check is completed, the load balancer node closes the connection that was established for the health check.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p> <p>&nbsp;</p> <p><strong>Application Load Balancer vs Network Load Balancer vs Classic Load Balancer:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567284, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are assigned to design a highly available architecture in AWS. You have two target groups with three EC2 instances each, which are added to an Application Load Balancer. In the security group of the EC2 instance, you have verified that the port 80 for HTTP is allowed. However, the instances are still showing out of service from the load balancer. What could be the root cause of this issue?', 'id': 10430126, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['Encrypt the EBS volume using the S3 server-side encryption service.', 'Encrypt the EBS volume using the S3 client-side encryption service.', 'Create the EBS Volume first and attach it to the instance then enable encryption.', 'In IAM, create a new policy that disallows any read and write access to the EBS volume.', 'Create an encrypted EBS Volume by ticking the encryption tickbox and attach it to the instance.'], 'feedbacks': ['', '', '', '', ''], 'question': 'You have an existing On-demand EC2 instance and you are planning to create a new EBS volume that will be attached to this instance. The data that will be stored are confidential medical records so you have to make sure that the data is protected. <br><br>How can you secure the data at rest of the new EBS volume that you will create?', 'explanation': '<p>You can secure the data at rest by creating an encrypted EBS Volume. The main difference between creating an unencrypted to an encrypted EBS Volume is just one tickbox called "<code>Encryption</code>". You can create an encrypted EBS Volume by ticking the encryption tickbox and attach it to the EC2 instance.</p> <p>Amazon EBS encryption offers a simple encryption solution for your EBS volumes without the need to build, maintain, and secure your own key management infrastructure. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:</p> <ul> <li>-Data at rest inside the volume</li> <li>-All data moving between the volume and the instance</li> <li>-All snapshots created from the volume</li> <li>-All volumes created from those snapshots</li> </ul> <p>&nbsp;</p> <p>Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage.</p> <p>Options 1 and 2 are incorrect since you can\'t use the server and client-side encryption of S3 in EBS Volumes.</p> <p>Option 3 is incorrect due to the fact that there is no direct way to encrypt an already existing unencrypted volume that you have created. You can only do it by enabling encryption during the time when you created the EBS Volume.</p> <p>Option 4 is incorrect because you cannot encrypt an EBS Volume using IAM.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['e'], 'original_assessment_id': 2567286, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You have an existing On-demand EC2 instance and you are planning to create a new EBS volume that will be attached to this instance. The data that will be stored are confidential medical records so you have to make sure that the data is protected. How can you secure the data at rest of the new EBS volume that you will create?', 'id': 10430128, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'relatedLectureIds': '', 'answers': ['Auto Scaling should be disabled for the load balancer to route the traffic to multiple Availability Zones.', 'By default, you are not allowed to use a load balancer with multiple Availability Zones. You have to send a request form to AWS in order for this to work.', 'The Availability Zone is not properly added to the load balancer which is why it is not receiving any traffic.', 'The Classic Load Balancer is down'], 'feedbacks': ['', '', '', ''], 'question': 'You are managing a global news website which has a very high traffic. To improve the performance, you redesigned the application architecture to use a Classic Load Balancer with an Auto Scaling Group in multiple Availability Zones. However, you noticed that one of the Availability Zones is not receiving any traffic. What is the root cause of this issue?', 'explanation': '<p>In this scenario, one of the Availability Zones is not properly added to the Elastic load balancer. Hence, that&nbsp;Availability Zone is not receiving any traffic.</p> <p>You can set up your load balancer in EC2-Classic to distribute incoming requests across EC2 instances in a single Availability Zone or multiple Availability Zones. First, launch EC2 instances in all the Availability Zones that you plan to use. Next, register these instances with your load balancer. Finally, add the Availability Zones to your load balancer. After you add an Availability Zone, the load balancer starts routing requests to the registered instances in that Availability Zone. Note that you can modify the Availability Zones for your load balancer at any time.</p> <p>By default, the load balancer routes requests evenly across its Availability Zones. To route requests evenly across the registered instances in the Availability Zones, enable cross-zone load balancing.&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-az.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-az.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567288, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are managing a global news website which has a very high traffic. To improve the performance, you redesigned the application architecture to use a Classic Load Balancer with an Auto Scaling Group in multiple Availability Zones. However, you noticed that one of the Availability Zones is not receiving any traffic. What is the root cause of this issue?', 'id': 10430130, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Database Migration Service', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Configure a Launch Template that automatically converts the source schema and code to match that of the target database. Then, use the AWS Database Migration Service to migrate data from the source database to the target database.\xa0 </p>', '<p>First, use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database.\xa0 </p>', '<p>Use Amazon Neptune to convert the source schema and code to match that of the target database in RDS. Use the AWS Batch to effectively migrate the data from the source database to the target database in a batch process.\xa0 </p>', '<p>Heterogeneous database migration is not supported in AWS. You have to transform your database first to PostgreSQL and then migrate it to RDS.\xa0 </p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A leading media company has recently adopted a hybrid cloud architecture which requires them to migrate their application servers and databases in AWS. One of their applications requires a heterogeneous database migration in which you need to transform your on-premises Oracle database to PostgreSQL in AWS. This entails a schema and code transformation before the proper data migration starts.\xa0 \xa0</p><p>Which of the following options is the most suitable approach to migrate the database in AWS?\xa0 </p>', 'explanation': '<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from most widely used commercial and open-source databases.</p> <p>AWS Database Migration Service can migrate your data to and from most of the widely used commercial and open source databases. It supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle to Amazon Aurora. Migrations can be from on-premises databases to Amazon RDS or Amazon EC2, databases running on EC2 to RDS, or vice versa, as well as from one RDS database to another RDS database. It can also move data between SQL, NoSQL, and text based targets.</p> <p>In heterogeneous database migrations the source and target databases engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts. That makes heterogeneous migrations a two step process. First use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database. All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located in your own premises outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS.</p> <p>Option 1 is incorrect because Launch templates are primarily used in EC2 to enable you to store launch parameters so that you do not have to specify them every time you launch an instance.</p> <p>Option 3 is incorrect because Amazon Neptune is a fully-managed graph database service and not a suitable service to use to convert the source schema. AWS Batch is not a database migration service and hence, it is not suitable to be used in this scenario. You should use the AWS Schema Conversion Tool and AWS Database Migration Service instead.</p> <p>Option 4 is incorrect because heterogeneous database migration is supported in AWS using the Database Migration Service.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/dms/ ">https://aws.amazon.com/dms/ </a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html ">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html </a></p> <p><a href="https://aws.amazon.com/batch/">https://aws.amazon.com/batch/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Database Migration Service Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-database-migration-service/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-database-migration-service/</span></a></p> <p>&nbsp;</p> <p><strong>Here is a case study on AWS Database Migration Service:</strong></p> <p><iframe src="https://www.youtube.com/embed/11IHvxjy4hw" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567290, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A leading media company has recently adopted a hybrid cloud architecture which requires them to migrate their application servers and databases in AWS. One of their applications requires a heterogeneous database migration in which you need to transform your on-premises Oracle database to PostgreSQL in AWS. This entails a schema and code transformation before the proper data migration starts.\xa0 \xa0Which of the following options is the most suitable approach to migrate the database in AWS?', 'id': 10430132, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['AWS Config', 'AWS CodePipeline', '<p>Run Command</p>', 'EC2Config'], 'feedbacks': ['', '', '', ''], 'question': '<p>You recently launched a fleet of on-demand EC2 instances to host a massively multiplayer online role-playing\xa0game\xa0(MMORPG) server\xa0in your VPC. The EC2 instances are configured with Auto Scaling and\xa0AWS Systems Manager.\xa0What can you use to configure your EC2 instances without having to establish a RDP or SSH connection to each instance?</p>', 'explanation': '<p>You can use Run Command from the console to configure instances without having to login to each instance.</p> <p>AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. A&nbsp;<em>managed instance</em>&nbsp;is any Amazon EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html">https://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html</a></p> <p>&nbsp;&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567292, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You recently launched a fleet of on-demand EC2 instances to host a massively multiplayer online role-playing\xa0game\xa0(MMORPG) server\xa0in your VPC. The EC2 instances are configured with Auto Scaling and\xa0AWS Systems Manager.\xa0What can you use to configure your EC2 instances without having to establish a RDP or SSH connection to each instance?', 'id': 10430134, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SQS', 'prompt': {'relatedLectureIds': '', 'answers': ['When the message visibility timeout expires, the message becomes available for processing by other EC2 instances', 'It will remain in the queue and still assigned to same EC2 instances when instances become online within the visibility timeout.', 'The message is deleted and becomes duplicated in the SQS when the EC2 instance comes online.'], 'feedbacks': ['', '', ''], 'question': 'A Dedicated EC2 instance retrieves a message from an SQS queue and begins processing the message. After five minutes, the instance crashes. <br><br>What happens to the message?', 'explanation': '<p>When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn\'t automatically delete the message. Because Amazon SQS is a distributed system, there\'s no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it.</p> <p>Immediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a&nbsp;<em>visibility timeout</em>, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html">http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p> <p>&nbsp;&nbsp;&nbsp;&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567294, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A Dedicated EC2 instance retrieves a message from an SQS queue and begins processing the message. After five minutes, the instance crashes. What happens to the message?', 'id': 10430136, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Aurora', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Amazon Aurora flips the canonical name record (CNAME) for your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary.</p>', '<p>Aurora will first attempt to create a new DB Instance in the same Availability Zone as the original instance. If unable to do so, Aurora will attempt to create a new DB Instance in a different Availability Zone.</p>', '<p>Amazon Aurora flips the A record of your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary.</p>', '<p>Aurora will first attempt to create a new DB Instance in a different Availability Zone of the original instance. If unable to do so, Aurora will attempt to create a new DB Instance in the original Availability Zone in which the instance was first launched.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are an IT Consultant for a top investment bank which is in the process of building its new Forex trading platform. To ensure high availability and scalability, you designed the trading platform to use an Elastic Load Balancer in front of an Auto Scaling group of On-Demand EC2 instances across multiple Availability Zones. For its database tier, you chose to use a single Amazon Aurora instance to take advantage of its distributed, fault-tolerant and self-healing storage system.\xa0 </p><p>In the event of system failure on the primary database instance, what happens to Amazon Aurora during the failover?</p>', 'explanation': '<p>Failover is automatically handled by Amazon Aurora so that your applications can resume database operations as quickly as possible without manual administrative intervention.</p> <p>If you have an Amazon Aurora Replica in the same or a different Availability Zone, when failing over, Amazon Aurora flips the canonical name record (CNAME) for your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary. Start-to-finish, failover typically completes within 30 seconds.</p> <p>If you do not have an Amazon Aurora Replica (i.e. single instance), Aurora will first attempt to create a new DB Instance in the same Availability Zone as the original instance. If unable to do so, Aurora will attempt to create a new DB Instance in a different Availability Zone. From start to finish, failover typically completes in under 15 minutes.</p> <p>Hence, the correct answer is Option 2.</p> <p>Options 1 and 3 are incorrect because this will only happen if you are using an Amazon Aurora Replica. In addition, Amazon Aurora flips the canonical name record (CNAME) and not the A record (IP address) of the instance.</p> <p>Option 4 is incorrect because Aurora will first attempt to create a new DB Instance in the same Availability Zone as the original instance. If unable to do so, Aurora will attempt to create a new DB Instance in a different Availability Zone and not the other way around.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://aws.amazon.com/rds/aurora/faqs/">https://aws.amazon.com/rds/aurora/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Aurora Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-aurora/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-aurora/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567358, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are an IT Consultant for a top investment bank which is in the process of building its new Forex trading platform. To ensure high availability and scalability, you designed the trading platform to use an Elastic Load Balancer in front of an Auto Scaling group of On-Demand EC2 instances across multiple Availability Zones. For its database tier, you chose to use a single Amazon Aurora instance to take advantage of its distributed, fault-tolerant and self-healing storage system.\xa0 In the event of system failure on the primary database instance, what happens to Amazon Aurora during the failover?', 'id': 10430196, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'relatedLectureIds': '', 'answers': ['HTTP or HTTPS health check', 'ICMP health check', 'FTP health check', 'TCP health check'], 'feedbacks': ['', '', '', ''], 'question': 'You have a web application hosted on a fleet of EC2 instances located in two Availability Zones that are all placed behind an Application Load Balancer. As a Solutions Architect, you have to add a health check configuration to ensure your application is highly-available.<br><br>Which health checks will you implement?', 'explanation': '<p>The type of ELB that is mentioned here is an Application Elastic Load Balancer. This is used if you want a flexible feature set for your web applications with HTTP and HTTPS traffic. Conversely, it only allows 2 types of health check: HTTP and HTTPS.</p> <p>Options 2 and 3 are incorrect as FTP and ICMP health checks are not supported.</p> <p>Option 4 is incorrect. A TCP health check is only offered in Network Load Balancer, which is another type of ELB. It is used if you need ultra-high performance and static IP addresses for your application.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p> <p>&nbsp;</p> <p><strong>EC2 Instance Health Check vs ELB Health Check vs Auto Scaling and Custom Health Check:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-ec2-instance-health-check-vs-elb-health-check-vs-auto-scaling-and-custom-health-check-2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-ec2-instance-health-check-vs-elb-health-check-vs-auto-scaling-and-custom-health-check-2/</span></a></p> <p>&nbsp;</p> <p><strong>Application Load Balancer vs Network Load Balancer vs Classic Load Balancer:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567296, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You have a web application hosted on a fleet of EC2 instances located in two Availability Zones that are all placed behind an Application Load Balancer. As a Solutions Architect, you have to add a health check configuration to ensure your application is highly-available.Which health checks will you implement?', 'id': 10430138, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'DynamoDB', 'prompt': {'relatedLectureIds': '', 'answers': ['DynamoDB', '<p>Amazon Neptune</p>', '<p>Amazon Aurora</p>', 'SimpleDB'], 'feedbacks': ['', '', '', ''], 'question': '<p>In a startup company you are working for, you are asked to design a web application that requires a NoSQL database that has no limit on the request capacity or storage size for a given table. The startup is still new in the market and it has very limited human resources who can take care of the database infrastructure.\xa0 \xa0</p><p>Which is the most suitable service that you can implement that provides a fully managed, scalable and highly available NoSQL service?</p>', 'explanation': '<p>The term "fully managed" means that Amazon will manage the underlying infrastructure of the service hence, you don\'t need an additional human resource to support or maintain the service. Therefore,&nbsp;Amazon DynamoDB is the right answer. Remember that Amazon RDS is a managed service but not "fully managed" as you still have the option to maintain and configure the underlying server of the database.</p> <p>Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed cloud database and supports both document and key-value store models. Its flexible data model, reliable performance, and automatic scaling of throughput capacity make it a great fit for mobile, web, gaming, ad tech, IoT, and many other applications.&nbsp;</p> <p>Option 2 is incorrect because Amazon Neptune is primarily used as a graph database.</p> <p>Option 3 is incorrect because Amazon Aurora is a relational database and not a NoSQL database.</p> <p>Option&nbsp;4 is incorrect because although SimpleDB is also a highly available and scalable NoSQL database, it has a limit on the request capacity or storage size for a given table, unlike DynamoDB.&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/dynamodb/">https://aws.amazon.com/dynamodb/</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-dynamodb/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567298, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'In a startup company you are working for, you are asked to design a web application that requires a NoSQL database that has no limit on the request capacity or storage size for a given table. The startup is still new in the market and it has very limited human resources who can take care of the database infrastructure.\xa0 \xa0Which is the most suitable service that you can implement that provides a fully managed, scalable and highly available NoSQL service?', 'id': 10430140, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['An Internet Gateway (IGW) attached to the VPC.', 'A Private IP address attached to the EC2 instance.', 'A Private Elastic IP address attached to the EC2 instance.', 'A VPN Peering connection.', 'A route entry to the Internet gateway in the Route table of the VPC.', 'A Public IP address attached to the EC2 instance.'], 'feedbacks': ['', '', '', '', '', ''], 'question': '<p>A web application is deployed in an On-Demand EC2 instance in your VPC. There is an issue with the application which requires you to connect to it via an SSH connection. Which of the following is needed in order to access an EC2 instance from the Internet? (Choose 3)</p>', 'explanation': '<p>Options 1, 5, and 6 are the correct answers. In order for you to access your EC2 instance from the Internet, you need to have:&nbsp;</p> <ol> <li>An Internet Gateway (IGW) attached to the VPC.</li> <li>A route entry to the Internet gateway in the Route table of the VPC.</li> <li>A Public IP address attached to the EC2 instance.</li> </ol> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/userguide/images/scenario-1-ipv6-diagram.png" alt="" width="508" height="401" /></p> <p>Option 2 is incorrect as you only use a Private IP inside your VPC.</p> <p>Option 3 is incorrect as an Elastic IP Address is a public IPv4 address, not private. It is reachable from the Internet and is designed for dynamic cloud computing.</p> <p>Option 4 is incorrect as you only use VPC Peering to connect two VPCs.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['a', 'e', 'f'], 'original_assessment_id': 2567360, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A web application is deployed in an On-Demand EC2 instance in your VPC. There is an issue with the application which requires you to connect to it via an SSH connection. Which of the following is needed in order to access an EC2 instance from the Internet? (Choose 3)', 'id': 10430198, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'CloudHSM', 'prompt': {'relatedLectureIds': '', 'answers': ['Restore a snapshot of the Hardware Security Module.', 'Contact AWS Support and they will provide you a copy of the keys.', 'The keys are lost permanently if you did not have a copy.', 'Use the Amazon CLI to get a copy of the keys.'], 'feedbacks': ['', '', '', ''], 'question': '<p>A news company has been using a Hardware Security Module (CloudHSM) for secure key storage. It is only used for generating keys for their On-demand EC2 instances. After a new support staff attempted to log in as the administrator three times using an invalid password, the Hardware Security Module has been zeroized which means that the encryption keys on it have been wiped. Unfortunately, you did not have a copy of the keys stored anywhere else.\xa0 \xa0</p><p>How can you obtain a new copy of the keys that you have stored on Hardware Security Module?</p>', 'explanation': '<p>Intentionally enter an incorrect administrator password three times in a row. Attempting to log in as the administrator more than twice with the wrong password zeroizes your HSM appliance.&nbsp;When an HSM is zeroized, all keys, certificates, and other data on the HSM is destroyed. You can use your cluster\'s security group to prevent an unauthenticated user from zeroizing your HSM.</p> <p>Amazon does not have access to your keys nor credentials of your&nbsp;Hardware Security Module (HSM) and therefore has no way to recover your keys if you lose your credentials.&nbsp;Amazon strongly recommends that you use two or more HSMs in separate Availability Zones in any production CloudHSM Cluster to avoid loss of cryptographic keys.</p> <p>Refer to the CloudHSM FAQs for reference:&nbsp;</p> <p><strong>Q: Could I lose my keys if a single HSM instance fails?</strong></p> <p>Yes. It is possible to lose keys that were created since the most recent daily backup if the CloudHSM cluster that you are using fails and you are not using two or more HSMs. Amazon strongly recommends that you use two or more HSMs, in separate Availability Zones, in any production CloudHSM Cluster to avoid loss of cryptographic keys.</p> <p><strong>Q: Can Amazon recover my keys if I lose my credentials to my HSM?</strong></p> <p>No. Amazon does not have access to your keys or credentials and therefore has no way to recover your keys if you lose your credentials.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/stop-cloudhsm/">https://aws.amazon.com/premiumsupport/knowledge-center/stop-cloudhsm/</a></p> <p><a href="https://aws.amazon.com/cloudhsm/faqs/">https://aws.amazon.com/cloudhsm/faqs/</a></p> <p><a href="https://d1.awsstatic.com/whitepapers/Security/security-of-aws-cloudhsm-backups.pdf">https://d1.awsstatic.com/whitepapers/Security/security-of-aws-cloudhsm-backups.pdf</a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567300, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A news company has been using a Hardware Security Module (CloudHSM) for secure key storage. It is only used for generating keys for their On-demand EC2 instances. After a new support staff attempted to log in as the administrator three times using an invalid password, the Hardware Security Module has been zeroized which means that the encryption keys on it have been wiped. Unfortunately, you did not have a copy of the keys stored anywhere else.\xa0 \xa0How can you obtain a new copy of the keys that you have stored on Hardware Security Module?', 'id': 10430142, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Cloudformation', 'prompt': {'relatedLectureIds': '', 'answers': ['Resources', 'Parameters', 'Outputs', 'Mappings'], 'feedbacks': ['', '', '', ''], 'question': 'You created a new CloudFormation template that creates 4 EC2 instances and are connected to one Elastic Load Balancer (ELB). Which section of the template should you configure to get the Domain Name Server hostname of the ELB upon the creation of the AWS stack?', 'explanation': '<div> <p><strong>Outputs</strong>&nbsp;is an optional section of the CloudFormation template that describes the values that are returned whenever you view your stack\'s properties.&nbsp;</p> <p>&nbsp;</p> </div> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p> <p><a href="https://aws.amazon.com/cloudformation/" target="_blank" rel="noopener">https://aws.amazon.com/cloudformation/</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudFormation Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567302, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You created a new CloudFormation template that creates 4 EC2 instances and are connected to one Elastic Load Balancer (ELB). Which section of the template should you configure to get the Domain Name Server hostname of the ELB upon the creation of the AWS stack?', 'id': 10430144, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['Use Amazon S3 Standard', 'Use Amazon S3 Standard - Infrequent Access', '<p>Use Amazon S3 -Intelligent Tiering</p>', 'Use Amazon Glacier'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as an IT Consultant for a large financial firm. They have a requirement to store irreproducible financial documents using Amazon S3. For their quarterly reporting, the files are required to be retrieved after a period of 3 months. There will be some occasions when a surprise audit will be held, which requires access to the archived data that they need to present immediately. <br><br>What will you do to satisfy this requirement in a cost-effective way?</p>', 'explanation': '<p>In this scenario, the requirement is to have a storage option that is&nbsp;cost-effective and has the ability to access or retrieve the archived data&nbsp;immediately.&nbsp;The cost-effective options are Amazon Glacier and Amazon S3 Standard- Infrequent Access (Standard - IA). However, the former option is not designed for rapid retrieval of data which is required for the surprise audit. Hence, option 4 is wrong and the best answer is option 2:&nbsp;Standard - IA.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonS3/latest/dev/images/ObjectStorageClass.png" alt="" width="578" height="181" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because the standard storage class is not cost-efficient in this scenario. It costs more than Glacier and&nbsp;S3 Standard - Infrequent Access.</p> <p>Option 3 is incorrect because the Intelligent Tiering storage class entails an additional fee for monitoring and automation of each object in your S3 bucket, vs. the Standard storage class and S3 Standard - Infrequent Access.</p> <p>Amazon S3 Standard - Infrequent Access is an Amazon S3 storage class for data that is accessed less frequently, but requires rapid access when needed. Standard - IA offers the high durability, throughput, and low latency of Amazon S3 Standard, with a low per GB storage price and per GB retrieval fee.</p> <p>This combination of low cost and high performance makes Standard - IA ideal for long-term storage, backups, and as a data store for disaster recovery. The Standard - IA storage class is set at the object level and can exist in the same bucket as Standard, allowing you to use lifecycle policies to automatically transition objects between storage classes without any application changes.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/s3/storage-classes/">https://aws.amazon.com/s3/storage-classes/</a></p> <p><a href="https://aws.amazon.com/s3/faqs/">https://aws.amazon.com/s3/faqs/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567304, '_class': 'assessment', 'updated': '2019-06-22T05:35:51Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are working as an IT Consultant for a large financial firm. They have a requirement to store irreproducible financial documents using Amazon S3. For their quarterly reporting, the files are required to be retrieved after a period of 3 months. There will be some occasions when a surprise audit will be held, which requires access to the archived data that they need to present immediately. What will you do to satisfy this requirement in a cost-effective way?', 'id': 10430146, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Security', 'prompt': {'relatedLectureIds': '', 'answers': ['The outbound security group needs to be modified to allow outbound traffic.', 'The outbound network ACL needs to be modified to allow outbound traffic.', 'No action needed. It can already be accessed from any IP address using SSH.', 'Both the outbound security group and outbound network ACL need to be modified to allow outbound traffic.'], 'feedbacks': ['', '', '', ''], 'question': '<p>An On-Demand EC2 instance is launched into a VPC subnet with the Network ACL configured to allow all inbound traffic and deny all outbound traffic.\xa0The instance’s security group has an inbound rule to allow SSH from any IP address and does not have any outbound rules.\xa0<br></p><p>In this scenario, what are the changes needed to allow SSH connection to the instance?</p>', 'explanation': '<p>In order for you to establish an SSH connection from your home computer to your EC2 instance, you need to do the following:</p> <ul> <li>On the Security Group, add an Inbound Rule to allow SSH traffic to your EC2 instance.&nbsp;</li> <li>On the NACL, add both an Inbound and Outbound Rule to allow SSH traffic to your EC2 instance.&nbsp;</li> </ul> <p>&nbsp;</p> <p>The reason why you have to add both Inbound and Outbound SSH rule is due to the fact that Network ACLs are stateless which means that responses to allow inbound traffic are subject to the rules for outbound traffic (and vice versa). In other words, if you only enabled an Inbound rule in NACL, the traffic can only go in but the SSH response will not go out since there is no Outbound rule.</p> <p>Security groups are stateful&nbsp;which means that if an incoming request is granted, then the outgoing traffic will be automatically granted as well,&nbsp;regardless of the outbound rules.</p> <p>&nbsp;</p> <p>References:&nbsp;</p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html</a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567306, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'An On-Demand EC2 instance is launched into a VPC subnet with the Network ACL configured to allow all inbound traffic and deny all outbound traffic.\xa0The instance’s security group has an inbound rule to allow SSH from any IP address and does not have any outbound rules.\xa0In this scenario, what are the changes needed to allow SSH connection to the instance?', 'id': 10430148, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SQS', 'prompt': {'relatedLectureIds': '', 'answers': ['Component Timeout', 'Visibility Timeout', 'Processing Timeout', 'Receiving Timeout'], 'feedbacks': ['', '', '', ''], 'question': '<p>An investment bank has a distributed batch processing application which is hosted in an Auto Scaling group of Spot EC2 instances with an SQS queue. You configured your components to use client-side buffering so that the calls made from the client will be buffered first and then sent as a batch request to SQS. What is a period of time during which the SQS queue prevents other consuming components from receiving and processing a message?</p>', 'explanation': '<p>The visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a message.&nbsp;</p> <p>When a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn\'t automatically delete the message. Because Amazon SQS is a distributed system, there\'s no guarantee that the consumer actually receives the message (for example, due to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after receiving and processing it.</p> <p>Immediately after the message is received, it remains in the queue. To prevent other consumers from processing the message again, Amazon SQS sets a&nbsp;<strong><em>visibility timeout</em></strong>, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/sqs/faqs/">https://aws.amazon.com/sqs/faqs/</a></p> <p><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567356, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'An investment bank has a distributed batch processing application which is hosted in an Auto Scaling group of Spot EC2 instances with an SQS queue. You configured your components to use client-side buffering so that the calls made from the client will be buffered first and then sent as a batch request to SQS. What is a period of time during which the SQS queue prevents other consuming components from receiving and processing a message?', 'id': 10430194, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['In the new Region, create a new IAM role and associated policies then assign it to the new instance.', 'Assign the existing IAM role to instances in the new region.', 'Duplicate the IAM role and associated policies to the new region and attach it to the instances.', 'Create an Amazon Machine Image (AMI) of the instance and copy it to the new region.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are consulted by a multimedia company that needs to deploy web services to an AWS region which they have never used before. The company currently has an IAM role for their Amazon EC2 instance which permits the instance to access Amazon DynamoDB. They want their EC2 instances in the new region to have the exact same privileges.\xa0 \xa0</p><p>What should you do to accomplish this?</p>', 'explanation': '<p>In this scenario, the company has an existing IAM role hence you don&rsquo;t need to create a new one. IAM roles are global service that are available to all regions hence, all you have to do is assign the existing IAM role to the instance in the new region.&nbsp;</p> <p>Option 1 is incorrect because you don\'t need to create another IAM role - there is already an existing one.</p> <p>Option 3 is incorrect as you don\'t need duplicate IAM roles&nbsp;for each region. One IAM role suffices for the instances on two regions.</p> <p>Option 4 is incorrect because creating an AMI image does not affect the IAM role of the instance.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567308, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are consulted by a multimedia company that needs to deploy web services to an AWS region which they have never used before. The company currently has an IAM role for their Amazon EC2 instance which permits the instance to access Amazon DynamoDB. They want their EC2 instances in the new region to have the exact same privileges.\xa0 \xa0What should you do to accomplish this?', 'id': 10430150, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Storage Gateway', 'prompt': {'relatedLectureIds': '', 'answers': ['Use a fleet of EC2 instance with EBS volumes to store the commonly used data.', 'Use both Elasticache and S3 for frequently accessed data.', 'Use the Amazon Storage Gateway -  Cached Volumes.', 'Use Amazon Glacier.'], 'feedbacks': ['', '', '', ''], 'question': '<p>A financial company wants to store their data in Amazon S3 but at the same time, they want to store their frequently accessed data locally on their on-premises server. This is due to the fact that they do not have the option to extend their on-premises storage, which is why they are looking for a durable and scalable storage service to use in AWS. <br><br>What is the best solution for this scenario?</p>', 'explanation': '<p>By using&nbsp;Cached volumes, you store your data in Amazon Simple Storage Service (Amazon S3) and retain a copy of frequently accessed data subsets locally in your on-premises network. Cached volumes offer substantial cost savings on primary storage and minimize the need to scale your storage on-premises. You also retain low-latency access to your frequently accessed data. This is the best solution for this scenario.&nbsp;</p> <p>Option 1 is incorrect because an EC2 instance is not a storage service and it does not provide the required durability and scalability.</p> <p>Option 2 is incorrect as storing frequently accessed data on both&nbsp;Elasticache and S3 is not efficient. Moreover, the question explicitly said that the frequently accessed data should be stored locally on their on-premises server and not on AWS.</p> <p>Option 4 is incorrect as Amazon Glacier is mainly used for data archiving.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/storagegateway/faqs/ ">https://aws.amazon.com/storagegateway/faqs/ </a></p> <p>&nbsp;&nbsp;&nbsp;</p> <p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-storage-gateway/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-storage-gateway/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567310, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A financial company wants to store their data in Amazon S3 but at the same time, they want to store their frequently accessed data locally on their on-premises server. This is due to the fact that they do not have the option to extend their on-premises storage, which is why they are looking for a durable and scalable storage service to use in AWS. What is the best solution for this scenario?', 'id': 10430152, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudTrail', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use CloudTrail and configure the destination Amazon Glacier archive to use Server-Side Encryption (SSE).</p>', '<p>Use CloudTrail and configure the destination S3 bucket to use Server-Side Encryption (SSE).</p>', '<p>Use CloudTrail and ensure that the Server-Side Encryption (SSE) option is enabled for the trail in the CloudTrail console.</p>', '<p>Use CloudTrail with its default settings</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a leading technology company where you are instructed to troubleshoot the operational issues of your cloud architecture by logging the AWS API call history of your AWS resources. You need to quickly identify the most recent changes made to resources in your environment, including creation, modification, and deletion of AWS resources. One of the requirements is that the generated log files should be encrypted to avoid any security issues.\xa0 \xa0</p><p>Which of the following is the most suitable approach to implement the encryption?</p>', 'explanation': '<p>By default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE). You can also choose to encrypt your log files with an AWS Key Management Service (AWS KMS) key. You can store your log files in your bucket for as long as you want. You can also define Amazon S3 lifecycle rules to archive or delete log files automatically. If you want notifications about log file delivery and validation, you can set up Amazon SNS notifications.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://media.amazonwebservices.com/blog/2014/cloudtrail_flow_9.png" alt="" width="750" height="711" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because CloudTrail stores the log files to S3 and not in Glacier. Take note that by default,&nbsp;CloudTrail event log files are already encrypted using Amazon S3 server-side encryption (SSE).</p> <p>Option 2 is incorrect because&nbsp;CloudTrail event log files are already encrypted using the Amazon S3 server-side encryption (SSE) which is why you do not have to do this anymore.</p> <p>Option 3 is incorrect because there is no available Server-Side Encryption (SSE) option in the CloudTrail console.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html</a></p> <p><a href="https://aws.amazon.com/blogs/aws/category/cloud-trail/">https://aws.amazon.com/blogs/aws/category/cloud-trail/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudTrail Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567328, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are working as a Solutions Architect for a leading technology company where you are instructed to troubleshoot the operational issues of your cloud architecture by logging the AWS API call history of your AWS resources. You need to quickly identify the most recent changes made to resources in your environment, including creation, modification, and deletion of AWS resources. One of the requirements is that the generated log files should be encrypted to avoid any security issues.\xa0 \xa0Which of the following is the most suitable approach to implement the encryption?', 'id': 10430170, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['The volume of the instance was not big enough to handle all of the processing data.', 'The EC2 instance was using EBS backed root volumes, which are ephemeral and only live for the life of the instance.', 'The EC2 instance was using instance store volumes, which are ephemeral and only live for the life of the instance.', 'The instance was hit by a virus that wipes out all data.'], 'feedbacks': ['', '', '', ''], 'question': 'You are running an EC2 instance store-based instance. You shut it down and then start the instance. You noticed that the data which you have saved earlier is no longer available. <br><br>What might be the cause of this?', 'explanation': '<p>An&nbsp;<em>instance store</em>&nbsp;provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p> <p>An instance store consists of one or more instance store volumes exposed as block devices. The size of an instance store as well as the number of devices available varies by instance type. While an instance store is dedicated to a particular instance, the disk subsystem is shared among instances on a host computer.</p> <p>The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists. However, data in the instance store is lost under the following circumstances:</p> <ul> <li>The underlying disk drive fails</li> <li>The instance stops</li> <li>The instance terminates</li> </ul> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p> <p>&nbsp;&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567312, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are running an EC2 instance store-based instance. You shut it down and then start the instance. You noticed that the data which you have saved earlier is no longer available. What might be the cause of this?', 'id': 10430154, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Amazon EBS Provisioned IOPS SSD</p>', '<p>Amazon EBS Throughput Optimized HDD</p>', '<p>Amazon EBS General Purpose SSD</p>', '<p>Amazon EBS Cold HDD</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are planning to migrate a MySQL database from your on-premises data center to your AWS Cloud. This database will be used by a legacy batch application which has steady-state workloads in the morning but has its peak load at night for the end-of-day processing. You need to choose an EBS volume which can handle a maximum of 450 GB of data and can also be used as the system boot volume for your EC2 instance.\xa0 </p><p>Which of the following is the most cost-effective storage type to use in this scenario?</p>', 'explanation': '<p>In this scenario, a&nbsp;legacy batch application which has steady-state workloads requires a <strong><em>relational MySQL database</em></strong>. The EBS volume that you should use has to handle a maximum of 450 GB of data and can also be used&nbsp;as the system <strong><em>boot volume</em> </strong>for your EC2 instance. Since HDD volumes cannot be used as a bootable volume, we can narrow down our options by selecting SSD volumes. In addition, SSD volumes are more suitable for transactional database workloads, as shown in the table below:</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_22-34-15-d1fd30e8eaa8701ddd964e5878e78242.png" alt="" width="750" height="562" /></p> <p>&nbsp;</p> <p>General Purpose SSD (<code class="code">gp2</code>) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 10,000 IOPS (at 3,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs&nbsp;<code class="code">gp2</code>&nbsp;volumes to deliver the provisioned performance 99% of the time. A&nbsp;<code class="code">gp2</code>&nbsp;volume can range in size from 1 GiB to 16 TiB.</p> <p>Option 1 is incorrect because Amazon EBS Provisioned IOPS SSD is not the most cost-effective EBS type and is primarily used for critical business applications that require sustained IOPS performance.</p> <p>Option 2 is incorrect because Amazon EBS Throughput Optimized HDD is primarily used for frequently accessed, throughput-intensive workloads. Although it is a low-cost HDD volume, it cannot be used as a system boot volume.</p> <p>Option 4 is incorrect because although Amazon EBS Cold HDD provides lower cost HDD volume compared to General Purpose SSD, it cannot be used as a system boot volume.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_gp2">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_gp2</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567314, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are planning to migrate a MySQL database from your on-premises data center to your AWS Cloud. This database will be used by a legacy batch application which has steady-state workloads in the morning but has its peak load at night for the end-of-day processing. You need to choose an EBS volume which can handle a maximum of 450 GB of data and can also be used as the system boot volume for your EC2 instance.\xa0 Which of the following is the most cost-effective storage type to use in this scenario?', 'id': 10430156, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Directory Service', 'prompt': {'relatedLectureIds': '', 'answers': ['AWS Directory Services, VPN connection, and ClassicLink', 'AWS Directory Services, VPN connection, and Amazon Workspaces', 'AWS Directory Services, VPN connection, and AWS Identity and Access Management', 'AWS Directory Services, VPN connection, and Amazon S3'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are a working as a Solutions Architect for a fast-growing startup which just started operations during the past 3 months. They currently have an on-premises Active Directory and 10 computers. To save costs in procuring physical workstations, they decided to deploy virtual desktops for their new employees in a virtual private cloud in AWS. The new cloud infrastructure should leverage on the existing security controls in AWS but can still communicate with their on-premises network.\xa0 </p><p>Which set of AWS services will you use to meet these requirements?</p>', 'explanation': '<p>For this scenario, the best choice is Option 2: AWS Directory Services, VPN connection, and Amazon Workspaces. First, you need a&nbsp;VPN connection to connect the VPC and your on-premises network. Second, you need AWS Directory Services to integrate with your on-premises Active Directory and lastly, you need to use Amazon Workspace to create the needed virtual desktops in your VPC.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/directoryservice/">https://aws.amazon.com/directoryservice/</a></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html</a></p> <p><a href="https://aws.amazon.com/workspaces/">https://aws.amazon.com/workspaces/</a></p> <p>&nbsp;</p> <p><strong>Check out these cheat sheets on AWS Directory Service, Amazon VPC and Amazon WorkSpaces:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-directory-service/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-directory-service/</span></a></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-workspaces/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-workspaces/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567316, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are a working as a Solutions Architect for a fast-growing startup which just started operations during the past 3 months. They currently have an on-premises Active Directory and 10 computers. To save costs in procuring physical workstations, they decided to deploy virtual desktops for their new employees in a virtual private cloud in AWS. The new cloud infrastructure should leverage on the existing security controls in AWS but can still communicate with their on-premises network.\xa0 Which set of AWS services will you use to meet these requirements?', 'id': 10430158, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Auto Scaling', 'prompt': {'relatedLectureIds': '', 'answers': ['You must stop the instance first.', 'You have to ensure that the AMI used to launch the instance still exists.', '<p>You have to ensure that the AMI used to launch the instance no longer exists.</p>', '<p>The instance is launched into one of the Availability Zones defined in your Auto Scaling group.</p>', 'You have to ensure that the instance is in a different Availability Zone as the Auto Scaling group.'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>A loan processing application is hosted in a single On-Demand EC2 instance in your VPC. To improve the scalability of your application, you have to use Auto Scaling to automatically add new EC2 instances to handle a surge of incoming requests. </p><p>Which of the following items should be done in order to add an existing EC2 instance to an Auto Scaling group? (Choose 2)</p>', 'explanation': '<p>Amazon EC2 Auto Scaling provides you with an option to enable automatic scaling for one or more EC2 instances by attaching them to your existing Auto Scaling group. After the instances are attached, they become a part of the Auto Scaling group.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://s3.amazonaws.com/chrisb/concept_diagram.jpg" alt="" width="750" height="409" /></p> <p>&nbsp;</p> <p>The instance that you want to attach must meet the following criteria:</p> <p style="padding-left: 30px;">&nbsp;- The instance is in the&nbsp;<strong><code class="code">running</code></strong>&nbsp;state.</p> <p style="padding-left: 30px;">&nbsp;- The AMI used to launch the instance must still exist.</p> <p style="padding-left: 30px;">&nbsp;- The instance is not a member of another Auto Scaling group.</p> <p style="padding-left: 30px;">&nbsp;- The instance is launched into one of the Availability Zones defined in your Auto Scaling group.</p> <p style="padding-left: 30px;">&nbsp;- If the Auto Scaling group has an attached load balancer, the instance and the load balancer must both be in EC2-Classic or the same VPC. If the Auto Scaling group has an attached target group, the instance and the load balancer must both be in the same VPC.</p> <p>&nbsp;</p> <p>Based on the above criteria, Options 2 and 4 are the correct answers.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/attach-instance-asg.html">http://docs.aws.amazon.com/autoscaling/latest/userguide/attach-instance-asg.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/</span></a></p>'}, 'correct_response': ['b', 'd'], 'original_assessment_id': 2567340, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A loan processing application is hosted in a single On-Demand EC2 instance in your VPC. To improve the scalability of your application, you have to use Auto Scaling to automatically add new EC2 instances to handle a surge of incoming requests. Which of the following items should be done in order to add an existing EC2 instance to an Auto Scaling group? (Choose 2)', 'id': 10430180, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'RDS', 'prompt': {'relatedLectureIds': '', 'answers': ['Security Groups', 'NACL Groups', 'Parameter Groups', 'IAM Roles'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a large financial company. In their enterprise application, they want to apply a group of database-specific settings to their Relational Database Instances.\xa0 \xa0</p><p>Which of the following options can be used to easily apply the settings in one go for all of the Relational database instances?</p>', 'explanation': '<p>You manage your DB engine configuration through the use of parameters in a DB parameter group. DB parameter groups act as a&nbsp;<em>container</em>&nbsp;for engine configuration values that are applied to one or more DB instances.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567318, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are working for a large financial company. In their enterprise application, they want to apply a group of database-specific settings to their Relational Database Instances.\xa0 \xa0Which of the following options can be used to easily apply the settings in one go for all of the Relational database instances?', 'id': 10430160, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Route53', 'prompt': {'relatedLectureIds': '', 'answers': ['Create an A record pointing to the IP address of the load balancer.', 'Create a CNAME record pointing to the load balancer DNS name.', 'Create an alias for CNAME record to the load balancer DNS name.', 'Create an A record aliased to the load balancer DNS name.'], 'feedbacks': ['', '', '', ''], 'question': '<p>A client is hosting their company website on a cluster of web servers that are behind a public-facing load balancer. The client also uses Amazon Route 53 to manage their public DNS.\xa0 \xa0</p><p>How should the client configure the DNS zone apex record to point to the load balancer?</p>', 'explanation': '<p>Route 53\'s DNS implementation connects user requests to infrastructure running inside (and outside) of Amazon Web Services (AWS). For example, if you have multiple web servers running on EC2 instances behind an Elastic Load Balancing load balancer, Route 53 will route all traffic addressed to your website (e.g. <code>www.tutorialsdojo.com</code>) to the load balancer DNS name (e.g. <code>elbtutorialsdojo123.elb.amazonaws.com</code>).</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/govcloud-us/latest/ug-west/images/r53-cf-elb.png" alt="" width="360" height="374" /></p> <p>&nbsp;</p> <p>Additionally, Route 53 supports the alias resource record set, which lets you map your <strong>zone apex (</strong>e.g. <code>tutorialsdojo.com</code>) DNS name to your load balancer DNS name. IP addresses associated with Elastic Load Balancing can change at any time due to scaling or software updates. Route 53 responds to each request for an Alias resource record set with one IP address for the load balancer.</p> <p>Option 1 is incorrect. You should be using an Alias record pointing to the DNS name of the load balancer since the IP address of the load balancer can change at any time.</p> <p>Option 2 and 3 are incorrect because CNAME records cannot be created for your <strong>zone</strong> apex. You should create an alias record at the top node of a DNS namespace which is also known as the&nbsp;<em>zone apex</em>. For example, if you register the DNS name tutorialsdojo.com, the zone apex is tutorialsdojo.com. You can\'t create a CNAME record directly for tutorialsdojo.com, but you can create an alias record for tutorialsdojo.com that routes traffic to <strong>www</strong>.tutorialsdojo.com.</p> <p>&nbsp;</p> <p><strong>References</strong>:</p> <p><a href="http://docs.aws.amazon.com/govcloud-us/latest/UserGuide/setting-up-route53-zoneapex-elb.html">http://docs.aws.amazon.com/govcloud-us/latest/UserGuide/setting-up-route53-zoneapex-elb.html</a></p> <p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567320, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A client is hosting their company website on a cluster of web servers that are behind a public-facing load balancer. The client also uses Amazon Route 53 to manage their public DNS.\xa0 \xa0How should the client configure the DNS zone apex record to point to the load balancer?', 'id': 10430162, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFront', 'prompt': {'relatedLectureIds': '', 'answers': ['Edge Location', 'Bastion Hosts', '<p>Hypervisor</p>', '<p>VPC Endpoint</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You recently launched a news website which is expected to be visited by millions of people around the world. You chose to deploy the website in AWS to take advantage of its extensive range of cloud services and global infrastructure. Aside from AWS Region and Availability Zones, which of the following is part of the AWS Global Infrastructure that is used for content distribution?</p>', 'explanation': '<p>An edge location helps deliver high availability, scalability, and performance of your application for all of your customers from anywhere in the world. This is used by other services such as Lambda and&nbsp;Amazon CloudFront.</p> <p>Amazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. CloudFront delivers your files to end-users using a global network of edge locations.</p> <p>Option 2 is incorrect because a bastion host is not part of the AWS Global Infrastructure. It is just a host computer or a "jump server" used to allow SSH access to your EC2 instances from an outside network.</p> <p>Option 3 is incorrect because a&nbsp;hypervisor&nbsp;is just a computer software, firmware or hardware that creates and runs virtual machines. This technology relates to EC2 instances but it is not part of the AWS Global Infrastructure.</p> <p>Option 4 is incorrect because VPC Endpoint is not part of the AWS Global Infrastructure and is just used to privately connect your VPC to other AWS services and endpoint services.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/cloudfront/">https://aws.amazon.com/cloudfront/</a></p> <p><a href="https://aws.amazon.com/about-aws/global-infrastructure/">https://aws.amazon.com/about-aws/global-infrastructure/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567322, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You recently launched a news website which is expected to be visited by millions of people around the world. You chose to deploy the website in AWS to take advantage of its extensive range of cloud services and global infrastructure. Aside from AWS Region and Availability Zones, which of the following is part of the AWS Global Infrastructure that is used for content distribution?', 'id': 10430164, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>EBS Provisioned IOPS SSD\xa0 </p>', '<p>EBS Throughput Optimized HDD\xa0 </p>', '<p>EBS General Purpose SSD\xa0 </p>', '<p>EBS Cold HDD\xa0 </p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are setting up a cost-effective architecture for a log processing application which has frequently accessed, throughput-intensive workloads. The application should be hosted in an On-Demand EC2 instance in your VPC.\xa0 \xa0</p><p>Which of the following is the most suitable EBS volume type to use in this scenario?\xa0 \xa0</p>', 'explanation': '<p>In the exam, always consider the difference between SSD and HDD as shown on the table below. This will allow you to easily eliminate specific EBS-types in the options which are not SSD or not HDD, depending on whether the question asks for a storage type which has <strong><em>small, random</em></strong> I/O operations or <strong><em>large, sequential</em></strong> I/O operations.</p> <p>Since the scenario has workloads with large, sequential I/O operations, we can narrow down our options by selecting HDD volumes, instead of SDD volumes which are more suitable for small, random I/O operations.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-19_22-34-15-d1fd30e8eaa8701ddd964e5878e78242.png" width="750" />&nbsp;</p> <p>&nbsp;</p> <p>Throughput Optimized HDD (<code class="code">st1</code>) volumes provide low-cost magnetic storage that defines performance in terms of throughput rather than IOPS. This volume type is a good fit for large, sequential workloads such as Amazon EMR, ETL, data warehouses, and log processing. Bootable&nbsp;<code class="code">st1</code>&nbsp;volumes are not supported.</p> <p>Throughput Optimized HDD (<code class="code">st1</code>) volumes, though similar to Cold HDD (<code class="code">sc1</code>) volumes, are designed to support&nbsp;<em>frequently</em>&nbsp;accessed data.&nbsp;&nbsp;</p> <p>Option 1 is incorrect because Amazon EBS Provisioned IOPS SSD is not the most cost-effective EBS type and is primarily used for critical business applications that require sustained IOPS performance.</p> <p>Option 3 is incorrect because although an Amazon EBS General Purpose SSD volume balances price and performance for a wide variety of workloads,&nbsp;it is not suitable for&nbsp;frequently accessed, throughput-intensive workloads.&nbsp;Throughput Optimized HDD is a more suitable option to use than General Purpose SSD.</p> <p>Option 4 is incorrect because although Amazon EBS Cold HDD provides lower cost HDD volume compared to General Purpose SSD, it is much suitable for <strong><em>less</em></strong> frequently accessed workloads.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_st1">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_st1</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567324, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are setting up a cost-effective architecture for a log processing application which has frequently accessed, throughput-intensive workloads. The application should be hosted in an On-Demand EC2 instance in your VPC.\xa0 \xa0Which of the following is the most suitable EBS volume type to use in this scenario?', 'id': 10430166, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Change the web architecture to access the financial data through a Gateway VPC Endpoint.</p>', '<p>Change the web architecture to access the financial data in your S3 bucket through a VPN connection. </p>', '<p>Change the web architecture to access the financial data hosted in your S3 bucket by creating a custom VPC endpoint service.</p>', '<p>Change the web architecture to access the financial data in S3 through an interface VPC endpoint, which is powered by AWS PrivateLink.</p>'], 'feedbacks': ['', '', '', ''], 'question': 'You have a web application running on EC2 instances which processes sensitive financial information. All of the data are stored on an Amazon S3 bucket. The financial information is accessed by users over the Internet. The security team of the company is concerned that the Internet connectivity to Amazon S3 is a security risk. In this scenario, what will you do to resolve this security concern?', 'explanation': '<p>Take note that your VPC lives within a larger AWS network and the services, such as S3, DynamoDB, RDS and many others, are located outside of your VPC, but still within the AWS network. By default, the connection that your VPC uses to connect to your S3 bucket or any other service traverses the public Internet via your&nbsp;Internet Gateway.&nbsp;</p> <p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.&nbsp;</p> <p>There are two types of VPC endpoints:&nbsp;<em>interface endpoints</em>&nbsp;and&nbsp;<em>gateway endpoints</em>. You have to create the type of VPC endpoint required by the supported service.</p> <div> <p>An&nbsp;<a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html">interface endpoint</a>&nbsp;is an elastic network interface with a private IP address that serves as an entry point for traffic destined to a supported service. A&nbsp;<a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html">gateway endpoint</a>&nbsp;is a gateway that is a target for a specified route in your route table, used for traffic destined to a supported AWS service. It is important to note that for Amazon S3 and DynamoDB service, you have to create a gateway endpoint and then use an interface endpoint for other services.</p> <p>&nbsp;</p> </div> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/userguide/images/vpc-endpoint-s3-diagram.png" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect because a VPN connection still goes through the public Internet. You have to use a VPC Endpoint in this scenario and not VPN,&nbsp;to privately connect your VPC to supported AWS services such as S3.</p> <p>Option 3 is incorrect because&nbsp;a "VPC endpoint <strong>service</strong>" is quite different from a "VPC endpoint". With VPC endpoint service, you are the service provider where you can create your own application in your VPC and configure it as an AWS PrivateLink-powered service (referred to as an endpoint service). Other AWS principals can create a connection from their VPC to your endpoint service using an interface VPC endpoint.&nbsp;</p> <p>Option 4 is incorrect because although you are correctly using a VPC Endpoint to satisfy the requirement, you chose a wrong type of VPC Endpoint. Remember that for S3 and DynamoDB service, you have to use a <strong>Gateway</strong> VPC Endpoint and not an <strong>Interface</strong> VPC Endpoint.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html</a></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567326, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You have a web application running on EC2 instances which processes sensitive financial information. All of the data are stored on an Amazon S3 bucket. The financial information is accessed by users over the Internet. The security team of the company is concerned that the Internet connectivity to Amazon S3 is a security risk. In this scenario, what will you do to resolve this security concern?', 'id': 10430168, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['The EBS volume can be used while the snapshot is in progress.', 'The EBS volume cannot be detached or attached to an EC2 instance until the snapshot completes', 'The EBS volume can be used in read-only mode while the snapshot is in progress.', 'The EBS volume cannot be used until the snapshot completes.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You have an On-Demand EC2 instance with an attached EBS volume. There is a scheduled job that creates a snapshot of this EBS volume every midnight at 12 AM when the instance is not used. One night, there has been a production incident where you need to perform a change on both the instance and on the EBS volume at the same time, when the snapshot is currently taking place.\xa0 </p><p>Which of the following scenario is true when it comes to the usage of an EBS volume while the snapshot is in progress?</p>', 'explanation': '<p>Snapshots occur asynchronously; the point-in-time snapshot is created immediately, but the status of the snapshot is&nbsp;<code>pending</code>&nbsp;until the snapshot is complete (when all of the modified blocks have been transferred to Amazon S3), which can take several hours for large initial snapshots or subsequent snapshots where many blocks have changed.</p> <p>While it is completing, an in-progress snapshot is not affected by ongoing reads and writes to the volume hence, you can still use the EBS volume normally.</p> <div>When you create an EBS volume based on a snapshot, the new volume begins as an exact replica of the original volume that was used to create the snapshot. The replicated volume loads data lazily in the background so that you can begin using it immediately. If you access data that hasn\'t been loaded yet, the volume immediately downloads the requested data from Amazon S3, and then continues loading the rest of the volume\'s data in the background.</div> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567330, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You have an On-Demand EC2 instance with an attached EBS volume. There is a scheduled job that creates a snapshot of this EBS volume every midnight at 12 AM when the instance is not used. One night, there has been a production incident where you need to perform a change on both the instance and on the EBS volume at the same time, when the snapshot is currently taking place.\xa0 Which of the following scenario is true when it comes to the usage of an EBS volume while the snapshot is in progress?', 'id': 10430172, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'SQS', 'prompt': {'relatedLectureIds': '', 'answers': ['The batch job application is configured to long polling.', 'Amazon SQS has automatically deleted the messages that have been in a queue for more than the maximum message retention period.', 'The SQS queue is set to short-polling.', 'Missing permissions in SQS.'], 'feedbacks': ['', '', '', ''], 'question': 'The start-up company that you are working for has a batch job application that is currently hosted on an EC2 instance. It is set to process messages from a queue created in SQS with default settings. You configured the application to process the messages once a week. After 2 weeks, you noticed that not all messages are being processed by the application. <br><br>What is the root cause of this issue?', 'explanation': '<p>Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days. Since the queue is configured to the default settings and the batch job application only processes the messages once a week, the messages that are in the queue for more than 4 days are deleted. This is the root cause of the issue.</p> <p>To fix this, you can increase the message retention period to a maximum of 14 days using the&nbsp;<a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html" target="_blank" rel="noopener">SetQueueAttributes</a>&nbsp;action.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/sqs/faqs/" target="_blank" rel="noopener">https://aws.amazon.com/sqs/faqs/</a></p> <p><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-message-lifecycle.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-message-lifecycle.html</a></p> <p>&nbsp;&nbsp;&nbsp;&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567332, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'The start-up company that you are working for has a batch job application that is currently hosted on an EC2 instance. It is set to process messages from a queue created in SQS with default settings. You configured the application to process the messages once a week. After 2 weeks, you noticed that not all messages are being processed by the application. What is the root cause of this issue?', 'id': 10430174, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Change the cooldown period of the Auto Scaling group and set the CloudWatch metric to a higher threshold</p>', 'Increase the instance type in the launch configuration', 'Increase the base number of Auto Scaling instances for the Auto Scaling group', 'Add provisioned IOPS to the instances', 'Add EBS Volumes to the instances'], 'feedbacks': ['', '', '', '', ''], 'question': 'You just joined a large tech company with an existing Amazon VPC. When reviewing the Auto Scaling events, you noticed that their web application is scaling up and down multiple times within the hour. <br><br>What design change could you make to optimize cost while preserving elasticity?', 'explanation': '<p>Since the application is scaling up and down multiple times within the hour, the issue lies on the cooldown period of the Auto Scaling group.</p> <p>The cooldown period is a configurable setting for your Auto Scaling group that helps to ensure that it doesn\'t launch or terminate additional instances before the previous scaling activity takes effect. After the Auto Scaling group dynamically scales using a simple scaling policy, it waits for the cooldown period to complete before resuming scaling activities.</p> <p>When you manually scale your Auto Scaling group, the default is not to wait for the cooldown period, but you can override the default and honor the cooldown period. If an instance becomes unhealthy, the Auto Scaling group does not wait for the cooldown period to complete before replacing the unhealthy instance.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/as-scale-based-on-demand.html">http://docs.aws.amazon.com/autoscaling/latest/userguide/as-scale-based-on-demand.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567334, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You just joined a large tech company with an existing Amazon VPC. When reviewing the Auto Scaling events, you noticed that their web application is scaling up and down multiple times within the hour. What design change could you make to optimize cost while preserving elasticity?', 'id': 10430176, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>By using your own keys in AWS Key Management Service (KMS).</p>', '<p>By using S3 Server-Side Encryption.</p>', '<p>By using Amazon-managed keys in AWS Key Management Service (KMS).</p>', '<p>By using S3 Client-Side Encryption.</p>', '<p>By using a password stored in CloudHSM.</p>', '<p>By using the SSL certificates provided by the AWS Certificate Manager (ACM).</p>'], 'feedbacks': ['', '', '', '', '', ''], 'question': "<p>A health organization is using a large Dedicated EC2 instance with multiple EBS volumes to host its health records web application. The EBS volumes must be encrypted due to the confidentiality of the data that they are handling and also to comply with the HIPAA (Health Insurance Portability and Accountability Act) standard.\xa0 \xa0</p><p>In EBS encryption, what service does AWS use to secure the volume's data at rest? (Choose 2)</p>", 'explanation': '<p>Amazon EBS encryption offers seamless encryption of EBS data volumes, boot volumes, and snapshots, eliminating the need to build and maintain a secure key management infrastructure. EBS encryption enables data at rest security by encrypting your data using Amazon-managed keys, or keys you create and manage using the AWS Key Management Service (KMS). The encryption occurs on the servers that host EC2 instances, providing encryption of data as it moves between EC2 instances and EBS storage. Hence, options 1 and 3 are the right answers.</p> <p>Options 2 and 4 are incorrect as these relate only to S3.</p> <p>Option 5 is incorrect as you only store keys in CloudHSM and not passwords.</p> <p>Option 6 is incorrect as ACM only provides SSL certificates and not data encryption of EBS Volumes.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/ebs/faqs/">https://aws.amazon.com/ebs/faqs/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2567336, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': "A health organization is using a large Dedicated EC2 instance with multiple EBS volumes to host its health records web application. The EBS volumes must be encrypted due to the confidentiality of the data that they are handling and also to comply with the HIPAA (Health Insurance Portability and Accountability Act) standard.\xa0 \xa0In EBS encryption, what service does AWS use to secure the volume's data at rest? (Choose 2)", 'id': 10430178, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'ECS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use ECS as the container management service then set up a combination of Reserved and Spot EC2 Instances for processing mission-critical and non-essential batch jobs respectively.\xa0 </p>', '<p>Use ECS as the container management service then set up Reserved EC2 Instances for processing both mission-critical and non-essential batch jobs.\xa0 </p>', '<p>Use ECS as the container management service then set up On-Demand EC2 Instances for processing both mission-critical and non-essential batch jobs.\xa0 </p>', '<p>Use ECS as the container management service then set up Spot EC2 Instances for processing both mission-critical and non-essential batch jobs.\xa0 </p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are required to deploy a Docker-based batch application to your VPC in AWS. The application will be used to process both mission-critical data as well as non-essential batch jobs. Which of the following is the most cost-effective option to use in implementing this architecture?\xa0 </p>', 'explanation': '<p>Amazon ECS lets you run batch workloads with managed or custom schedulers on Amazon EC2 On-Demand Instances, Reserved Instances, or Spot Instances. You can launch a combination of EC2 instances to set up a cost-effective architecture depending on your workload. You can launch Reserved EC2 instances to process the mission-critical data and Spot EC2 instances for processing non-essential batch jobs.</p> <p>There are two different charge models for Amazon Elastic Container Service (ECS):&nbsp;Fargate Launch Type Model and&nbsp;EC2 Launch Type Model. With Fargate, you pay for the amount of vCPU and memory resources that your containerized application requests while for EC2 launch type model, there is no additional charge. You pay for AWS resources (e.g. EC2 instances or EBS volumes) you create to store and run your application. You only pay for what you use, as you use it; there are no minimum fees and no upfront commitments.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/images/overview-containeragent-fargate.png" alt="" width="440" height="401" /></p> <p>&nbsp;</p> <p>In this scenario, the most cost-effective solution is to use ECS as the container management service then set up a combination of Reserved and Spot EC2 Instances for processing mission-critical and non-essential batch jobs respectively. You can use Scheduled Reserved Instances (Scheduled Instances) which enables you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. This will ensure that you have an interrupted compute capacity to process your mission-critical batch jobs. Hence, the correct answer is 1.</p> <p>Option 2 is incorrect because&nbsp;processing the non-essential batch jobs can be handled much cheaper by using Spot EC2 instances instead of Reserved Instances.</p> <p>Option 3 is incorrect because an On-Demand instance&nbsp;costs more compared to Reserved and Spot EC2 instances. Processing the non-essential batch jobs can be handled much cheaper by using Spot EC2 instances instead of On-Demand instances.&nbsp;</p> <p>Option 4 is incorrect because although this set up provides the cheapest solution among other options, it will not be able to meet the required workload. Using Spot instances to process mission-critical workloads is not suitable since these types of instances can be terminated by AWS at any time, which can affect critical processing.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p> <p><a href="https://aws.amazon.com/ec2/spot/containers-for-less/get-started/">https://aws.amazon.com/ec2/spot/containers-for-less/get-started/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-container-service-amazon-ecs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-container-service-amazon-ecs/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567362, '_class': 'assessment', 'updated': '2019-06-22T04:59:38Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are required to deploy a Docker-based batch application to your VPC in AWS. The application will be used to process both mission-critical data as well as non-essential batch jobs. Which of the following is the most cost-effective option to use in implementing this architecture?', 'id': 10430200, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['<p><code>aws ec2 describe-instances</code></p>', '<p><code>aws ec2 describe-images</code></p>', '<p><code>aws ec2 get-console-screenshot</code></p>', '<p><code>aws ec2 describe-volume-status</code></p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A website is running on an Auto Scaling group of On-Demand EC2 instances which are abruptly getting terminated from time to time. To automate the monitoring process, you started to create a simple script which uses the AWS CLI to find the root cause of this issue.\xa0 </p><p>Which of the following is the most suitable command to use?</p>', 'explanation': '<p>The <code>describe-instances</code> command shows the status of the EC2 instances including the recently terminated instances. It also returns a <code>StateReason</code> of why the instance was terminated.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567342, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A website is running on an Auto Scaling group of On-Demand EC2 instances which are abruptly getting terminated from time to time. To automate the monitoring process, you started to create a simple script which uses the AWS CLI to find the root cause of this issue.\xa0 Which of the following is the most suitable command to use?', 'id': 10430182, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Route53', 'prompt': {'relatedLectureIds': '', 'answers': ['Latency', 'Failover', 'Weighted', 'Geolocation'], 'feedbacks': ['', '', '', ''], 'question': '<p>Your manager instructed you to use Route 53 instead of an ELB to load balance the incoming request to your web application. The system is deployed to two EC2 instances to which the traffic needs to be\xa0 distributed to. You want to set a specific percentage of traffic to go to each instance.\xa0 \xa0</p><p>Which routing policy would you use?</p>', 'explanation': '<p>Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes including load balancing and testing new versions of software. You can set a specific percentage of how much traffic will be allocated to the resource by specifying the weights.</p> <p>For example, if you want to send a tiny portion of your traffic to one resource and the rest to another resource, you might specify weights of 1 and 255. The resource with a weight of 1 gets 1/256th of the traffic (1/1+255), and the other resource gets 255/256ths (255/1+255).</p> <p>You can gradually change the balance by changing the weights. If you want to stop sending traffic to a resource, you can change the weight for that record to 0.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567346, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'Your manager instructed you to use Route 53 instead of an ELB to load balance the incoming request to your web application. The system is deployed to two EC2 instances to which the traffic needs to be\xa0 distributed to. You want to set a specific percentage of traffic to go to each instance.\xa0 \xa0Which routing policy would you use?', 'id': 10430184, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudHSM', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>AWS manages the hardware security module (HSM) appliance, but does not have access to your keys.</p>', '<p>Your HSMs are in your Virtual Private Cloud (VPC) and isolated from other AWS networks.</p>', '<p>You control and manage your own encryption keys.</p>', '<p>It provides a secure key storage in tamper-resistant hardware available in a single Availability Zone.</p>'], 'feedbacks': ['', '', '', ''], 'question': "<p>An online stock trading portal is deployed in AWS and in order to complete the set up, you need to offload the SSL/TLS processing for your web servers using CloudHSM. This will reduce the burden on your web servers and provides extra security by storing your web server's private key in this cloud-based hardware security module.\xa0 \xa0</p><p>Which of the following statements is not true about Amazon CloudHSM?</p>", 'explanation': '<p>Take note that CloudHSM provides a secure key storage in tamper-resistant hardware available in multiple Availability Zones (AZs) and not just on one AZ. Hence, Option 4 is the incorrect answer.</p> <p>AWS CloudHSM runs in your own Amazon Virtual Private Cloud (VPC), enabling you to easily use your HSMs with applications running on your Amazon EC2 instances. With CloudHSM, you can use standard VPC security controls to manage access to your HSMs.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/product-marketing/cloudhsm/CloudHSM_Diagrams_2-final.6f427ebb14d021b9cd6120aeee72cf4d3723e89b.png" alt="" width="502" height="300" /></p> <p>&nbsp;</p> <p>Your applications connect to your HSMs using mutually authenticated SSL channels established by your HSM client software. Since your HSMs are located in Amazon datacenters near your EC2 instances, you can reduce the network latency between your applications and HSMs versus an on-premises HSM.</p> <ul> <li>AWS manages the hardware security module (HSM) appliance but does not have access to your keys</li> <li>You control and manage your own keys</li> <li>Application performance improves (due to close proximity with AWS workloads)</li> <li>Secure key storage in tamper-resistant hardware available in multiple Availability Zones (AZs)</li> <li>Your HSMs are in your Virtual Private Cloud (VPC) and isolated from other AWS networks.</li> </ul> <p>Separation of duties and role-based access control is inherent in the design of the AWS CloudHSM. AWS monitors the health and network availability of your HSMs but is not involved in the creation and management of the key material stored within your HSMs. You control the HSMs and the generation and use of your encryption keys.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/cloudhsm/">https://aws.amazon.com/cloudhsm/</a></p> <p>&nbsp;</p> <p><strong>Learn more about AWS CloudHSM and AWS WAF in this video tutorial:</strong></p> <iframe width="560" height="315" src="https://www.youtube.com/embed/IMxImFoVpmI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'}, 'correct_response': ['d'], 'original_assessment_id': 2567364, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': "An online stock trading portal is deployed in AWS and in order to complete the set up, you need to offload the SSL/TLS processing for your web servers using CloudHSM. This will reduce the burden on your web servers and provides extra security by storing your web server's private key in this cloud-based hardware security module.\xa0 \xa0Which of the following statements is not true about Amazon CloudHSM?", 'id': 10430202, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'relatedLectureIds': '', 'answers': ['Enable AWS CloudTrail for their application load balancer.', 'Enable access logs on the application load balancer.', 'Add an Amazon CloudWatch Logs agent on the application load balancer.', 'Enable Amazon CloudWatch metrics on the application load balancer.'], 'feedbacks': ['', '', '', ''], 'question': 'The social media company that you are working for needs to capture the detailed information of all HTTP requests that went through their public-facing application load balancer every five minutes. They want to use this data for analyzing traffic patterns and for troubleshooting their web applications in AWS. <br><br>Which of the following options meet the customer requirements?', 'explanation': '<p>Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client\'s IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.</p> <p>Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. You can disable access logging at any time.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html">http://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567348, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'The social media company that you are working for needs to capture the detailed information of all HTTP requests that went through their public-facing application load balancer every five minutes. They want to use this data for analyzing traffic patterns and for troubleshooting their web applications in AWS. Which of the following options meet the customer requirements?', 'id': 10430186, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'relatedLectureIds': '', 'answers': ['Provide the system administrators the secret access key and access key id.', 'Enable multi-factor authentication on their accounts and define a password policy.', 'Provide a password for each user created and give these passwords to your system administrators.', 'Add the administrators to the Security Group.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are a Solutions Architect working for a startup which is currently migrating their production environment to AWS. Your manager asked you to set up access to the AWS console using Identity Access Management (IAM). You have created 5 users for your system administrators.\xa0 \xa0</p><p>What further steps do you need to take to enable your system administrators to get access to the AWS console?</p>', 'explanation': '<p>The&nbsp;AWS Management Console is the web interface used to manage your AWS resources using your web browser. To access this, your users should have a password that they can use to login to the web console.</p> <p>Option 1 is incorrect as the&nbsp;secret access key and access key id are used to trigger AWS API calls.</p> <p>Option 2 is incorrect because the multi-factor authentication and a password policy are just additional security measures for the IAM user but these won\'t enable them to access the AWS Management Console.</p> <p>Option 4 is incorrect as you could not add an IAM user to a security group. Remember that a security group is used for EC2 instances only.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_how-users-sign-in.html">http://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_how-users-sign-in.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567350, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are a Solutions Architect working for a startup which is currently migrating their production environment to AWS. Your manager asked you to set up access to the AWS console using Identity Access Management (IAM). You have created 5 users for your system administrators.\xa0 \xa0What further steps do you need to take to enable your system administrators to get access to the AWS console?', 'id': 10430188, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'API Gateway', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>HTTP/2</p>', 'HTTPS', 'HTTP', '<p>WebSocket</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You have just launched a new API Gateway service which uses AWS Lambda as a serverless computing service. In what type of protocol will your API endpoint be exposed?</p>', 'explanation': '<p>All of the APIs created with Amazon API Gateway expose <strong>HTTPS</strong> endpoints only. Amazon API Gateway does not support unencrypted (HTTP) endpoints. By default, Amazon API Gateway assigns an internal domain to the API that automatically uses the Amazon API Gateway certificate. When configuring your APIs to run under a custom domain name, you can provide your own certificate for the domain.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/api-gateway/faqs/" target="_blank" rel="noopener">https://aws.amazon.com/api-gateway/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon API Gateway Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-api-gateway/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-api-gateway/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567352, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You have just launched a new API Gateway service which uses AWS Lambda as a serverless computing service. In what type of protocol will your API endpoint be exposed?', 'id': 10430190, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudTrail', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Amazon CloudWatch</p>', '<p>AWS CloudTrail</p>', '<p>AWS X-Ray</p>', '<p>Amazon API Gateway</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>An application is hosted in an On-Demand EC2 instance and is using Amazon SDK to communicate to other AWS services such as S3, DynamoDB, and many others. As part of the upcoming IT audit, you need to ensure that all API calls to your AWS resources are logged and durably stored.\xa0 </p><p>Which is the most suitable service that you should use\xa0to meet this requirement?</p>', 'explanation': '<p>AWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred.</p> <p>Option 1 is incorrect because&nbsp;Amazon CloudWatch is primarily used for systems monitoring based on the server metrics. It does not have the capability to track API calls to your AWS resources.</p> <p>Option 3 is incorrect because AWS X-Ray is usually used to debug and analyze your microservices applications with request tracing so you can find the root cause of issues and performance. Unlike CloudTrail, it does not record the API calls that were made to your AWS resources.</p> <p>Option 4 is incorrect because Amazon API Gateway is not used for logging each and every API call to your AWS resources. It is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;AWS CloudTrail Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudtrail/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567354, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'An application is hosted in an On-Demand EC2 instance and is using Amazon SDK to communicate to other AWS services such as S3, DynamoDB, and many others. As part of the upcoming IT audit, you need to ensure that all API calls to your AWS resources are logged and durably stored.\xa0 Which is the most suitable service that you should use\xa0to meet this requirement?', 'id': 10430192, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Set a lifecycle policy in the bucket to transition the data to Glacier after one week (7 days).</p>', '<p>Set a lifecycle policy in the bucket to transition the data to S3 - Standard IA storage class after one week (7 days).</p>', '<p>Set a lifecycle policy in the bucket to transition the data to S3 - One Zone-Infrequent Access storage class after one week (7 days).</p>', '<p>Set a lifecycle policy in the bucket to transition the data to S3 Glacier Deep Archive storage class after one week (7 days).</p>', '<p>Set a lifecycle policy in the bucket to transition to S3 - Standard IA after 30 days</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working as a Principal Solutions Architect for a leading digital news company which has both an on-premises data center as well as an AWS cloud infrastructure. They store their graphics, audios, videos, and other multimedia assets primarily in their on-premises storage server and use an S3 Standard storage class bucket as a backup. Their data are heavily used for only a week (7 days) but after that period, it will be infrequently used by their customers. You are instructed to save storage costs in AWS yet maintain the ability to immediately fetch their media assets for a surprise annual data audit, which will be conducted both on-premises and on their cloud storage. </p><p>Which of the following options should you implement to meet the above requirement? (Choose 2)</p>', 'explanation': '<p>You can add rules in a lifecycle configuration to tell Amazon S3 to transition objects to another Amazon S3 storage class. For example: When you know that objects are infrequently accessed, you might transition them to the STANDARD_IA storage class. Or&nbsp;transition your data to the GLACIER storage class in case you want to archive objects that you don\'t need to access in real time.</p> <p>In a lifecycle configuration, you can define rules to transition objects from one storage class to another to save on storage costs. When you don\'t know the access patterns of your objects or your access patterns are changing over time, you can transition the objects to the INTELLIGENT_TIERING storage class for automatic cost savings.</p> <p>The lifecycle storage class transitions have a constraint when you want to transition from the STANDARD storage classes to either STANDARD_IA or ONEZONE_IA. The following constraints apply:&nbsp;</p> <p style="padding-left: 30px;">&nbsp;- For larger objects, there is a cost benefit for transitioning to STANDARD_IA or ONEZONE_IA. Amazon S3 does not transition objects that are smaller than 128 KB to the STANDARD_IA or ONEZONE_IA storage classes because it\'s not cost effective.&nbsp;</p> <p style="padding-left: 30px;">&nbsp;- Objects must be stored <strong>at least 30 days</strong> in the current storage class before you can transition them to STANDARD_IA or ONEZONE_IA. For example, you cannot create a lifecycle rule to transition objects to the STANDARD_IA storage class one day after you create them. Amazon S3 doesn\'t transition objects within the first 30 days because newer objects are often accessed more frequently or deleted sooner than is suitable for STANDARD_IA or ONEZONE_IA storage.&nbsp;</p> <p style="padding-left: 30px;">&nbsp;- If you are transitioning noncurrent objects (in versioned buckets), you can transition only objects that are at least 30 days noncurrent to STANDARD_IA or ONEZONE_IA storage.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-04-02_04-26-21-158b6e1eb7ef7be3b91049e88f056f3a.gif" alt="" width="560" height="556" /></p> <p>&nbsp;</p> <p>Since there is a time constraint in transitioning objects in S3, you can only change the storage class of your objects from S3 Standard storage class to STANDARD_IA or ONEZONE_IA storage after 30 days. This limitation does not apply on&nbsp;INTELLIGENT_TIERING, GLACIER, and DEEP_ARCHIVE storage class. In this scenario, you can&nbsp;set a lifecycle policy in the bucket to transition to S3 - Standard IA after 30 days or alternatively, you can directly transition your data to Glacier after one week (7 days).</p> <p>In addition, the requirement says that the media assets should be fetched immediately for a surprise <strong>annual</strong> data audit. This means that the retrieval will only happen once a year. You can use expedited retrievals in Glacier which will allow you to quickly access your data (within 1&ndash;5 minutes) when occasional urgent requests for a subset of archives are required. Hence, Options 1 and 5 are the correct answers.</p> <p>Options 2 and 3 are both incorrect because&nbsp;there is a constraint in S3 that objects must be stored at least 30 days in the current storage class before you can transition them to STANDARD_IA or ONEZONE_IA. You cannot create a lifecycle rule to transition objects to either STANDARD_IA or ONEZONE_IA storage class 7 days after you create them because you can only do this after the 30-day period has elapsed. Hence, these options are incorrect.</p> <p>Option 4 is incorrect because although&nbsp;DEEP_ARCHIVE storage class provides the most cost-effective storage option, it does not have the ability to do expedited retrievals, unlike Glacier. In the event that the surprise annual data audit happens, it may take several hours before you can retrieve your data.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/restoring-objects.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/restoring-objects.html</a></p> <p><a href="https://aws.amazon.com/s3/storage-classes/">https://aws.amazon.com/s3/storage-classes/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['a', 'e'], 'original_assessment_id': 4519792, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are working as a Principal Solutions Architect for a leading digital news company which has both an on-premises data center as well as an AWS cloud infrastructure. They store their graphics, audios, videos, and other multimedia assets primarily in their on-premises storage server and use an S3 Standard storage class bucket as a backup. Their data are heavily used for only a week (7 days) but after that period, it will be infrequently used by their customers. You are instructed to save storage costs in AWS yet maintain the ability to immediately fetch their media assets for a surprise annual data audit, which will be conducted both on-premises and on their cloud storage. Which of the following options should you implement to meet the above requirement? (Choose 2)', 'id': 10430210, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'SQS', 'prompt': {'relatedLectureIds': '', 'answers': ['Configure Amazon SQS to use long polling by setting the ReceiveMessageWaitTimeSeconds to zero.', 'Configure Amazon SQS to use long polling by setting the ReceiveMessageWaitTimeSeconds to a number greater than zero.', 'Configure Amazon SQS to use short polling by setting the ReceiveMessageWaitTimeSeconds to a number greater than zero.', 'Configure Amazon SQS to use short polling by setting the ReceiveMessageWaitTimeSeconds to zero.'], 'feedbacks': ['', '', '', ''], 'question': '<p>Your company has a web-based ticketing service that utilizes Amazon SQS and a fleet of EC2 instances. The EC2 instances that consume messages from the SQS queue are configured to poll the queue as often as possible to keep end-to-end throughput as high as possible. You noticed that polling the queue in tight loops is using unnecessary CPU cycles, resulting in increased operational costs due to empty responses.\xa0 \xa0</p><p>In this scenario, what will you do to make the system more cost-effective?</p>', 'explanation': '<p>In this scenario, the application is deployed in a fleet of EC2 instances that are polling messages from a single SQS queue.&nbsp;Amazon SQS uses short polling by default, querying only a subset of the servers (based on a weighted random distribution) to determine whether any messages are available for inclusion in the response. Short polling works for scenarios that require higher throughput. However, you can also configure the queue to use Long polling instead, to reduce cost.</p> <p>The ReceiveMessageWaitTimeSeconds is the&nbsp;queue attribute that determines whether you are using Short or Long polling. By default, its value is zero which means&nbsp;it is using Short polling. If it is set to a value greater than zero, then it is Long polling. Hence, Option 2 is correct.</p> <p>Quick facts about SQS Long Polling:</p> <ul> <li>-Long polling helps reduce your cost of using Amazon SQS by reducing the number of empty responses when there are no messages available to return in reply to a <em>ReceiveMessage</em> request sent to an Amazon SQS queue and eliminating false empty responses when messages are available in the queue but aren\'t included in the response.&nbsp;</li> <li>-Long polling reduces the number of empty responses by allowing Amazon SQS to wait until a message is available in the queue before sending a response. Unless the connection times out, the response to the <code>ReceiveMessage</code>&nbsp;request contains at least one of the available messages, up to the maximum number of messages specified in the&nbsp;<code>ReceiveMessage</code>&nbsp;action.</li> <li>-Long polling eliminates false empty responses by querying all (rather than a limited number) of the servers. Long polling returns messages as soon any message becomes available.</li> </ul> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-sqs/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567366, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'Your company has a web-based ticketing service that utilizes Amazon SQS and a fleet of EC2 instances. The EC2 instances that consume messages from the SQS queue are configured to poll the queue as often as possible to keep end-to-end throughput as high as possible. You noticed that polling the queue in tight loops is using unnecessary CPU cycles, resulting in increased operational costs due to empty responses.\xa0 \xa0In this scenario, what will you do to make the system more cost-effective?', 'id': 10430204, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['Maintain two snapshots: the original snapshot and the latest incremental snapshot.', 'Maintain a volume snapshot; subsequent snapshots will overwrite one another.', 'Just maintain a single snapshot of the EBS volume since the latest snapshot is both incremental and complete.', 'Maintain the most current snapshot and then archive the original and incremental snapshots to Amazon Glacier.'], 'feedbacks': ['', '', '', ''], 'question': '<p>A leading bank has an application that is hosted on an Auto Scaling group of EBS-backed EC2 instances. As the Solutions Architect, you need to provide the ability to fully restore the data stored in their EBS volumes by using EBS snapshots.\xa0 \xa0</p><p>Which of the following approaches provide the lowest cost for Amazon Elastic Block Store snapshots?</p>', 'explanation': '<p>To meet the requirement on this scenario, you can just maintain a single snapshot of the EBS volume since its latest snapshot is both incremental and complete.</p> <p>You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are&nbsp;<em>incremental</em>&nbsp;backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. This minimizes the time required to create the snapshot and saves on storage costs by not duplicating data.</p> <p>When you delete a snapshot, only the data unique to that snapshot is removed. Each snapshot contains all of the information needed to restore your data (from the moment the snapshot was taken) to a new EBS volume.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/snapshot_1a.png" alt="" width="767" height="758" /></p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567370, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A leading bank has an application that is hosted on an Auto Scaling group of EBS-backed EC2 instances. As the Solutions Architect, you need to provide the ability to fully restore the data stored in their EBS volumes by using EBS snapshots.\xa0 \xa0Which of the following approaches provide the lowest cost for Amazon Elastic Block Store snapshots?', 'id': 10430206, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Stop the Reserved instances as soon as possible.', 'Contact AWS to cancel your AWS subscription.', 'Go to the AWS Reserved Instance Marketplace and sell the Reserved instances.', '<p>Terminate the Reserved instances as soon as possible to avoid getting billed at the on-demand price when it expires </p>', 'Go to the Amazon.com online shopping website and sell the Reserved instances.'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working for a large financial firm in the country. They have an AWS environment which contains several Reserved EC2 instances hosted in a web application that has been decommissioned last week. To save cost, you need to stop incurring charges for the Reserved instances as soon as possible.\xa0 \xa0</p><p>What cost-effective steps will you take in this circumstance? (Choose 2)</p>', 'explanation': '<div> <p>The correct options are:</p> <ul> <li>Go to the AWS Reserved Instance Marketplace and sell the Reserved instances.</li> <li>Terminate the Reserved instances as soon as possible to avoid getting billed at the on-demand price when it expires</li> </ul> <p>The Reserved Instance Marketplace is a platform that supports the sale of third-party and AWS customers\' unused Standard Reserved Instances, which vary in terms of lengths and pricing options. For example, you may want to sell Reserved Instances after moving instances to a new AWS region, changing to a new instance type, ending projects before the term expiration, when your business needs change, or if you have unneeded capacity.</p> <p>Option 1 is incorrect because a stopped instance can still be restarted. Take note that when a Reserved Instance expires, any instances that were covered by the Reserved Instance are billed at the on-demand price which costs significantly higher. Since the application is already decommissioned, there is no point of keeping the unused instances. It is also possible that there are associated Elastic IP addresses, which will incur charges if they are associated with stopped instances</p> <p>Option 2 is incorrect as you don\'t need to close down your AWS account.</p> <p>Option 5 is incorrect as you have to use&nbsp;AWS Reserved Instance Marketplace to sell your instances.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> </div> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-general.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-general.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-lifecycle.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c', 'd'], 'original_assessment_id': 2567372, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'You are working for a large financial firm in the country. They have an AWS environment which contains several Reserved EC2 instances hosted in a web application that has been decommissioned last week. To save cost, you need to stop incurring charges for the Reserved instances as soon as possible.\xa0 \xa0What cost-effective steps will you take in this circumstance? (Choose 2)', 'id': 10430208, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Create an Internet gateway in the public subnet with a corresponding route entry that directs the data to S3.</p>', '<p>Configure a VPC Interface Endpoint along with a corresponding route entry that directs the data to S3.</p>', '<p>Configure a VPC Endpoint Gateway along with a corresponding route entry that directs the data to S3.</p>', '<p>Provision a NAT gateway in the private subnet with a corresponding route entry that directs the data to S3.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A local bank has an in-house application which handles sensitive financial data in a private subnet. After the data is processed by the EC2 worker instances, they will be delivered to S3 for ingestion by other services. </p><p>How should you design this solution so that the data does not pass through the public Internet?</p>', 'explanation': '<p>The important concept that you have to understand in the scenario is that your VPC and your S3 bucket are located within the larger AWS network. However, the traffic coming from your VPC to your S3 bucket is traversing the public Internet by default. To better protect your data in transit,&nbsp;you can set up a VPC endpoint so the incoming traffic from your VPC will not pass through the public Internet, but instead through the private AWS network.</p> <p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an Internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other services do not leave the Amazon network.</p> <p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components that allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p> <p>There are two types of VPC endpoints:&nbsp;<em>interface endpoints</em>&nbsp;and&nbsp;<em>gateway endpoints</em>. You&nbsp;should create the type of VPC endpoint required by the supported service. As a rule of thumb, most AWS services use VPC <em>Interface</em> Endpoint except for S3 and DynamoDB, which use VPC <em>Gateway</em> Endpoint.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p><img src="https://docs.aws.amazon.com/vpc/latest/userguide/images/vpc-endpoint-s3-diagram.png" /></p> <p>&nbsp;</p> <p>Option 3 is correct because VPC Endpoint Gateway supports private connection to S3.</p> <p>Option 1 is incorrect because Internet gateway is used for instances in the public subnet to have accessibility to the Internet.</p> <p>Option 2 is incorrect because VPC&nbsp;Interface Endpoint does not support the S3 service. You should use a VPC Endpoint Gateway instead. As mentioned in the above explanation, most AWS services use VPC <em>Interface</em> Endpoint except for S3 and DynamoDB, which use VPC <em>Gateway</em> Endpoint.</p> <p>Option 4 is incorrect because NAT Gateway allows instances in the private subnet to gain access to the Internet, but not vice versa.</p> <p>&nbsp;</p> <p><strong>References</strong>:</p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a> <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a><br /><br /></p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 7786612, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A local bank has an in-house application which handles sensitive financial data in a private subnet. After the data is processed by the EC2 worker instances, they will be delivered to S3 for ingestion by other services. How should you design this solution so that the data does not pass through the public Internet?', 'id': 10430212, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Storage Gateway', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Stored Volume Gateway</p>', '<p>Tape Gateway</p>', '<p>Cached Volume Gateway</p>', '<p>File Gateway</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A data analytics company keeps a massive volume of data which they store in their on-premises data center. To scale their storage systems, they are looking for cloud-backed storage volumes that they can mount using Internet Small Computer System Interface (iSCSI) devices from their on-premises application servers. They have an on-site data analytics application which frequently access the latest data subsets locally while the older data are rarely accessed. You are required to minimize the need to scale the on-premises storage infrastructure while still providing their web application with low-latency access to the data. </p><p>Which type of AWS Storage Gateway service will you use to meet the above requirements?</p>', 'explanation': '<p>In this scenario, the technology company is looking for a storage service that will enable their analytics application to frequently access the latest data subsets and not the entire data set because it was mentioned that the old data are rarely being used. This requirement can be fulfilled by setting up a Cached Volume Gateway in AWS Storage Gateway.</p> <p>By using cached volumes, you can use Amazon S3 as your primary data storage, while retaining frequently accessed data locally in your storage gateway. Cached volumes minimize the need to scale your on-premises storage infrastructure, while still providing your applications with low-latency access to frequently accessed data. You can create storage volumes up to 32 TiB in size and afterwards, attach these volumes as iSCSI devices to your on-premises application servers. When you write to these volumes, your gateway stores the data in Amazon S3. It retains the recently read data in your on-premises storage gateway\'s cache and uploads buffer storage.</p> <p>Cached volumes can range from 1 GiB to 32 TiB in size and must be rounded to the nearest GiB. Each gateway configured for cached volumes can support up to 32 volumes for a total maximum storage volume of 1,024 TiB (1 PiB).</p> <p>In the cached volumes solution, AWS Storage Gateway stores all your on-premises application data in a storage volume in Amazon S3. Hence, the correct answer is Option 3.</p> <p>Option 1 is incorrect because the requirement is to provide low latency access to the frequently accessed data subsets locally. Stored Volume Gateway is used if you need low-latency access to your entire dataset.</p> <p>Option 2 is incorrect because a Tape Gateway is a cost-effective, durable, long-term offsite alternative for data archiving, which is not needed in this scenario.&nbsp;</p> <p>Option 4 is incorrect because a File gateway does not provide you the required low-latency access to the frequently accessed data that the on-site analytics application needs.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#volume-gateway-concepts ">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#volume-gateway-concepts </a></p> <p><a href="https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html">https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-storage-gateway/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-storage-gateway/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 8217126, '_class': 'assessment', 'updated': '2019-06-22T04:59:36Z', 'created': '2019-06-22T04:59:36Z', 'question_plain': 'A data analytics company keeps a massive volume of data which they store in their on-premises data center. To scale their storage systems, they are looking for cloud-backed storage volumes that they can mount using Internet Small Computer System Interface (iSCSI) devices from their on-premises application servers. They have an on-site data analytics application which frequently access the latest data subsets locally while the older data are rarely accessed. You are required to minimize the need to scale the on-premises storage infrastructure while still providing their web application with low-latency access to the data. Which type of AWS Storage Gateway service will you use to meet the above requirements?', 'id': 10430214, 'related_lectures': [], 'assessment_type': 'multiple-choice'}]}, 'type': 'practice-test', 'title': 'AWS Certified Solutions Architect Associate Practice Test 5'}, {'quiz_data': {'next': None, 'count': 65, 'previous': None, 'results': [{'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Launch the instances to a single Availability Zone.', 'Launch the instances to multiple Availability Zones.', 'Launch the instances in the Amazon Virtual Private Cloud (VPC).', 'Launch the instances in a Placement Group.', 'Launch the instances in EC2-Classic.'], 'feedbacks': ['', '', '', '', ''], 'question': 'You are a Solutions Architect working for a software development company. You are planning to launch a fleet of EBS-backed EC2 instances and want to automatically assign each instance with a static private IP address which does not change even if the instances are restarted. <br><br>What should you do to accomplish this?', 'explanation': '<p>In EC2-Classic, your EC2 instance receives a private IPv4 address from the EC2-Classic range each time it\'s started. In EC2-VPC on the other hand, your EC2 instance receives a static private IPv4 address from the address range of your default VPC. Hence, the correct answer is Option 3 and not Option 5.</p> <p style="text-align: center;">&nbsp;</p> <table> <tbody> <tr style="text-align: center;"> <th>Characteristic</th> <th>EC2-Classic</th> <th>Default VPC</th> <th>Nondefault VPC</th> </tr> <tr style="text-align: center;"> <td> <p>Public IPv4 address (from Amazon\'s public IP address pool)</p> </td> <td> <p>Your instance receives a public IPv4 address from the EC2-Classic public IPv4 address pool.</p> </td> <td> <p>Your instance launched in a default subnet receives a public IPv4 address by default, unless you specify otherwise during launch, or you modify the subnet\'s public IPv4 address attribute.</p> </td> <td> <p>Your instance doesn\'t receive a public IPv4 address by default, unless you specify otherwise during launch, or you modify the subnet\'s public IPv4 address attribute.</p> </td> </tr> <tr style="text-align: center;"> <td> <p>Private IPv4 address</p> </td> <td> <p><span style="background-color: #ffff00;">Your instance receives a private IPv4 address from the EC2-Classic range each time it\'s started.</span></p> </td> <td> <p>Your instance receives a static private IPv4 address from the address range of your default VPC.</p> </td> <td> <p>Your instance receives a static private IPv4 address from the address range of your VPC.</p> </td> </tr> <tr style="text-align: center;"> <td> <p>Multiple private IPv4 addresses</p> </td> <td> <p>We select a single private IP address for your instance; multiple IP addresses are not supported.</p> </td> <td> <p>You can assign multiple private IPv4 addresses to your instance.</p> </td> <td> <p>You can assign multiple private IPv4 addresses to your instance.</p> </td> </tr> <tr> <td style="text-align: center;"> <p>Elastic IP address (IPv4)</p> </td> <td style="text-align: center;"> <p>An Elastic IP is disassociated from your instance when you stop it.</p> </td> <td style="text-align: center;"> <p>An Elastic IP remains associated with your instance when you stop it.</p> </td> <td> <p style="text-align: center;">An Elastic IP remains associated with your instance when you stop it.</p> </td> </tr> </tbody> </table> <p>&nbsp;</p> <p>Options 1 and 2 are incorrect due to the fact that&nbsp;Availability Zones do not provide static private IP addresses to EC2 instances.</p> <p>Option 4 is incorrect as a Placement Group is just a grouping of instances.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-classic-platform.html#differences-ec2-classic-vpc">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-classic-platform.html#differences-ec2-classic-vpc</a></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567374, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You are a Solutions Architect working for a software development company. You are planning to launch a fleet of EBS-backed EC2 instances and want to automatically assign each instance with a static private IP address which does not change even if the instances are restarted. What should you do to accomplish this?', 'id': 10079110, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use On-Demand EC2 instances which allows you to pay for the instances that you launch and use by the second.</p>', '<p>Use Reserved Instances which provides a compute capacity that is always available for a term from one to three years.</p>', '<p>Use Scheduled Reserved Instances which provides compute capacity that is always available on the specified recurring schedule, for a one-year term.\xa0 </p>', '<p>Use Spot EC2 Instances launched by a persistent Spot request, which can significantly lower your Amazon EC2 costs.</p>', '<p>Use Dedicated Hosts which provide a physical host that is fully dedicated to running your instances, and bring your existing per-socket, per-core, or per-VM software licenses to reduce costs.</p>', '<p>Use Dedicated Instances that provide a compute capacity which runs on single-tenant hardware and are paid by the hour.</p>'], 'feedbacks': ['', '', '', '', '', ''], 'question': '<p>A multinational corporate and investment bank is regularly processing steady workloads of accruals, loan interests, and other critical financial calculations every night at 10 PM to 3 AM on their on-premises data center for their corporate clients. Once the process is done, the results are then uploaded to the Oracle General Ledger which means that the processing should not be delayed nor interrupted. The CTO has decided to move their IT infrastructure to AWS to save cost and to improve the scalability of their digital financial services.\xa0 \xa0</p><p>As the Senior Solutions Architect, how can you implement a cost-effective architecture in AWS for their financial system?\xa0 </p>', 'explanation': '<p>Scheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. You reserve the capacity in advance, so that you know it is available when you need it. You pay for the time that the instances are scheduled, even if you do not use them.</p> <p>Scheduled Instances are a good choice for workloads that do not run continuously, but do run on a regular schedule. For example, you can use Scheduled Instances for an application that runs during business hours or for batch processing that runs at the end of the week.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://aws.amazon.com/blogs/aws/new-scheduled-reserved-instances/">https://aws.amazon.com/blogs/aws/new-scheduled-reserved-instances/</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-scheduled-instances.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567376, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'A multinational corporate and investment bank is regularly processing steady workloads of accruals, loan interests, and other critical financial calculations every night at 10 PM to 3 AM on their on-premises data center for their corporate clients. Once the process is done, the results are then uploaded to the Oracle General Ledger which means that the processing should not be delayed nor interrupted. The CTO has decided to move their IT infrastructure to AWS to save cost and to improve the scalability of their digital financial services.\xa0 \xa0As the Senior Solutions Architect, how can you implement a cost-effective architecture in AWS for their financial system?', 'id': 10079112, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['IPsec VPN connection', '<p>Amazon Connect</p>', 'AWS Direct Connect', '<p>VPC Peering</p>', '<p>NAT Gateway</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are the Solutions Architect of a software development company where you are required to connect the on-premises infrastructure to their AWS cloud. Which of the following AWS services can you use to accomplish this? (Choose 2)</p>', 'explanation': '<p>You can connect your VPC to remote networks by using a VPN connection which can be Direct Connect, IPsec VPN connection,&nbsp;AWS VPN CloudHub, or a third party software VPN appliance. Hence, Options 1 and 3 are correct.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/images/aws-direct-connect-gateway.png" /></p> <p>&nbsp;</p> <p>Option 2 is incorrect because Amazon Connect is not a VPN connectivity option. It is actually a self-service, cloud-based contact center service in AWS that makes it easy for any business to deliver better customer service at a lower cost. Amazon Connect is based on the same contact center technology used by Amazon customer service associates around the world to power millions of customer conversations.&nbsp;</p> <p>Option 4 is incorrect because a VPC peering connection is a networking connection between two VPCs only, which enables you to route traffic between them privately. This can\'t be used to connect your on-premises network to your VPC.</p> <p>Option 5 is incorrect because&nbsp;you only use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances. This is not used to connect to your on-premises network.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html</a></p> <p><a href="https://aws.amazon.com/connect/">https://aws.amazon.com/connect/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['a', 'c'], 'original_assessment_id': 2567382, '_class': 'assessment', 'updated': '2019-06-02T00:09:41Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You are the Solutions Architect of a software development company where you are required to connect the on-premises infrastructure to their AWS cloud. Which of the following AWS services can you use to accomplish this? (Choose 2)', 'id': 10079118, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'ELB', 'prompt': {'relatedLectureIds': '', 'answers': ['Elastic Load Balancing request routing', '<p>An Amazon Route 53 weighted routing policy</p>', 'Cross-zone load balancing', 'An Amazon Route 53 latency routing policy', '<p>Elastic Load Balancing Redirects </p>', '<p>An Amazon Route 53 failover routing policy</p>'], 'feedbacks': ['', '', '', '', '', ''], 'question': '<p>You are working as a Senior Solutions Architect in a digital media services startup. Your current project is about a movie streaming app where you are required to launch several EC2 instances on multiple availability zones. Which of the following will configure your load balancer to distribute incoming requests evenly to all EC2 instances across multiple Availability Zones? </p>', 'explanation': '<p>The right answer is to enable&nbsp;<em>cross-zone load balancing.</em></p> <p>If the load balancer nodes for your Classic Load Balancer can distribute requests regardless of Availability Zone, this is known as&nbsp;<em>cross-zone load balancing</em>. With cross-zone load balancing enabled, your load balancer nodes distribute incoming requests evenly across the Availability Zones enabled for your load balancer. Otherwise, each load balancer node distributes requests only to instances in its Availability Zone.</p> <p>For example, if you have 10 instances in Availability Zone us-west-2a and 2 instances in us-west-2b, the requests are distributed evenly across all 12 instances if cross-zone load balancing is enabled. Otherwise, the 2 instances in us-west-2b serve the same number of requests as the 10 instances in us-west-2a.</p> <p>Cross-zone load balancing reduces the need to maintain equivalent numbers of instances in each enabled Availability Zone, and improves your application\'s ability to handle the loss of one or more instances. However, we still recommend that you maintain approximately equivalent numbers of instances in each enabled Availability Zone for higher fault tolerance.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-crosszone-lb.html">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-crosszone-lb.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567378, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You are working as a Senior Solutions Architect in a digital media services startup. Your current project is about a movie streaming app where you are required to launch several EC2 instances on multiple availability zones. Which of the following will configure your load balancer to distribute incoming requests evenly to all EC2 instances across multiple Availability Zones?', 'id': 10079114, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ElastiCache', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Enable the in-transit encryption for Redis replication groups.</p>', '<p>Create a new Redis replication group and set the <code>AtRestEncryptionEnabled</code> parameter to <code>true</code>.</p>', '<p>Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the <code>--transit-encryption-enabled</code> and <code>--auth-token</code> parameters enabled.</p>', '<p>Do nothing. This feature is already enabled by default.</p>', '<p>None of the above. </p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>An online stock trading system is hosted in AWS and uses an Auto Scaling group of EC2 instances, an RDS database, and an Amazon ElastiCache for Redis. You need to improve the data security of your in-memory data store by requiring the user to enter a password before they are granted permission to execute Redis commands.\xa0 \xa0</p><p>Which of the following should you do to meet the above requirement?</p>', 'explanation': '<p>Using Redis&nbsp;<code class="code">AUTH</code>&nbsp;command can improve data security by requiring the user to enter a password before they are granted permission to execute Redis commands on a password-protected Redis server. Hence, Option 3 is the correct answer.</p> <p>To require that users enter a password on a password-protected Redis server, include the parameter&nbsp;<code class="code"><strong>--auth-token</strong></code>&nbsp;with the correct password when you create your replication group or cluster and on all subsequent commands to the replication group or cluster.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/images/ElastiCache-Redis-Secure-Compliant.png" alt="" width="600" height="381" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because although&nbsp;in-transit encryption is part of the solution, it is missing the most important thing which is the Redis AUTH option.</p> <p>Option 2 is incorrect because&nbsp;the Redis At-Rest Encryption feature only secures&nbsp;the data inside&nbsp;the in-memory data store. You have to use Redis AUTH option instead.</p> <p>Option 4 is incorrect because the Redis AUTH option is disabled by default.</p> <p>&nbsp;</p> <p><strong>References</strong>:</p> <p><a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html</a></p> <p><a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Elasticache cheat sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/</a></span></p> <p>&nbsp;</p> <p><strong>Redis Append-Only Files vs Redis Replication:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-redis-append-only-files-vs-redis-replication/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-redis-append-only-files-vs-redis-replication/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567380, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'An online stock trading system is hosted in AWS and uses an Auto Scaling group of EC2 instances, an RDS database, and an Amazon ElastiCache for Redis. You need to improve the data security of your in-memory data store by requiring the user to enter a password before they are granted permission to execute Redis commands.\xa0 \xa0Which of the following should you do to meet the above requirement?', 'id': 10079116, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'relatedLectureIds': '', 'answers': ['Ensure that automated backups are enabled for the RDS', 'Use MyISAM as the storage engine for MySQL.', 'Use InnoDB as the storage engine for MySQL.', 'Partition your large tables so that file sizes does not exceed the 16 TB limit.'], 'feedbacks': ['', '', '', ''], 'question': 'Your boss has asked you to launch a new MySQL RDS which ensures that you are available to recover from a database crash. <br><br>Which of the below is not a recommended practice for RDS?', 'explanation': '<p>Using MyISAM as the storage engine for MySQL is not recommended hence, this option is incorrect. The recommended storage engine for MySQL is InnoDB and not&nbsp;MyISAM.</p> <p>Options 1, 3, and 4 are best practices in the AWS MySQL RDS documentation. Again, InnoDB is the recommended storage engine for MySQL. However, in case you require intense, full-text search capability, use MyISAM storage engine instead.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html#CHAP_BestPractices.MySQLStorage" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html#CHAP_BestPractices.MySQLStorage</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567388, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'Your boss has asked you to launch a new MySQL RDS which ensures that you are available to recover from a database crash. Which of the below is not a recommended practice for RDS?', 'id': 10079122, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use host conditions to define rules that forward requests to different target groups based on the host name in the host header. This enables you to support multiple domains using a single load balancer.</p>', '<p>Replace your ALB with a Classic Load Balancer then use path conditions to define rules that forward requests to different target groups based on the URL in the request.</p>', '<p>Use path conditions to define rules that forward requests to different target groups based on the URL in the request.</p>', '<p>Replace your ALB with a Network Load Balancer then use host conditions to define rules that forward requests to different target groups based on the URL in the request.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A fast food company is using AWS to host their online ordering system which uses an Auto Scaling group of EC2 instances deployed across multiple Availability Zones with an Application Load Balancer in front. To better handle the incoming traffic from various digital devices, you are planning to implement a new routing system where requests which have a URL of &lt;server&gt;/api/android are forwarded to one specific target group named "Android-Target-Group". Conversely, requests which have a URL of &lt;server&gt;/api/ios are forwarded to another separate target group named "iOS-Target-Group".\xa0 \xa0</p><p>How can you implement this change in AWS?</p>', 'explanation': '<p>You can use path conditions to define rules that forward requests to different target groups based on the URL in the request (also known as&nbsp;<em>path-based routing</em>).&nbsp;This type of routing is the most appropriate solution for this scenario hence, Option 3 is correct.</p> <p>Each path condition has one path pattern. If the URL in a request matches the path pattern in a listener rule exactly, the request is routed using that rule.&nbsp;</p> <p>A path pattern is case-sensitive, can be up to 128 characters in length, and can contain any of the following characters. You can include up to three wildcard characters.</p> <div> <ul> <li>A&ndash;Z, a&ndash;z, 0&ndash;9</li> <li>_ - . $ / ~ " \' @ : +</li> <li>&amp; (using &amp;amp;)</li> <li>* (matches 0 or more characters)</li> <li>? (matches exactly 1 character)</li> </ul> </div> <div> <p>Example path patterns</p> <ul> <li><code>/img/*</code></li> <li><code>/js/*</code></li> </ul> </div> <p>&nbsp;</p> <p>Option 1 is incorrect because host-based routing defines rules that forward requests to different target groups based on the host name in the host header instead of the URL, which is what is needed in this scenario.</p> <p>Option 2 is incorrect because a Classic Load Balancer does not support path-based routing. You must use an Application Load Balancer.</p> <p>Option 4 is incorrect because a&nbsp;Network Load Balancer is used for applications that need extreme network performance and static IP. It also does not support path-based routing which is what is needed in this scenario. Furthermore, the statement mentions host-based routing yet, the description is about path-based routing.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html#application-load-balancer-benefits">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html#application-load-balancer-benefits</a></p> <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions"> https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</a></span></p> <p>&nbsp;</p> <p><strong>Application Load Balancer vs Network Load Balancer vs Classic Load Balancer:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567390, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'A fast food company is using AWS to host their online ordering system which uses an Auto Scaling group of EC2 instances deployed across multiple Availability Zones with an Application Load Balancer in front. To better handle the incoming traffic from various digital devices, you are planning to implement a new routing system where requests which have a URL of &lt;server&gt;/api/android are forwarded to one specific target group named "Android-Target-Group". Conversely, requests which have a URL of &lt;server&gt;/api/ios are forwarded to another separate target group named "iOS-Target-Group".\xa0 \xa0How can you implement this change in AWS?', 'id': 10079124, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'relatedLectureIds': '', 'answers': ['Put the access keys in Amazon Glacier instead.', 'Put the access keys in an Amazon S3 bucket instead.', 'Remove the stored access keys in the AMI. Create a new IAM role with permissions to access the DynamoDB table and assign it to the EC2 instances.', 'Do nothing. The architecture is already secure because the access keys are already in the Amazon Machine Image.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solution Architect for a startup in Silicon Valley. Their application architecture is currently set up to store both the access key ID and the secret access key in a plain text file on a custom Amazon Machine Image (AMI). The EC2 instances, which are created by using this AMI, are using the stored access keys to connect to a DynamoDB table.\xa0 \xa0What should you do to make the current architecture more secure?</p>', 'explanation': '<p>You should use an IAM role to manage&nbsp;<em>temporary</em>&nbsp;credentials for applications that run on an EC2 instance. When you use an IAM role, you don\'t have to distribute long-term credentials (such as a user name and password or access keys) to an EC2 instance.</p> <p>Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests.</p> <p>Hence, the best option here is to remove the stored access keys first in the AMI. Then, create a new IAM role with permissions to access the DynamoDB table and assign it to the EC2 instances.</p> <p>Options 1 and 2 are incorrect because S3 and Glacier are mainly used as a storage option. It is better to use an IAM role instead of storing access keys in these storage services.</p> <p>Option 4 is incorrect because you can make the architecture more secure by using IAM.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567392, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You are working as a Solution Architect for a startup in Silicon Valley. Their application architecture is currently set up to store both the access key ID and the secret access key in a plain text file on a custom Amazon Machine Image (AMI). The EC2 instances, which are created by using this AMI, are using the stored access keys to connect to a DynamoDB table.\xa0 \xa0What should you do to make the current architecture more secure?', 'id': 10079126, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Storage Optimized Instances', 'Memory Optimized Instances', 'Compute Optimized Instances', 'General Purpose Instances'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are setting up the required compute resources in your VPC for your application which have workloads that require high, sequential read and write access to very large data sets on local storage. Which of the following instance type is the most suitable one to use in this scenario? </p>', 'explanation': '<p>Option 1 is the correct answer. Storage optimized instances are designed for workloads that require high, sequential read and write access to very large data sets on local storage. They are optimized to deliver tens of thousands of low-latency, random I/O operations per second (IOPS) to applications.</p> <p>Option 2 is incorrect because Memory optimized instances are designed to deliver fast performance for workloads that process large data sets in memory, which is quite different from handling high read and write capacity on local storage.</p> <p>Option 3 is incorrect because Compute optimized instances are ideal for compute-bound applications that benefit from high-performance processors, such as batch processing workloads and media transcoding.</p> <p>Option 4 is incorrect because General purpose instances are the most basic type of instances. They provide a balance of compute, memory, and networking resources, and can be used for a variety of workloads. Since you are requiring higher read and write capacity, storage optimized instances should be selected instead.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-optimized-instances.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567486, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are setting up the required compute resources in your VPC for your application which have workloads that require high, sequential read and write access to very large data sets on local storage. Which of the following instance type is the most suitable one to use in this scenario?', 'id': 10079214, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Lambda', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>The failed Lambda functions have been running for over 15 minutes and reached the maximum execution time.</p>', '<p>The concurrent execution limit has been reached.</p>', '<p>The Lambda function contains a recursive code and has been running for over 15 minutes.</p>', '<p>The failed Lambda Invocations contain a <code>ServiceException</code> error which means that the AWS Lambda service encountered an internal error.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>An application is using a Lambda function to process complex financial data which runs for about 10 to 15 minutes. You noticed that there are a few terminated invocations throughout the day, which caused data discrepancy in the application.\xa0 \xa0</p><p>Which of the following is the most likely cause of this issue?</p>', 'explanation': '<p>A Lambda function consists of code and any associated dependencies. In addition, a Lambda function also has configuration information associated with it. Initially, you specify the configuration information when you create a Lambda function. Lambda provides an API for you to update some of the configuration data.</p> <p>You pay for the AWS resources that are used to run your Lambda function. To prevent your Lambda function from running indefinitely, you specify a <strong>timeout</strong>. When the specified timeout is reached, AWS Lambda terminates execution of your Lambda function. It is recommended that you set this value based on your expected execution time. The default timeout is 3 seconds and the maximum execution duration per request in AWS Lambda is 900 seconds, which is equivalent to 15 minutes. Hence, Option 1 is the correct answer.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2019-01-16_00-06-49-7fc593e456d2ce9edb7d49cf69d68e7e.png" alt="" width="650" height="403" /></p> <p>&nbsp;</p> <p>Take note that you can invoke a Lambda function synchronously either by calling the&nbsp;<code class="code">Invoke</code>&nbsp;operation or by using an AWS SDK in your preferred runtime. If you anticipate a long-running Lambda function, your client may time out before function execution completes. To avoid this, update the client timeout or your SDK configuration.</p> <p>Option 2 is incorrect because, by default, the AWS Lambda limits the total concurrent executions across all functions within a given region to 1000. By setting a concurrency limit on a function, Lambda guarantees that allocation will be applied specifically to that function, regardless of the amount of traffic processing the remaining functions. If that limit is exceeded, the function will be throttled but not terminated, which is in contrast with what is happening in the scenario.</p> <p>Option 3 is incorrect because&nbsp;having a&nbsp;recursive code in your Lambda function does not directly result to an abrupt termination of the function execution. This is a scenario wherein the function automatically calls itself until some arbitrary criteria is met. This could lead to an unintended volume of function invocations and escalated costs, but not an abrupt termination because Lambda will&nbsp;throttle all invocations to the function.</p> <p>Option 4 is incorrect because although this is a valid root cause, it is unlikely to have several&nbsp;<strong>ServiceException</strong> errors throughout the day unless there is an outage or disruption in AWS. Since the scenario says that the Lambda function runs for about 10 to 15 minutes, the maximum execution duration is the most likely cause of the issue and not the AWS Lambda service encountering an internal error.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/lambda/latest/dg/limits.html">https://docs.aws.amazon.com/lambda/latest/dg/limits.html</a></p> <p><a href="https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html">https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Lambda Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567386, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'An application is using a Lambda function to process complex financial data which runs for about 10 to 15 minutes. You noticed that there are a few terminated invocations throughout the day, which caused data discrepancy in the application.\xa0 \xa0Which of the following is the most likely cause of this issue?', 'id': 10079120, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['The Cross-Region Replication feature is only available for Amazon S3 - RRS.', 'This is a premium feature which is only for AWS Enterprise accounts.', 'In order to use the Cross-Region Replication feature in S3, you need to first enable versioning on the bucket.', 'The Cross-Region Replication feature is only available for Amazon S3 - Infrequent Access.'], 'feedbacks': ['', '', '', ''], 'question': 'You are trying to enable Cross-Region Replication to your S3 bucket but this option is disabled.<br><br>Which of the following options is a valid reason for this?', 'explanation': '<p>To enable the cross-region replication feature in S3, the following items should be met:</p> <ol> <li>The source and destination buckets must have <span style="text-decoration: underline;">versioning</span> enabled.</li> <li>The source and destination buckets must be in different AWS Regions.</li> <li>Amazon S3 must have permissions to replicate objects from that source bucket to the destination bucket on your behalf.</li> </ol> <p>&nbsp;</p> <ul> <li>Options 1 and 4 are wrong as this feature is available to all types of S3 classes.</li> <li>Option 2 is incorrect as this CRR feature is available to all&nbsp;Support Plans.</li> </ul> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567394, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You are trying to enable Cross-Region Replication to your S3 bucket but this option is disabled.Which of the following options is a valid reason for this?', 'id': 10079128, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'relatedLectureIds': '', 'answers': ['Open up the port that ELB uses in a security group and then give the user access to that security group via a policy.', '<p>Create a new IAM Role which will be assumed by the IAM user. Attach a policy allowing access to modify the ELB and once it is done, remove the IAM role from the user.</p>', 'Create a new IAM user that has access to modify the ELB. Delete that user when the work is completed.', 'Provide the user temporary access to the root account for 8 hours only. Afterwards, change the password once the activity is completed.'], 'feedbacks': ['', '', '', ''], 'question': 'In a tech company that you are working for, there is a requirement to allow one IAM user to modify the configuration of one of your Elastic Load Balancers (ELB). This access is required only once. <br><br>Which of the following would be the best way to allow this access?', 'explanation': '<p>In this scenario, the best option is to use IAM Role to provide access. You can create a new IAM Role then associate it to the IAM user. Attach a policy allowing access to modify the ELB and once it is done, remove the IAM role to the user.</p> <p>An IAM&nbsp;<em>role</em>&nbsp;is similar to a user in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have standard long-term credentials (password or access keys) associated with it. Instead, if a user assumes a role, temporary security credentials are created dynamically and provided to the user.</p> <p>You can use roles to delegate access to users, applications, or services that don\'t normally have access to your AWS resources. For example, you might want to grant users in your AWS account access to resources they don\'t usually have, or grant users in one AWS account access to resources in another account. Or you might want to allow a mobile app to use AWS resources, but not want to embed AWS keys within the app (where they can be difficult to rotate and where users can potentially extract them). Sometimes you want to give AWS access to users who already have identities defined outside of AWS, such as in your corporate directory. Or, you might want to grant access to your account to third parties so that they can perform an audit on your resources.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567396, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'In a tech company that you are working for, there is a requirement to allow one IAM user to modify the configuration of one of your Elastic Load Balancers (ELB). This access is required only once. Which of the following would be the best way to allow this access?', 'id': 10079130, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Networking', 'prompt': {'relatedLectureIds': '', 'answers': ['The two EC2 Instances have different versions of the SUSE Linux AMI.', 'You have not configured the Security Group to allow the required traffic between the two subnets. ', 'The EC2 instances do not have Public IPs attached to them.', 'The EC2 Instances do not have Elastic IPs.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You have 2 SUSE Linux Enterprise Server instances located in different subnets in the same VPC. These EC2 instances should be able to communicate with each other, but you always get a timeout when you try to ping from one instance to another. In addition, the route tables seem to be valid and have the entry for the Target ‘local’ for your VPC CIDR.\xa0 \xa0</p><p>Which of the following could be a valid reason for this issue?</p>', 'explanation': '<p>To ensure that ping commands can go through between EC2 instances, security groups need to be configured. The ping utility uses the ICMP protocol, so this needs to be set in the Inbound Rules of your security group to ensure that the ping packets can be routed to the EC2 instances.&nbsp;</p><br /><p>Resources:</p><p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html"  target="_blank" >http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html</a></p><p>&nbsp;</p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567398, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You have 2 SUSE Linux Enterprise Server instances located in different subnets in the same VPC. These EC2 instances should be able to communicate with each other, but you always get a timeout when you try to ping from one instance to another. In addition, the route tables seem to be valid and have the entry for the Target ‘local’ for your VPC CIDR.\xa0 \xa0Which of the following could be a valid reason for this issue?', 'id': 10079132, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EIP', 'prompt': {'relatedLectureIds': '', 'answers': ['There is no cost if the instance is running and it has only one associated EIP. ', 'There is no cost if the instance is terminated and it has only one associated EIP. ', 'There is no cost if the instance is stopped and it has only one associated EIP. ', 'There is no cost if the instance is running and it has at least two associated EIP. '], 'feedbacks': ['', '', '', ''], 'question': '<p>You are instructed by your manager to create a publicly accessible EC2 instance by using an Elastic IP (EIP) address and also to give him a report on how much it will cost to use that EIP. <br><br>Which of the following statements is correct regarding the pricing of EIP?</p>', 'explanation': '<p>An Elastic IP address doesn&rsquo;t incur charges as long as the following conditions are true:</p><ul><li>-The Elastic IP address is associated with an Amazon EC2 instance.</li><li>-The instance associated with the Elastic IP address is running.</li><li>-The instance has only one Elastic IP address attached to it.</li></ul><br /><p>If you&rsquo;ve stopped or terminated an EC2 instance with an associated Elastic IP address and you don&rsquo;t need that Elastic IP address anymore, consider disassociating or releasing the Elastic IP address .</p><p>&nbsp;</p><p>References:</p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/elastic-ip-charges/">https://aws.amazon.com/premiumsupport/knowledge-center/elastic-ip-charges/</a></p><p>&nbsp;</p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567400, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You are instructed by your manager to create a publicly accessible EC2 instance by using an Elastic IP (EIP) address and also to give him a report on how much it will cost to use that EIP. Which of the following statements is correct regarding the pricing of EIP?', 'id': 10079134, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Launch a new RDS database instance with the Backtrack feature enabled.</p>', '<p>Configure your RDS database to enable encryption.</p>', '<p>Set up an RDS database and enable the IAM DB Authentication.</p>', '<p>Launch the mysql client using the <code>--ssl-ca</code> parameter when connecting to the database.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A web application, which is hosted in your on-premises data center and uses a MySQL database, must be migrated to AWS Cloud. You need to ensure that the network traffic to and from your RDS database instance is encrypted using SSL. For improved security, you have to use the profile credentials specific to your EC2 instance to access your database, instead of a password.\xa0 \xa0</p><p>Which of the following should you do to meet the above requirement?</p>', 'explanation': '<p>You can authenticate to your DB&nbsp;instance&nbsp;using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don\'t need to use a password when you connect to a DB instance. Instead, you use an authentication token.</p> <p>An&nbsp;<em>authentication token</em>&nbsp;is a unique string of characters that&nbsp;Amazon RDS&nbsp;generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don\'t need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication.</p> <p>IAM database authentication provides the following benefits:</p> <div> <ol type="disc"> <li> <p>Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL).</p> </li> <li> <p>You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB&nbsp;instance.</p> </li> <li> <p>For applications running on Amazon EC2, you can use profile credentials specific to your EC2 instance to access your database instead of a password, for greater security</p> </li> </ol> <p>&nbsp;</p> <p>Hence, Option 3 is the correct answer based on the above reference.</p> <p>Option 1 is incorrect because the&nbsp;Backtrack feature simply "rewinds" the DB cluster to the time you specify. Backtracking is not a replacement for backing up your DB cluster so that you can restore it to a point in time. However, you can easily undo mistakes using the backtrack feature if you mistakenly perform a destructive action, such as a <code>DELETE</code> without a <code>WHERE</code> clause.</p> <p>Option 2 is incorrect because the encryption feature in RDS is mainly for securing your Amazon RDS DB&nbsp;instances&nbsp;and snapshots at rest. The data that is encrypted at rest includes the underlying storage for DB&nbsp;instances, its automated backups, Read Replicas, and snapshots.</p> <p>Option 4 is incorrect because even though using the <code>--ssl-ca</code> parameter can provide SSL connection to your database, you still need to use IAM database connection to use the profile credentials specific to your EC2 instance to access your database instead of a password.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS cheat sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p> </div>'}, 'correct_response': ['c'], 'original_assessment_id': 2567402, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'A web application, which is hosted in your on-premises data center and uses a MySQL database, must be migrated to AWS Cloud. You need to ensure that the network traffic to and from your RDS database instance is encrypted using SSL. For improved security, you have to use the profile credentials specific to your EC2 instance to access your database, instead of a password.\xa0 \xa0Which of the following should you do to meet the above requirement?', 'id': 10079136, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['User data', 'EC2Config service', 'IAM roles', 'AWS Config'], 'feedbacks': ['', '', '', ''], 'question': 'A startup company wants to launch a fleet of EC2 instances on AWS. Your manager wants to ensure that the Java programming language is installed automatically when the instance is launched. In which of the below configurations can you achieve this requirement?', 'explanation': '<p>When you launch an instance in Amazon EC2, you have the option of passing <strong>user data</strong> to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can write and run scripts that install new packages, software, or tools in your instance when it is launched.</p> <p>You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the command line tools), or as base64-encoded text (for API calls).</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567404, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'A startup company wants to launch a fleet of EC2 instances on AWS. Your manager wants to ensure that the Java programming language is installed automatically when the instance is launched. In which of the below configurations can you achieve this requirement?', 'id': 10079138, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Networking', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Provision Elastic Network Interfaces between the subnets.</p>', '<p>Create all the subnets on another VPC and enable VPC peering.</p>', '<p>Create a virtual overlay network running on the OS level of the instance.</p>', '<p>All of the above.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Solutions Architect for a major accounting firm, and they have a legacy general ledger accounting application that needs to be moved to AWS. However, the legacy application has a dependency on multicast networking. On this scenario, which of the following options should you consider to ensure the legacy application works in AWS?</p>', 'explanation': '<p>Multicast is a network capability that allows one-to-many distribution of data. With multicasting, one or more sources can transmit network packets to subscribers that typically reside within a multicast group. However, take note that Amazon VPC does not support multicast or broadcast networking.</p> <p>You can use an overlay multicast in order to migrate the legacy application. An overlay multicast is a method of building IP level multicast across a network fabric supporting unicast IP routing, such as Amazon Virtual Private Cloud (Amazon VPC).</p> <p>Option 1 is incorrect because just providing ENIs between the subnets would not resolve the dependency on multicast.</p> <p>Option 2 is incorrect because VPC peering and multicast are not the same.</p> <p>Option 3 is correct because overlay multicast is a method of building IP level multicast across a network fabric supporting unicast IP routing, such as Amazon Virtual Private Cloud (Amazon VPC).</p> <p>Option 4 is incorrect because the only option that will work in this scenario is creating a virtual overlay network.<br />\u2028</p> <p><br /><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/articles/overlay-multicast-in-amazon-virtual-private-cloud">https://aws.amazon.com/articles/overlay-multicast-in-amazon-virtual-private-cloud</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567406, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You are working as a Solutions Architect for a major accounting firm, and they have a legacy general ledger accounting application that needs to be moved to AWS. However, the legacy application has a dependency on multicast networking. On this scenario, which of the following options should you consider to ensure the legacy application works in AWS?', 'id': 10079140, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'AWS Organizations', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Set up a common IAM policy that can be applied across all AWS accounts.</p>', '<p>Connect all departments by setting up a cross-account access to each of the AWS accounts of the company. Create and attach IAM policies to your resources based on their respective departments to control access.</p>', '<p>Provide access to externally authenticated users via Identity Federation. Set up an IAM role to specify permissions for users from each department whose identity is federated from your organization or a third-party identity provider.</p>', '<p>Use AWS Organizations and Service Control Policies to control services on each account.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A multinational manufacturing company has multiple accounts in AWS to separate their various departments such as finance, human resources, engineering and many others. There is a requirement to ensure that certain access to services and actions are properly controlled to comply with the security policy of the company. </p><p>As the Solutions Architect, which is the most suitable way to set up the multi-account AWS environment of the company?</p>', 'explanation': '<p>Option 4 is the correct answer. Refer to the diagram below:</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-10-26_01-25-11-8da501431a6200367e0672f1387defa8.png" /></p> <p>&nbsp;</p> <p>AWS Organizations offers policy-based management for multiple AWS accounts. With Organizations, you can create groups of accounts, automate account creation, apply and manage policies for those groups. Organizations enables you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes. It allows you to create Service Control Policies (SCPs) that centrally control AWS service use across multiple AWS accounts.</p> <p>Option 1 is incorrect because it is not possible to create a common IAM policy for multiple AWS accounts.</p> <p>Option 2 is incorrect because although you can set up cross-account access to each department, this entails a lot of configuration compared with using AWS Organizations and Service Control Policies (SCPs). Cross-account access would be a more suitable choice if you only have two accounts to manage, but not for multiple accounts.</p> <p>Option 3 is incorrect as this option is focused on the Identity Federation authentication set up for your AWS accounts but not the IAM policy management for multiple AWS accounts. A combination of AWS Organizations and Service Control Policies (SCPs) is a better choice compared to this option.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://aws.amazon.com/organizations/">https://aws.amazon.com/organizations/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-organizations/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-organizations/</span></a></p> <p>&nbsp;</p> <p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-service-control-policies-scp-vs-iam-policies/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-service-control-policies-scp-vs-iam-policies/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567410, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'A multinational manufacturing company has multiple accounts in AWS to separate their various departments such as finance, human resources, engineering and many others. There is a requirement to ensure that certain access to services and actions are properly controlled to comply with the security policy of the company. As the Solutions Architect, which is the most suitable way to set up the multi-account AWS environment of the company?', 'id': 10079144, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFront', 'prompt': {'relatedLectureIds': '', 'answers': ['An object is only cached by Cloudfront once a successful request has been made hence, the objects were not requested before, which is why the request is still directed to the origin server.', '<p>The file sizes of the cached objects are too large for CloudFront to handle.</p>', '<p>The Cache-Control max-age directive is set to zero.</p>', 'You did not add an SSL certificate.'], 'feedbacks': ['', '', '', ''], 'question': "You are working for a global news network where you have set up a CloudFront distribution for your web application. However, you noticed that your application's origin server is being hit for each request instead of the AWS Edge locations, which serve the cached objects. The issue occurs even for the commonly requested objects. <br><br>What could be a possible cause of this issue?", 'explanation': '<p>In this scenario, the main culprit is that the Cache-Control max-age directive is set to a low value, which is why the request is always directed to your origin server. Hence, option 3 is correct.&nbsp;</p> <p>Option 1 is incorrect because the issue also occurs even for the commonly requested objects. This means that these objects were successfully requested before but due to a low&nbsp;Cache-Control max-age directive value, it causes this issue in Cloudfront.</p> <p>Options 2 and 4 are incorrect because they are not related to the issue in caching.</p> <p>You can control how long your objects stay in a CloudFront cache before CloudFront forwards another request to your origin. Reducing the duration allows you to serve dynamic content. Increasing the duration means your users get better performance because your objects are more likely to be served directly from the edge cache. A longer duration also reduces the load on your origin.</p> <p>Typically, CloudFront serves an object from an edge location until the cache duration that you specified passes &mdash; that is, until the object expires. After it expires, the next time the edge location gets a user request for the object, CloudFront forwards the request to the origin server to verify that the cache contains the latest version of the object.</p> <p>The&nbsp;<code>Cache-Control</code>&nbsp;and&nbsp;<code>Expires</code>&nbsp;headers control how long objects stay in the cache.&nbsp;The&nbsp;<code>Cache-Control max-age</code>&nbsp;directive lets you specify how long (in seconds) you want an object to remain in the cache before CloudFront gets the object again from the origin server. The minimum expiration time CloudFront supports is 0 seconds for web distributions and 3600 seconds for RTMP distributions.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567412, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': "You are working for a global news network where you have set up a CloudFront distribution for your web application. However, you noticed that your application's origin server is being hit for each request instead of the AWS Edge locations, which serve the cached objects. The issue occurs even for the commonly requested objects. What could be a possible cause of this issue?", 'id': 10079146, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Billing and Cost Management', 'prompt': {'relatedLectureIds': '', 'answers': ['A one-time charge of $10.', '$10 per month for each dataset.', '$10 per month for all datasets.', 'No charge.'], 'feedbacks': ['', '', '', ''], 'question': '<p>AWS hosts a variety of public datasets such as satellite imagery, geospatial, or genomic data that you want to use for your web application hosted in Amazon EC2.\xa0 \xa0</p><p>If you use these datasets, how much will it cost you?</p>', 'explanation': '<div><p>AWS hosts a variety of public datasets that anyone can access for <strong>free</strong>.</p><p>Previously, large datasets such as satellite imagery or genomic data have required hours or days to locate, download, customize, and analyze. When data is made publicly available on AWS, anyone can analyze any volume of data without needing to download or store it themselves.&nbsp;</p></div><p>&nbsp;</p><p>References:</p><p><a href="https://aws.amazon.com/public-datasets/" target="_blank" rel="noopener">https://aws.amazon.com/public-datasets/</a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567414, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'AWS hosts a variety of public datasets such as satellite imagery, geospatial, or genomic data that you want to use for your web application hosted in Amazon EC2.\xa0 \xa0If you use these datasets, how much will it cost you?', 'id': 10079148, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Lambda', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use AWS Lambda and Amazon API Gateway.</p>', '<p>Set up a micro-service architecture with ECS, ECR, and Fargate.</p>', '<p>Host the APIs in a static S3 web hosting bucket behind a CloudFront web distribution.</p>', '<p>Use Spot Amazon EC2 instances behind an Application Load Balancer.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A game development company operates several virtual reality (VR) and augmented reality (AR) games which use various RESTful web APIs hosted on their on-premises data center. Due to the unprecedented growth of their company, they decided to migrate their system to AWS Cloud to scale out their resources as well to minimize costs.\xa0 </p><p>Which of the following should you recommend as the most cost-effective and scalable solution to meet the above requirement?</p>', 'explanation': '<p>With AWS Lambda, you pay only for what you use. You are charged based on the number of requests for your functions and the duration, the time it takes for your code to execute.</p> <p>Lambda counts a&nbsp;request&nbsp;each time it starts executing in response to an event notification or invoke call, including test invokes from the console. You are charged for the total number of requests across all your functions. Duration&nbsp;is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms. The price depends on the amount of memory you allocate to your function. The Lambda free tier includes 1M free requests per month&nbsp;and over 400,000 GB-seconds of compute time per month.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/serverless/Serverless%20Migration/Serverlesswebapp.45052e1feb8f1748d96a678311d73434599095b1.png" alt="" width="706" height="392" /></p> <p>&nbsp;</p> <p>The best possible answer here is to use Lambda and API Gateway because this solution is both scalable and cost-effective. You will only be charged when you use your Lambda function, unlike having an EC2 instance which always runs even though you don&rsquo;t use it.</p> <p>Option 2 is incorrect because ECS is mainly used to host Docker applications and in addition, using ECS, ECR, and Fargate alone is not scalable and not recommended for this type of scenarios.</p> <p>Option 3 is not a suitable option as there is no compute capability for S3 and you can only use it as a static website. Although this solution is scalable since it is using CloudFront, the use of S3 to host the web APIs or the dynamic website&nbsp;is still incorrect.&nbsp;</p> <p>Option 4 is incorrect because EC2 alone, without Auto Scaling, is not scalable. Even though you use Spot EC2 instance, it is still more expensive compared to Lambda because you will be charged only when your function is being used.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html</a></p> <p><a href="https://aws.amazon.com/lambda/pricing/">https://aws.amazon.com/lambda/pricing/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Lambda Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-lambda/</span></a></p> <p>&nbsp;</p> <p><strong>EC2 Container Service (ECS) vs Lambda:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-ec2-container-service-ecs-vs-lambda/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-ec2-container-service-ecs-vs-lambda/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567416, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'A game development company operates several virtual reality (VR) and augmented reality (AR) games which use various RESTful web APIs hosted on their on-premises data center. Due to the unprecedented growth of their company, they decided to migrate their system to AWS Cloud to scale out their resources as well to minimize costs.\xa0 Which of the following should you recommend as the most cost-effective and scalable solution to meet the above requirement?', 'id': 10079150, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Egress-Only Internet Gateway\xa0 </p>', '<p>VPC Endpoint\xa0 </p>', 'NAT Gateway', '<p>NAT Instance\xa0 </p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Cloud Engineer in a leading technology consulting firm which is using a fleet of Windows-based EC2 instances with IPv4 addresses launched in a private subnet. Several software installed in the EC2 instances are required to be updated via the Internet.\xa0 \xa0</p><p>Which of the following services can provide you with a highly available solution to safely allow the instances to fetch the software patches from the Internet but prevent outside network from initiating a connection?\xa0 </p>', 'explanation': '<p>AWS offers two kinds of NAT devices &mdash; a NAT gateway or a NAT instance. It is recommended to use NAT gateways, as they provide better availability and bandwidth over NAT instances. The NAT Gateway service is also a managed service that does not require your administration efforts. A NAT instance is launched from a NAT AMI.</p> <p>Just like a NAT instance, you can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances.</p> <p>Here is a diagram showing the differences between NAT gateway and NAT instance:</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://tutorialsdojo.com/wp-content/uploads/2018/12/Natcomparison.jpg" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because an Egress-only Internet gateway is primarily used for VPCs that use IPv6 to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances, just like what NAT Instance and NAT Gateway do. The scenario explicitly says that the EC2 instances are using IPv4 addresses which is why Egress-only Internet gateway is invalid, even though it can provide the required high availability.</p> <p>Option 2 is incorrect because a VPC endpoint simply enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an Internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.</p> <p>Option 4 is incorrect because although a NAT instance can also enable instances in a private subnet to connect to the Internet or other AWS services and prevent the Internet from initiating a connection with those instances, it is not as highly available compared to a NAT Gateway.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html</a></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567484, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are working as a Cloud Engineer in a leading technology consulting firm which is using a fleet of Windows-based EC2 instances with IPv4 addresses launched in a private subnet. Several software installed in the EC2 instances are required to be updated via the Internet.\xa0 \xa0Which of the following services can provide you with a highly available solution to safely allow the instances to fetch the software patches from the Internet but prevent outside network from initiating a connection?', 'id': 10079212, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Elastic Beanstalk', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>ECS</p>', '<p>OpsWorks</p>', '<p>AWS CodeDeploy</p>', 'AWS Elastic Beanstalk'], 'feedbacks': ['', '', '', ''], 'question': '<p>Your company has developed a financial analytics web application hosted in a Docker container using MEAN (MongoDB, Express.js, AngularJS, and Node.js) stack. You want to easily port that web application to AWS Cloud which can automatically handle all the tasks such as balancing load, auto-scaling, monitoring, and placing your containers across your cluster.\xa0 \xa0</p><p>Which of the following services can be used to fulfill this requirement?</p>', 'explanation': '<p>Elastic Beanstalk supports the deployment of web applications from Docker containers. With Docker containers, you can define your own runtime environment. You can choose your own platform, programming language, and any application dependencies (such as package managers or tools), that aren\'t supported by other platforms. Docker containers are self-contained and include all the configuration information and software your web application requires to run.</p> <p>By using Docker with Elastic Beanstalk, you have an infrastructure that automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. You can manage your web application in an environment that supports the range of services that are integrated with Elastic Beanstalk, including but not limited to VPC, RDS, and IAM. Hence, Option 4 is correct.</p> <p>Option 1 is incorrect because although ECS also provides Service Auto Scaling, Service Load Balancing and Monitoring with CloudWatch, these features are not&nbsp;<strong><em>automatically</em></strong>&nbsp;enabled by default unlike with Elastic Beanstalk. Take note that the scenario requires a service that will<em>&nbsp;</em><strong><em>automatically</em></strong><em>&nbsp;handle all the tasks such as balancing load, auto-scaling, monitoring, and placing your containers across your cluster.&nbsp;</em>You will have to manually configure these things if you wish to use ECS. With Elastic Beanstalk, you can manage your web application in an environment that supports the range of services easier.</p> <p>Options 2 and 3 are incorrect because&nbsp;OpsWorks and CodeDeploy are primarily used for application deployment and configuration only, without&nbsp;providing&nbsp;load balancing, auto-scaling, monitoring or ECS cluster management.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-beanstalk/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-beanstalk/</span></a></p> <p>&nbsp;</p> <p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567420, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'Your company has developed a financial analytics web application hosted in a Docker container using MEAN (MongoDB, Express.js, AngularJS, and Node.js) stack. You want to easily port that web application to AWS Cloud which can automatically handle all the tasks such as balancing load, auto-scaling, monitoring, and placing your containers across your cluster.\xa0 \xa0Which of the following services can be used to fulfill this requirement?', 'id': 10079152, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['Enable Enhanced Networking to your EC2 Instances.', 'Use Amazon S3 Multipart Upload API.', 'Leverage on Amazon CloudFront and use HTTP POST method to reduce latency.', '<p>Use Amazon Elastic Block Store Provisioned IOPS and an Amazon EBS-optimized instance.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a computer animation film studio that has a web application running on an Amazon EC2 instance. It uploads 5 GB video objects to an Amazon S3 bucket. Video uploads are taking longer than expected, which impacts the performance of your application. <br><br>Which method will help improve the performance of your application?</p>', 'explanation': '<p>The main issue is the slow upload time of the video objects to Amazon S3. To address this issue, you can use Multipart upload in S3 to improve the throughput. It allows you to upload parts of your object in parallel thus, decreasing the time it takes to upload big objects. Each part is a contiguous portion of the object\'s data.</p> <p>You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.</p> <p>Using multipart upload provides the following advantages:</p> <ol> <li>Improved throughput - You can upload parts in parallel to improve throughput.</li> <li>Quick recovery from any network issues - Smaller part size minimizes the impact of restarting a failed upload due to a network error.</li> <li>Pause and resume object uploads - You can upload object parts over time. Once you initiate a multipart upload, there is no expiry; you must explicitly complete or abort the multipart upload.</li> <li>Begin an upload before you know the final object size - You can upload an object as you are creating it.</li> </ol> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p> <p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567422, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You are working for a computer animation film studio that has a web application running on an Amazon EC2 instance. It uploads 5 GB video objects to an Amazon S3 bucket. Video uploads are taking longer than expected, which impacts the performance of your application. Which method will help improve the performance of your application?', 'id': 10079154, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'relatedLectureIds': '', 'answers': ['SAML-based Identity Federation', 'Cross-Account Access', 'AWS Identity and Access Management roles', 'Web Identity Federation'], 'feedbacks': ['', '', '', ''], 'question': 'A mobile application stores pictures in Amazon Simple Storage Service (S3) and allows application sign-in using an OpenID Connect-compatible identity provider. <br><br>Which AWS Security Token Service approach to temporary access should you use for this scenario?', 'explanation': '<p>With web identity federation, you don\'t need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known identity provider (IdP) &mdash;such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP, receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Using an IdP helps you keep your AWS account secure because you don\'t have to embed and distribute long-term security credentials with your application.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567424, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'A mobile application stores pictures in Amazon Simple Storage Service (S3) and allows application sign-in using an OpenID Connect-compatible identity provider. Which AWS Security Token Service approach to temporary access should you use for this scenario?', 'id': 10079156, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ELB', 'prompt': {'relatedLectureIds': '', 'answers': ['Sticky Sessions', 'Availability Zone', 'Placement Group', 'Security Group'], 'feedbacks': ['', '', '', ''], 'question': "<p>As the Solutions Architect, you have built a photo-sharing site for an entertainment company. The site was hosted using 3 EC2 instances in a single availability zone with a Classic Load Balancer in front to evenly distribute the incoming load.\xa0 \xa0</p><p>What should you do to enable your Classic Load Balancer to bind a user's session to a specific instance?\xa0 </p>", 'explanation': '<div> <p>By default, a Classic Load Balancer routes each request independently to the registered instance with the smallest load. However, you can use the&nbsp;<em>sticky session</em>&nbsp;feature (also known as&nbsp;<em>session affinity</em>), which enables the load balancer to bind a user\'s session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance.</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-10-28_00-42-12-120ae3a1821b473c8a0c3de2d13b7227.png" /></p> <p>The key to managing sticky sessions is to determine how long your load balancer should consistently route the user\'s request to the same instance. If your application has its own session cookie, then you can configure Elastic Load Balancing so that the session cookie follows the duration specified. If your application does not have its own session cookie, then you can configure Elastic Load Balancing to create a session cookie by specifying your own stickiness duration.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> </div> <p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-load-balancing-elb/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567426, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': "As the Solutions Architect, you have built a photo-sharing site for an entertainment company. The site was hosted using 3 EC2 instances in a single availability zone with a Classic Load Balancer in front to evenly distribute the incoming load.\xa0 \xa0What should you do to enable your Classic Load Balancer to bind a user's session to a specific instance?", 'id': 10079158, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EFS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>EBS Volumes</p>', '<p>Elastic File System</p>', '<p>Amazon S3</p>', '<p>ElastiCache</p>'], 'feedbacks': ['', '', '', ''], 'question': "<p>A multinational company has been building its new generation big data and analytics platform in AWS in which they need a scalable storage service. The data need to be stored redundantly across multiple AZ's and allows concurrent connections from multiple EC2 instances hosted on multiple Availability Zones. </p><p>Which of the following AWS storage service is the best one to use in this scenario?</p>", 'explanation': '<p>In this question, you should take note of this phrase: "allows concurrent connections from multiple EC2 instances". There are various AWS storage options that you can choose but whenever these criteria show up, always consider using EFS instead of using EBS Volumes which is mainly used as a "block" storage and can only have one connection to one EC2 instance at a time.</p> <p>Amazon EFS is a fully-managed service that makes it easy to set up and scale file storage in the Amazon Cloud. With a few clicks in the AWS Management Console, you can create file systems that are accessible to Amazon EC2 instances via a file system interface (using standard operating system file I/O APIs) and supports full file system access semantics (such as strong consistency and file locking).</p> <p>Amazon EFS file systems can automatically scale from gigabytes to petabytes of data without needing to provision storage. Tens, hundreds, or even thousands of Amazon EC2 instances can access an Amazon EFS file system at the same time, and Amazon EFS provides consistent performance to each Amazon EC2 instance. Amazon EFS is designed to be highly durable and highly available.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/efs/latest/ug/performance.html">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p> <p><a href="https://aws.amazon.com/efs/faq/">https://aws.amazon.com/efs/faq/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EFS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-efs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-efs/</span></a></p> <p>&nbsp;</p> <p><strong>Here\'s a short video tutorial on Amazon EFS:</strong></p> <p><iframe src="https://www.youtube.com/embed/AvgAozsfCrY" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567428, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': "A multinational company has been building its new generation big data and analytics platform in AWS in which they need a scalable storage service. The data need to be stored redundantly across multiple AZ's and allows concurrent connections from multiple EC2 instances hosted on multiple Availability Zones. Which of the following AWS storage service is the best one to use in this scenario?", 'id': 10079160, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['You should adjust the security group to allow traffic from port 22', 'You should adjust the security group to allow traffic from port 3389', 'You should restart the EC2 instance since there might be some issue with the instance', 'You should create a new instance since there might be some issue with the instance'], 'feedbacks': ['', '', '', ''], 'question': 'You are a Solutions Architect of a tech company. You are having an issue whenever you try to connect to your newly created EC2 instance using a Remote Desktop connection from your computer. Upon checking, you have verified that the instance has a public IP and the Internet gateway and route tables are in place. <br><br>What else should you do for you to resolve this issue?', 'explanation': '<p>Since you are using a Remote Desktop connection to access your EC2 instance, you have to ensure that the&nbsp;Remote Desktop Protocol is allowed in the security group. By default, the server listens on TCP port 3389 and UDP port 3389.</p> <p>Option 1 is incorrect as the port 22 is used for SSH connections and not for RDP.</p> <p>Options 3 and 4 are incorrect as the EC2 instance is newly created and hence, unlikely to cause the issue. You have to check the security group first if it allows the Remote Desktop Protocol (3389) before investigating if there is indeed an issue on the specific instance.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/troubleshooting-windows-instances.html#rdp-issues">https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/troubleshooting-windows-instances.html#rdp-issues</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567430, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You are a Solutions Architect of a tech company. You are having an issue whenever you try to connect to your newly created EC2 instance using a Remote Desktop connection from your computer. Upon checking, you have verified that the instance has a public IP and the Internet gateway and route tables are in place. What else should you do for you to resolve this issue?', 'id': 10079162, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'relatedLectureIds': '', 'answers': ['It makes the database fault-tolerant to an Availability Zone failure.', 'Significantly increases the database performance.', 'Creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ) in a different region.', 'Increased database availability in the case of system upgrades like OS patching or DB Instance scaling.', 'Provides SQL optimization.'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You were hired as an IT Consultant in a startup cryptocurrency company that wants to go global with their international money transfer app. Your project is to make sure that the database of the app is highly available on multiple regions.\xa0 \xa0</p><p>What are the benefits of adding Multi-AZ deployments in Amazon RDS? (Choose 2)\xa0 </p>', 'explanation': '<p>The correct answers are options 1 &amp; 4:</p> <ul> <li>Increased database availability in the case of system upgrades like OS patching or DB Instance scaling.</li> <li>It makes the database fault-tolerant to an Availability Zone failure</li> </ul> <p>&nbsp;</p> <p>Option 3 is almost correct. RDS synchronously replicates the data to a standby instance in a different Availability Zone (AZ) that is in the same region and not in a different one.</p> <p>Options 2 and 5 are incorrect as it does not affect the performance nor provide SQL optimization.</p> <p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.</p> <p>In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/rds/details/multi-az/" target="_blank" rel="noopener">https://aws.amazon.com/rds/details/multi-az/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>'}, 'correct_response': ['a', 'd'], 'original_assessment_id': 2567408, '_class': 'assessment', 'updated': '2019-06-02T00:09:38Z', 'created': '2019-06-02T00:09:38Z', 'question_plain': 'You were hired as an IT Consultant in a startup cryptocurrency company that wants to go global with their international money transfer app. Your project is to make sure that the database of the app is highly available on multiple regions.\xa0 \xa0What are the benefits of adding Multi-AZ deployments in Amazon RDS? (Choose 2)', 'id': 10079142, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'Scaling', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Vertical scaling means running the same software on a fully serverless architecture using Lambda. Horizontal scaling means adding more servers to the existing pool and it doesn’t run into limitations of individual servers.</p>', '<p>Horizontal scaling means running the same software on bigger machines which is limited by the capacity of individual servers. Vertical scaling is adding more servers to the existing pool and doesn’t run into limitations of individual servers.</p>', '<p>Vertical scaling means running the same software on bigger machines which is limited by the capacity of the individual server. Horizontal scaling is adding more servers to the existing pool and doesn’t run into limitations of individual servers.</p>', '<p>Horizontal scaling means running the same software on smaller containers such as Docker and Kubernetes using ECS or EKS. Vertical scaling is adding more servers to the existing pool and doesn’t run into limitations of individual servers.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are building a microservices architecture in which a software is composed of small independent services that communicate over well-defined APIs. In building large-scale systems, fine-grained decoupling of microservices is a recommended practice to implement. The decoupled services should scale horizontally from each other to improve scalability. </p><p>What is the difference between Horizontal scaling and Vertical scaling?</p>', 'explanation': '<p>Vertical scaling means running the same software on bigger machines which is limited by the capacity of the individual server. Horizontal scaling is adding more servers to the existing pool and doesn&rsquo;t run into limitations of individual servers.</p> <p><br /><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-12-10_10-50-47-8b7b5c45cd789db9c3d60d111ad22276.png" width="700" height="394" /></p> <p>&nbsp;</p> <p>Fine-grained decoupling of microservices is a best practice for building large-scale systems. It&rsquo;s a prerequisite for performance optimization since it allows choosing the appropriate and optimal technologies for a specific service. Each service can be implemented with the appropriate programming languages and frameworks, leverage the optimal data persistence solution, and be fine-tuned with the best performing service configurations.</p> <p>Properly decoupled services can be scaled horizontally and independently from each other. Vertical scaling, which is running the same software on bigger machines, is limited by the capacity of individual servers and can incur downtime during the scaling process. Horizontal scaling, which is adding more servers to the existing pool, is highly dynamic and doesn&rsquo;t run into limitations of individual servers. The scaling process can be completely automated.</p> <p>Furthermore, the resiliency of the application can be improved because failing components can be easily and automatically replaced. Hence, Option 3 is the correct answer.</p> <p>Option 1 is incorrect because Vertical scaling is not about running the same software on a fully serverless architecture. AWS Lambda is not required for scaling.</p> <p>Option 2 is incorrect because the definitions for the two concepts were switched.&nbsp;Vertical scaling means running the same software on bigger machines which is limited by the capacity of the individual server. Horizontal scaling is adding more servers to the existing pool and doesn&rsquo;t run into limitations of individual servers.</p> <p>Option 4 is incorrect because Horizontal scaling is not related to using ECS or EKS containers on a smaller instance.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/microservices-on-aws.pdf#page=8">https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/microservices-on-aws.pdf#page=8</a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567434, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are building a microservices architecture in which a software is composed of small independent services that communicate over well-defined APIs. In building large-scale systems, fine-grained decoupling of microservices is a recommended practice to implement. The decoupled services should scale horizontally from each other to improve scalability. What is the difference between Horizontal scaling and Vertical scaling?', 'id': 10079166, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['Add this new entry to the route table: 0.0.0.0/27 -&gt; Your Internet Gateway', 'Modify the above route table: 10.0.0.0/27 -&gt; Your Internet Gateway', ' Add the following entry to the route table: 10.0.0.0/27 -&gt; Your Internet Gateway', 'Add a new entry to the route table - 0.0.0.0/27 -&gt; Internet Gateway', 'Add this new entry to the route table: 0.0.0.0/0 -&gt; Your Internet Gateway'], 'feedbacks': ['', '', '', '', ''], 'question': 'You have created a VPC with a single subnet then you launched an On-Demand EC2 instance in that subnet. You have attached Internet gateway (IGW) to the VPC and verified that the EC2 instance has a public IP. The main route table of the VPC is as shown below:<br><br><img src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-01-29_09-51-47-e9ccf269ea2ff9fafe4a307c1257507d.png"><br><br>However, the instance still cannot be reached from the Internet when you tried to connect to it from your computer. Which of the following should be made to the route table to fix this issue?', 'explanation': '<p>Apparently, the route table does not have an entry for the Internet Gateway. This is why you cannot connect to the EC2 instance. To fix this, you have to add a route with a destination of&nbsp;<code>0.0.0.0/0</code>&nbsp;for IPv4 traffic or&nbsp;<code class="code">::/0</code>&nbsp;for IPv6 traffic, and then a target of the Internet gateway ID (<code class="code">igw-xxxxxxxx</code>).</p> <p>This should be the correct route table configuration after adding the new entry.</p> <p><img src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-01-29_10-12-42-b725ca3ed0b358d7a00e8b0fd1c1bc51.png" width="693" height="342" /></p> <p>&nbsp;</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.htm</a><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html" target="_blank" rel="noopener">l</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['e'], 'original_assessment_id': 2567436, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You have created a VPC with a single subnet then you launched an On-Demand EC2 instance in that subnet. You have attached Internet gateway (IGW) to the VPC and verified that the EC2 instance has a public IP. The main route table of the VPC is as shown below:However, the instance still cannot be reached from the Internet when you tried to connect to it from your computer. Which of the following should be made to the route table to fix this issue?', 'id': 10079168, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Billing and Cost Management', 'prompt': {'relatedLectureIds': '', 'answers': ['$0.00', '$0.06', '$0.08', '$0.07'], 'feedbacks': ['', '', '', ''], 'question': '<p>The company that you are working for has instructed you to create a cost-effective cloud solution for their online movie ticketing service. Your team has designed a solution of using a fleet of Spot EC2 instances to host the new ticketing web application. You requested a spot instance at a maximum price of $0.06/hr which has been fulfilled immediately. After 45 minutes, the spot price increased to $0.08/hr and then your instance was terminated by AWS.\xa0 \xa0</p><p>What was the total EC2 compute cost of running your spot instances?</p>', 'explanation': '<p>In this scenario, you don\'t need to pay at all hence, option 1 is correct.</p><p>If your Spot instance is terminated or stopped by Amazon EC2 in the first instance hour, <span style="text-decoration: underline;">you will not be charged for that usage</span>. However, if you terminate the instance yourself, you will be charged to the nearest second.</p><p>If the Spot instance is terminated or stopped by Amazon EC2 in any subsequent hour, you will be charged for your usage to the nearest second. If you are running on Windows and you terminate the instance yourself, you will be charged for an entire hour.</p><p>&nbsp;</p><p>References:&nbsp;</p><p><a href="https://aws.amazon.com/ec2/faqs/">https://aws.amazon.com/ec2/faqs/</a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567438, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'The company that you are working for has instructed you to create a cost-effective cloud solution for their online movie ticketing service. Your team has designed a solution of using a fleet of Spot EC2 instances to host the new ticketing web application. You requested a spot instance at a maximum price of $0.06/hr which has been fulfilled immediately. After 45 minutes, the spot price increased to $0.08/hr and then your instance was terminated by AWS.\xa0 \xa0What was the total EC2 compute cost of running your spot instances?', 'id': 10079170, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFront', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use versioned objects</p>', '<p>Invalidate the files in your CloudFront web distribution</p>', '<p>Add a separate cache behavior path for the content and configure a custom object caching with a Minimum TTL of 0</p>', "<p>Add Cache-Control no-cache, no-store, or private directives to the objects that you don't want CloudFront to cache.</p>"], 'feedbacks': ['', '', '', ''], 'question': '<p>A global medical research company has a molecular imaging system which provides each client with frequently updated images of what is happening inside the human body at the molecular and cellular level. The system is hosted in AWS and the images are hosted in an S3 bucket behind a CloudFront web distribution. There was a new batch of updated images that were uploaded in S3, however, the users were reporting that they were still seeing the old content. You need to control which image will be returned by the system even when the user has another version cached either locally or behind a corporate caching proxy.\xa0 </p><p>Which of the following is the most suitable solution to solve this issue?</p>', 'explanation': '<p>To control the versions of files that are served from your distribution, you can either invalidate files or give them versioned file names. If you want to update your files frequently, AWS recommends that you primarily use file versioning for the following reasons:</p> <div class="itemizedlist"> <ul> <li>-Versioning enables you to control which file a request returns even when the user has a version cached either locally or behind a corporate caching proxy. If you invalidate the file, the user might continue to see the old version until it expires from those caches.</li> <li>-CloudFront access logs include the names of your files, so versioning makes it easier to analyze the results of file changes.</li> <li>-Versioning provides a way to serve different versions of files to different users.</li> <li>-Versioning simplifies rolling forward and back between file revisions.</li> <li>-Versioning is less expensive. You still have to pay for CloudFront to transfer new versions of your files to edge locations, but you don\'t have to pay for invalidating files.</li> </ul> <p>&nbsp;</p> <p>Option 2 is incorrect because even though using invalidation will solve this issue, this solution is more expensive as compared to Option 1.</p> <p>Option 3&nbsp;is incorrect because configuring a separate cache behavior path having a custom object caching with a Minimum TTL of 0 alone is not enough to solve the problem.&nbsp;A cache behavior is primarily used to configure a variety of CloudFront functionality for a given URL path pattern for files on your website. Although this solution may work,&nbsp;it is still better to use&nbsp;versioned objects where you can control which image will be returned by the system even when the user has another version cached either locally or behind a corporate caching proxy.</p> <p>Option 4 is incorrect because although it is right to configure your origin to add the <em><strong>Cache-Control</strong></em> or&nbsp;<em><strong>Expires</strong></em> header field, you should do this to your objects and not on the entire S3 bucket.</p> <p>&nbsp;&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/UpdatingExistingObjects.html">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/UpdatingExistingObjects.html</a></p> <p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/prevent-cloudfront-from-caching-files/">https://aws.amazon.com/premiumsupport/knowledge-center/prevent-cloudfront-from-caching-files/</a></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html#PayingForInvalidation">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html#PayingForInvalidation</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudfront/</span></a></p> </div>'}, 'correct_response': ['a'], 'original_assessment_id': 2567440, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A global medical research company has a molecular imaging system which provides each client with frequently updated images of what is happening inside the human body at the molecular and cellular level. The system is hosted in AWS and the images are hosted in an S3 bucket behind a CloudFront web distribution. There was a new batch of updated images that were uploaded in S3, however, the users were reporting that they were still seeing the old content. You need to control which image will be returned by the system even when the user has another version cached either locally or behind a corporate caching proxy.\xa0 Which of the following is the most suitable solution to solve this issue?', 'id': 10079172, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Glacier', 'prompt': {'relatedLectureIds': '', 'answers': ['Amazon Storage Gateway', 'Amazon Glacier', 'Amazon EBS', 'Amazon S3'], 'feedbacks': ['', '', '', ''], 'question': 'Your company would like to store their old yet confidential corporate files that are infrequently accessed. What cost-efficient solution in AWS should you recommend?', 'explanation': '<p>Amazon Glacier is a secure, durable, and extremely low-cost cloud storage service for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provides comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements. Amazon Glacier provides query-in-place functionality, allowing you to run powerful analytics directly on your archive data at rest.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/glacier/faqs/">https://aws.amazon.com/glacier/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Glacier Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-glacier/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-glacier/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567442, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'Your company would like to store their old yet confidential corporate files that are infrequently accessed. What cost-efficient solution in AWS should you recommend?', 'id': 10079174, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Kinesis', 'prompt': {'relatedLectureIds': '', 'answers': ['Amazon Kinesis', 'AWS Data Pipeline', 'Amazon AppStream', 'Amazon Simple Queue Service'], 'feedbacks': ['', '', '', ''], 'question': 'You are planning to launch an application that tracks the GPS coordinates of delivery trucks in your country. The coordinates are transmitted from each delivery truck every five seconds. You need to design an architecture that will enable real-time processing of these coordinates from multiple consumers. The aggregated data will be analyzed in a separate reporting application.<br><br>Which AWS service should you use for this scenario?', 'explanation': '<p>Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. It offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.</p> <p>With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and responds instantly instead of having to wait until all your data are collected before the processing can begin.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/kinesis/">https://aws.amazon.com/kinesis/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this&nbsp;Amazon Kinesis Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567444, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are planning to launch an application that tracks the GPS coordinates of delivery trucks in your country. The coordinates are transmitted from each delivery truck every five seconds. You need to design an architecture that will enable real-time processing of these coordinates from multiple consumers. The aggregated data will be analyzed in a separate reporting application.Which AWS service should you use for this scenario?', 'id': 10079176, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudWatch', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>CloudWatch Logs agent </p>', '<p>CloudTrail\xa0 </p>', '<p>VPC Flow Logs\xa0 </p>', '<p>CloudTrail Logs agent\xa0 </p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as an AWS Engineer in a major telecommunications company in which you are tasked to make a network monitoring system. You launched an EC2 instance to host the monitoring system and used CloudWatch to monitor, store, and access the log files of your instance.\xa0 \xa0</p><p>Which of the following provides an automated way to send log data to CloudWatch Logs from your Amazon EC2 instance?\xa0 </p>', 'explanation': '<p>CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances hence, Option 1 is the correct answer.</p> <p>The CloudWatch Logs agent is comprised of the following components:</p> <ul> <li>-A plug-in to the AWS CLI that pushes log data to CloudWatch Logs.</li> <li>-A script (daemon) that initiates the process to push data to CloudWatch Logs.</li> <li>-A cron job that ensures that the daemon is always running.</li> </ul> <p>&nbsp;</p> <p>Option 2 is incorrect as CloudTrail is mainly used for tracking the API calls of your AWS resources and not for sending EC2 logs to CloudWatch.</p> <p>Option 3 is incorrect as VPC Flow logs is mainly used for tracking the traffic coming into the VPC and not for EC2 instance monitoring.</p> <p>Option 4 is incorrect because CloudTrail Logs agent does not exist.</p> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon CloudWatch Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567432, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are working as an AWS Engineer in a major telecommunications company in which you are tasked to make a network monitoring system. You launched an EC2 instance to host the monitoring system and used CloudWatch to monitor, store, and access the log files of your instance.\xa0 \xa0Which of the following provides an automated way to send log data to CloudWatch Logs from your Amazon EC2 instance?', 'id': 10079164, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CodeDeploy', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Create CloudFormation templates that have the latest configurations and code in them. </p>', '<p>Use CodeCommit to publish your code quickly in a private repository and push them to your resources for fast updates.</p>', '<p>Use deployment groups in CodeDeploy to automate code deployments in a consistent manner.</p>', '<p>Create OpsWorks recipes that will automatically launch resources containing the latest version of the code. </p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A web application is hosted on a fleet of EC2 instances inside an Auto Scaling Group with a couple of Lambda functions for ad hoc processing. Whenever you release updates to your application every week, there are inconsistencies where some resources are not updated properly. You need a way to group the resources together and deploy the new version of your code consistently among the groups with minimal downtime.\xa0 </p><p>Which among these options should you do to satisfy the given requirement with the least effort?</p>', 'explanation': '<p>CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions. It allows you to rapidly release new features, update Lambda function versions, avoid downtime during application deployment, and handle the complexity of updating your applications, without many of the risks associated with error-prone manual deployments.&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://udemy-images.s3.amazonaws.com/redactor/raw/2018-11-03_07-56-49-1bd8f8ea00d8fcd3ab56fa3e74c5a70a.png" /></p> <p>Option 1 is incorrect since it is used for provisioning and managing stacks of AWS resources based on templates you create to model your infrastructure architecture. CloudFormation is recommended if you want a tool for granular control over the provisioning and management of your own infrastructure.</p> <p>Option 2 is incorrect as you mainly use CodeCommit for managing a source-control service that hosts private Git repositories. You can store anything from code to binaries and work seamlessly with your existing Git-based tools. CodeCommit integrates with CodePipeline and CodeDeploy to streamline your development and release process.</p> <p>You could also use OpsWorks to deploy your code, however, option 4 is still incorrect because you don\'t need to launch new resources containing your new code when you can just update the ones that are already running.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html ">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html </a></p> <p><a href="https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html</a></p> <p>&nbsp;</p> <p><strong>Overview of Deployment Options on AWS whitepaper </strong></p> <p><a href="https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf">https://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-codedeploy/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-codedeploy/</span></a></p> <p>&nbsp;</p> <p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567446, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A web application is hosted on a fleet of EC2 instances inside an Auto Scaling Group with a couple of Lambda functions for ad hoc processing. Whenever you release updates to your application every week, there are inconsistencies where some resources are not updated properly. You need a way to group the resources together and deploy the new version of your code consistently among the groups with minimal downtime.\xa0 Which among these options should you do to satisfy the given requirement with the least effort?', 'id': 10079178, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Elastic Beanstalk', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Application files are stored in S3. The server log files can only be stored in the attached EBS volumes of the EC2 instances, which were launched by AWS Elastic Beanstalk.</p>', '<p>Application files are stored in S3. The server log files can be stored directly in Glacier or in CloudWatch Logs.</p>', '<p>Application files are stored in S3. The server log files can be optionally stored in CloudTrail or in CloudWatch Logs.</p>', '<p>Application files are stored in S3. The server log files can also optionally be stored in S3 or in CloudWatch Logs.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>An online shopping platform has been deployed to AWS using Elastic Beanstalk. They simply uploaded their Node.js application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Since the entire deployment process is automated, the DevOps team is not sure where to get the application log files of their shopping platform.\xa0 </p><p>In Elastic Beanstalk, where does it store the application files and server log files?</p>', 'explanation': '<p>Option 4 is correct. AWS Elastic Beanstalk stores your application files and optionally, server log files in Amazon S3. If you are using the AWS Management Console, the AWS Toolkit for Visual Studio, or AWS Toolkit for Eclipse, an Amazon S3 bucket will be created in your account and the files you upload will be automatically copied from your local client to Amazon S3. Optionally, you may configure Elastic Beanstalk to copy your server log files every hour to Amazon S3. You do this by editing the environment configuration settings.</p> <p>With CloudWatch Logs, you can monitor and archive your Elastic Beanstalk application, system, and custom log files from Amazon EC2 instances of your environments. You can also configure alarms that make it easier for you to react to specific log stream events that your metric filters extract. The CloudWatch Logs agent installed on each Amazon EC2 instance in your environment publishes metric data points to the CloudWatch service for each log group you configure. Each log group applies its own filter patterns to determine what log stream events to send to CloudWatch as data points. Log streams that belong to the same log group share the same retention, monitoring, and access control settings. You can configure Elastic Beanstalk to automatically stream logs to the CloudWatch service.</p> <p>Option 1 is incorrect because&nbsp;the server log files can also be stored in either S3 or CloudWatch Logs, and not only on the EBS volumes of the EC2 instances which are launched by AWS Elastic Beanstalk.</p> <p>Option 2 is incorrect because&nbsp;the server log files can optionally be stored in either S3 or CloudWatch Logs, but not directly to Glacier. You can create a lifecycle policy to the S3 bucket to store the server logs and archive it in Glacier, but there is no&nbsp;direct way of storing the server logs to Glacier using Elastic Beanstalk unless you do it programmatically.</p> <p>Option 3 is incorrect because&nbsp;the server log files can optionally be stored in either S3 or CloudWatch Logs, but not directly to CloudTrail as this service is primarily used for auditing API calls.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/elasticbeanstalk/faqs/">https://aws.amazon.com/elasticbeanstalk/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-beanstalk/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-elastic-beanstalk/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567448, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'An online shopping platform has been deployed to AWS using Elastic Beanstalk. They simply uploaded their Node.js application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Since the entire deployment process is automated, the DevOps team is not sure where to get the application log files of their shopping platform.\xa0 In Elastic Beanstalk, where does it store the application files and server log files?', 'id': 10079180, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Billing and Cost Management', 'prompt': {'relatedLectureIds': '', 'answers': ['Use AWS Trusted Advisor', 'Enable IAM cross-account access for all corporate IT administrators in each child account.', 'Create separate VPCs for each division within the corporate IT AWS account.', 'Use AWS Consolidated Billing by creating AWS Organizations to link the divisions’ accounts to a parent corporate account.', 'Create separate Availability Zones for each division within the corporate IT AWS account.'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working as a Solutions Architect in a global investment bank which requires corporate IT governance and cost oversight of all of their AWS resources across their divisions around the world. Their corporate divisions want to maintain administrative control of the discrete AWS resources they consume and ensure that those resources are separate from other divisions.\xa0 \xa0</p><p>Which of the following options will support the autonomy of each corporate division while enabling the corporate IT to maintain governance and cost oversight? (Choose 2)</p>', 'explanation': '<p>In this scenario, Options 2 and 4 are the correct choices. The combined use of IAM and Consolidated Billing will support the autonomy of each corporate division while enabling corporate IT to maintain governance and cost oversight.&nbsp;</p> <p>You can use an IAM role to delegate access to resources that are in different AWS accounts that you own. You share resources in one account with users in a different account. By setting up cross-account access in this way, you don\'t need to create individual IAM users in each account. In addition, users don\'t have to sign out of one account and sign into another in order to access resources that are in different AWS accounts.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/images/BillingBitsOfOrganizations.png" width="700" height="285" /></p> <p>&nbsp;</p> <p>You can use the consolidated billing feature in AWS Organizations to consolidate payment for multiple AWS accounts or multiple AISPL accounts. With consolidated billing, you can see a combined view of AWS charges incurred by all of your accounts. You can also get a cost report for each member account that is associated with your master account. Consolidated billing is offered at no additional charge. AWS and AISPL accounts can\'t be consolidated together.</p> <p>Option 1 is incorrect. Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. It only provides you alerts on areas where you do not adhere to best practices and tells you how to improve them. It does not assist in maintaining governance over your AWS accounts.</p> <p>Option 3 is incorrect because creating separate VPCs would not separate the divisions from each other since they will still be operating under the same account and therefore contribute to the same billing each month.</p> <p>Option 5 is incorrect because you do not need to create Availability Zones. They are already provided for you by AWS right from the start, and not all services support multiple AZ deployments. In addition, having separate Availability Zones in your VPC does not meet the requirement of supporting the autonomy of each corporate division.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html">http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html</a></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-billing-and-cost-management/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-billing-and-cost-management/</span></a></p>'}, 'correct_response': ['b', 'd'], 'original_assessment_id': 2567452, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are working as a Solutions Architect in a global investment bank which requires corporate IT governance and cost oversight of all of their AWS resources across their divisions around the world. Their corporate divisions want to maintain administrative control of the discrete AWS resources they consume and ensure that those resources are separate from other divisions.\xa0 \xa0Which of the following options will support the autonomy of each corporate division while enabling the corporate IT to maintain governance and cost oversight? (Choose 2)', 'id': 10079184, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'ENI', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Create a VPC with a public and private subnet. The public subnet will house the EC2 instances, while the private subnet will house the dependent license.</p>', '<p>Ensure EC2 instances that you deploy have their static IP addresses mapped to the MAC address.</p>', '<p>Provision an ENI with a fixed MAC address.</p>', '<p>Create a VPC with the MAC address tied to its private subnet.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A well-known liquor company has a legacy application which needs to be transferred to the AWS cloud. The legacy application has a dependency on the license which is based on its media access control (MAC) address. They will launch the application in an on-demand EC2 instance. The company has hired you to assist them in this transition. </p><p>In this scenario, what can you do to ensure that the MAC address of the EC2 instance will not change even if the instance is restarted or rebooted?</p>', 'explanation': '<p>An elastic network interface (ENI) is a logical networking component in a VPC that represents a virtual network card. A network interface can include the following attributes: a primary private IPv4 address from the IPv4 address range of your VPC, One or more secondary private IPv4 addresses from the IPv4 address range of your VPC; one Elastic IP address (IPv4) per private IPv4 address; one public IPv4 address; one or more IPv6 addresses; one or more security groups; MAC address and many other network interfaces.<br /><br />Option 1 is incorrect because putting license server in private subnet would not resolve the dependency on the license that is based on a MAC address.<br /><br />Option 2 is incorrect because you cannot map a static IP address to a MAC address.<br /><br />Option 3 is correct because you should use Elastic Network Interface that is associated with a fixed MAC address. This will ensure that the legacy license based software would always work and not lose the MAC address at any point in the future.<br /><br />Option 4 is incorrect because MAC addresses cannot be tied to subnets.<br />\u2028<br /><br /><strong>Reference</strong>: <br /><br /><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567454, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A well-known liquor company has a legacy application which needs to be transferred to the AWS cloud. The legacy application has a dependency on the license which is based on its media access control (MAC) address. They will launch the application in an on-demand EC2 instance. The company has hired you to assist them in this transition. In this scenario, what can you do to ensure that the MAC address of the EC2 instance will not change even if the instance is restarted or rebooted?', 'id': 10079186, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['VPC Connection', 'VPN Connection', '<p>VPC Endpoint</p>', 'VPC Peering'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as a Senior Solutions Architect for a data analytics company which has a VPC for their human resource department, and another VPC for their finance department. You need to configure your architecture to allow the finance department to access all resources that are in the human resource department and vice versa. </p><p>Which type of networking connection in AWS should you set up to satisfy the above requirement?</p>', 'explanation': '<p>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.&nbsp;</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/peering/images/peering-intro-diagram.png" /></p> <p>&nbsp;</p> <p>AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck.</p> <p>Option 1 is incorrect since a VPC connection is rather a broad term which encompasses all connections to your VPC. In this scenario, the most suitable connection type to establish is a&nbsp;VPC peering connection.</p> <p>Option 2 is incorrect because a VPN connection does not let you share the resources of each VPC with each other. It only creates a network connection between the two VPCs.</p> <p>Option 3 is incorrect because a VPC Endpoint is primarily used to allow you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink, but not to the other VPC itself.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html</a></p> <p>&nbsp;</p> <p><strong>Check out these Amazon VPC and VPC Peering Cheat Sheets:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></span></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-vpc-peering/">https://tutorialsdojo.com/aws-cheat-sheet-vpc-peering/</a></span></p> <p>&nbsp;</p> <p><strong>Here is a quick introduction to VPC Peering:</strong></p> <p><iframe src="https://www.youtube.com/embed/i1A1eH8vLtk" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567460, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are working as a Senior Solutions Architect for a data analytics company which has a VPC for their human resource department, and another VPC for their finance department. You need to configure your architecture to allow the finance department to access all resources that are in the human resource department and vice versa. Which type of networking connection in AWS should you set up to satisfy the above requirement?', 'id': 10079190, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Aurora', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Create Amazon Aurora Replicas.</p>', '<p>Deploy Aurora to two Auto-Scaling groups of EC2 instances across two Availability Zones with an elastic load balancer which handles load balancing.</p>', '<p>Enable Hash Joins to improve the database query performance.</p>', '<p>Use an Asynchronous Key Prefetch in Amazon Aurora to improve the performance of queries that join tables across indexes.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A top university has recently launched its online learning portal where the students can take e-learning courses from the comforts of their homes. The portal is on a large On-Demand EC2 instance with a single Amazon Aurora database.\xa0 \xa0</p><p>How can you improve the availability of your Aurora database to prevent any unnecessary downtime of the online portal?</p>', 'explanation': '<p>Amazon Aurora MySQL and Amazon Aurora PostgreSQL support Amazon Aurora Replicas, which share the same underlying volume as the primary instance. Updates made by the primary are visible to all Amazon Aurora Replicas. With Amazon Aurora MySQL, you can also create MySQL Read Replicas based on MySQL&rsquo;s binlog-based replication engine. In MySQL Read Replicas, data from your primary instance is replayed on your replica as transactions. For most use cases, including read scaling and high availability, we recommend using Amazon Aurora Replicas.</p> <p>Hence, the right answer here is Option 1.</p> <p>Option 2 is incorrect because Aurora is a database engine for RDS and not deployed on a typical EC2 instance.</p> <p>Option 3 is incorrect because Hash Joins are mainly used if you need to join a large amount of data by using an equijoin and not for improving availability.</p> <p>Option 4 is incorrect because the Asynchronous Key Prefetch is mainly used to improve the performance of queries that join tables across indexes.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/AuroraMySQL.BestPractices.html ">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/AuroraMySQL.BestPractices.html </a></p> <p><a href="https://aws.amazon.com/rds/aurora/faqs/">https://aws.amazon.com/rds/aurora/faqs/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Aurora Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-aurora/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-aurora/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 4505304, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A top university has recently launched its online learning portal where the students can take e-learning courses from the comforts of their homes. The portal is on a large On-Demand EC2 instance with a single Amazon Aurora database.\xa0 \xa0How can you improve the availability of your Aurora database to prevent any unnecessary downtime of the online portal?', 'id': 10079230, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Scaling', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Launch multiple EC2 instances behind an Application Load Balancer to host your application services and SNS which will act as a highly-scalable buffer that stores messages as they travel between distributed applications.</p>', '<p>Launch an Auto-Scaling group of EC2 instances to host your application services and an SQS queue. Include an Auto Scaling trigger to watch the SQS queue size which will either scale in or scale out the number of EC2 instances based on the queue.</p>', '<p>Launch multiple EC2 instances behind an Application Load Balancer to host your application services, and SWF which will act as a highly-scalable buffer that stores messages as they travel between distributed applications.</p>', '<p>Launch multiple On-Demand EC2 instances to host your application services and an SQS queue which will act as a highly-scalable buffer that stores messages as they travel between distributed applications.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A commercial bank has designed their next generation online banking platform to use a distributed system architecture. As their Software Architect, you have to ensure that their architecture is highly scalable, yet still cost-effective. Which of the following will provide the most suitable solution for this scenario?</p>', 'explanation': '<p>There are three main parts in a distributed messaging system: the components of your distributed system which can be hosted on EC2 instance; your queue (distributed on Amazon SQS servers); and the messages in the queue.</p> <p>To improve the scalability of your distributed system, you can add Auto Scaling group to your EC2 instances.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html"> https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-basic-architecture.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-auto-scaling/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567462, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A commercial bank has designed their next generation online banking platform to use a distributed system architecture. As their Software Architect, you have to ensure that their architecture is highly scalable, yet still cost-effective. Which of the following will provide the most suitable solution for this scenario?', 'id': 10079192, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Encrypt the API credentials and store in any directory of the EC2 instance.', 'Create a role in IAM. Afterwards, assign this role to a new EC2 instance.', 'Store your API credentials in Amazon Glacier.', 'Store the API credentials in the root web application directory of the EC2 instance.'], 'feedbacks': ['', '', '', ''], 'question': '<p>You deployed a web application to an EC2 instance that adds a variety of photo effects to a picture uploaded by the users. The application will put the generated photos to an S3 bucket by sending PUT requests to the S3 API.\xa0 \xa0</p><p>What is the best option for this scenario considering that you need to have API credentials to be able to send a request to the S3 API?</p>', 'explanation': '<p>The best option is to create a role in IAM. Afterwards, assign this role to a new EC2 instance.&nbsp;Applications must sign their API requests with AWS credentials. Therefore, if you are an application developer, you need a strategy for managing credentials for your applications that run on EC2 instances.</p> <p>You can securely distribute your AWS credentials to the instances, enabling the applications on those instances to use your credentials to sign requests while protecting your credentials from other users. However, it\'s challenging to securely distribute credentials to each instance, especially those that AWS creates on your behalf such as Spot Instances or instances in Auto Scaling groups. You must also be able to update the credentials on each instance when you rotate your AWS credentials.</p> <p>In this scenario, you have to use IAM roles so that your applications can securely make API requests from your instances without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.</p> <p>Options 1 and 4 are incorrect. Though you can store and use the API credentials in the EC2 instance, it will be difficult to manage just as mentioned above. You have to use IAM Roles.</p> <p>Option 3 is incorrect as Amazon Glacier is used for data archives and not for managing API credentials.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567464, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You deployed a web application to an EC2 instance that adds a variety of photo effects to a picture uploaded by the users. The application will put the generated photos to an S3 bucket by sending PUT requests to the S3 API.\xa0 \xa0What is the best option for this scenario considering that you need to have API credentials to be able to send a request to the S3 API?', 'id': 10079194, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Blue/Green Deployment', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Blue/green deployments provide a level of isolation between your blue and green application environments, which reduce the deployment risk. The blue environment represents the current application version serving production traffic while the green one is staged running a different or upgrade version of your application.</p>', '<p>It has the ability to simply roll the incoming traffic back to the currently working environment, in case of system failures, any time during the deployment process.</p>', '<p>You can use Blue/Green Deployment with CodeCommit and CodeBuild to automatically deploy the new version of your application.</p>', '<p>Impaired operation or downtime is minimized because impact is limited to the window of time between green environment issue detection and shift of traffic back to the blue environment.</p>', "<p>Blue/green deployment is more cost-effective than in-place upgrade. You don't need to launch a new environment with additional AWS resources.</p>"], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are working as an IT Consultant for a top investment firm. Your task is to ensure smooth upgrade of their accounting system in AWS to a new version without any system outages. The Technical Manager gave an advice to implement an in-place upgrade strategy while a DevOps Engineer suggested to use Blue/Green Deployment strategy instead. </p><p>Which of the following options are not the advantages of using Blue/Green Deployment over in-place upgrade strategy? (Choose 2)</p>', 'explanation': '<p>All of the options are advantages of Blue/Green deployments, except for Options 3 and 5.&nbsp;Take note that the Blue/Green deployment sets up a new <strong><em>green</em></strong> environment which uses entirely new AWS resources. In addition,&nbsp;CodeCommit and CodeBuild are not used for deployment and hence, it does not relate with Blue/Green deployments.</p> <p>Traditionally, with in-place upgrades, it was difficult to validate your new application version in a production deployment while also continuing to run your old version of the application. Blue/green deployments provide a level of isolation between your blue and green application environments. It ensures that spinning up a parallel green environment does not affect resources underpinning your blue environment. This isolation reduces your deployment risk.</p> <p>After you deploy the green environment, you have the opportunity to validate it. You might do that with test traffic before sending production traffic to the green environment, or by using a very small fraction of production traffic, to better reflect real user traffic. This is called canary analysis or canary testing. If you discover the green environment is not operating as expected, there is no impact on the blue environment. You can route traffic back to it, minimizing impaired operation or downtime, and limiting the blast radius of impact.</p> <p>This ability to simply roll traffic back to the still-operating blue environment is a key benefit of blue/green deployments. You can roll back to the blue environment at any time during the deployment process.&nbsp;Blue/green deployments also fit well with continuous integration and continuous deployment (CI/CD) workflows, in many cases limiting their complexity. Your deployment automation would have to consider fewer dependencies on an existing environment, state, or configuration.&nbsp;</p> <p>In AWS, blue/green deployments also provide cost optimization benefits. You&rsquo;re not tied to the same underlying resources. So if the performance envelope of the application changes from one version to another, you simply launch the new environment with optimized resources, whether that means fewer resources or just different compute resources. You also don&rsquo;t have to run an overprovisioned architecture for an extended period of time.</p> <p><strong>Reference: </strong></p> <p><a href="https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf#page=6">https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf#page=6</a></p>'}, 'correct_response': ['c', 'e'], 'original_assessment_id': 2567458, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are working as an IT Consultant for a top investment firm. Your task is to ensure smooth upgrade of their accounting system in AWS to a new version without any system outages. The Technical Manager gave an advice to implement an in-place upgrade strategy while a DevOps Engineer suggested to use Blue/Green Deployment strategy instead. Which of the following options are not the advantages of using Blue/Green Deployment over in-place upgrade strategy? (Choose 2)', 'id': 10079188, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['Convert the EC2 instance to On-Demand instances', 'You can opt to sell these EC2 instances on the AWS Reserved Instance Marketplace', 'Take snapshots of the EBS volumes and terminate the EC2 instances.', '<p>Convert the EC2 instances to Spot instances with a persistent Spot request type.</p>'], 'feedbacks': ['', '', '', ''], 'question': 'You have several EC2 Reserved Instances in your account that needs to be decommissioned and shut down since they are no longer required. The data is still required by the Audit team. <br><br>Which of the following steps can be taken for this scenario? (Choose 2)', 'explanation': '<p>You can create a snapshot of the instance to save its data and then sell the instance to the&nbsp;Reserved Instance Marketplace.</p> <p>The Reserved Instance Marketplace is a platform that supports the sale of third-party and AWS customers\' unused Standard Reserved Instances, which vary in terms of length and pricing options. For example, you may want to sell Reserved Instances after moving instances to a new AWS region, changing to a new instance type, ending projects before the term expiration, when your business needs change, or if you have unneeded capacity.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-general.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-general.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['b', 'c'], 'original_assessment_id': 2567466, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You have several EC2 Reserved Instances in your account that needs to be decommissioned and shut down since they are no longer required. The data is still required by the Audit team. Which of the following steps can be taken for this scenario? (Choose 2)', 'id': 10079196, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'IAM', 'prompt': {'relatedLectureIds': '', 'answers': ['SQS polling from an EC2 instance deployed with an IAM role', 'An SWF workflow', 'SQS polling from an EC2 instance using IAM user credentials', '<p>Establish a Direct Connect connection from your on-premises network and VPC</p>'], 'feedbacks': ['', '', '', ''], 'question': 'Your IT Manager asks you to create a decoupled application whose process includes dependencies on EC2 instances and servers located in your company’s on-premises data center. <br><br>Which of these options are you least likely to recommend as part of that process?', 'explanation': '<p>For decoupled applications, it is best to use SWF and SQS which are both available in all options. Note that this question asks you for the option that you would <strong>LEAST</strong> likely to recommend.</p> <p>If you notice the 3rd option, it uses IAM user credentials for the EC2 instances which is not the recommended way to do so. It should use an IAM role instead. Hence, the correct answer is option 3.</p> <p>Options 1, 2 and 4 are the recommended steps to satisfy the given requirement. You have to establish first a Direct Connect connection from your data center to your VPC to allow the on-premises servers to connect to SQS. You can either use SWF or SQS to create a decoupled application and you have to use an IAM Role, not an IAM&nbsp;user credential, on the EC2 instance to allow polling to the SQS queue.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567470, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'Your IT Manager asks you to create a decoupled application whose process includes dependencies on EC2 instances and servers located in your company’s on-premises data center. Which of these options are you least likely to recommend as part of that process?', 'id': 10079198, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Stop and restart the instances in the Placement Group and then try the launch again.</p>', '<p>Create another Placement Group and launch the new instances in the new group.</p>', '<p>Verify all running instances are of the same size and type and then try the launch again.</p>', '<p>Submit a capacity increase request to AWS as you are initially limited to only 12 instances per Placement Group.</p>'], 'feedbacks': ['', '', '', ''], 'question': "<p>You are working for a weather station in Asia with a weather monitoring system that needs to be migrated to AWS. Since the monitoring system requires a low network latency, high network throughput, you decided to launch your EC2 instances to a cluster placement group. However, when you try to add new instances to the new placement group, you receive an 'insufficient capacity error'. </p><p>How will you fix this issue?</p>", 'explanation': '<p>It is recommended that you launch the number of instances that you need in the placement group in a single launch request and that you use the same instance type for all instances in the placement group. If you try to add more instances to the placement group later, or if you try to launch more than one instance type in the placement group, you increase your chances of getting an insufficient capacity error.</p> <p>If you stop an instance in a placement group and then start it again, it still runs in the placement group. However, the start fails if there isn\'t enough capacity for the instance.</p> <p>If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Restarting the instances may migrate them to hardware that has capacity for all the requested instances.</p> <p>Option 1 is correct because you can resolve this issue just by launching again. If the instances are stopped and restarted, AWS may move the instances to a hardware that has capacity for all the requested instances.</p> <p>Option 2 is incorrect because to benefit from the enhanced networking, all the instances should be in the same Placement Group. Launching the new ones in a new Placement Group will not work in this case.</p> <p>Option 3 is incorrect because the capacity error is not related to the instance size.</p> <p>Option 4 is incorrect because there is no such limit on the number of instances in a Placement Group.</p> <p>&nbsp;</p> <p><strong>References:</strong><br /><br /><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster</a></p> <p><a href="http://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/troubleshooting-launch.html#troubleshooting-launch-capacity">http://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/troubleshooting-launch.html#troubleshooting-launch-capacity</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567472, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': "You are working for a weather station in Asia with a weather monitoring system that needs to be migrated to AWS. Since the monitoring system requires a low network latency, high network throughput, you decided to launch your EC2 instances to a cluster placement group. However, when you try to add new instances to the new placement group, you receive an 'insufficient capacity error'. How will you fix this issue?", 'id': 10079200, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Highly Available Network Design', 'prompt': {'relatedLectureIds': '', 'answers': ['Elastic Load Balancing, Amazon EC2, and Auto Scaling', 'Elastic Load Balancing, Amazon RDS with Multi-AZ, and Amazon S3', 'Amazon RDS with Multi-AZ and Auto Scaling', 'Amazon EC2, Amazon DynamoDB, and Amazon S3'], 'feedbacks': ['', '', '', ''], 'question': 'A large Philippine-based Business Process Outsourcing company is building a two-tier web application in their VPC to serve dynamic transaction-based content. The data tier is leveraging an Online Transactional Processing (OLTP) database but for the web tier, they are still deciding what service they will use. <br><br>What AWS services should you leverage to build an elastic and scalable web tier?', 'explanation': '<p>Amazon RDS is a suitable database service for online transaction processing (OLTP) applications. However, the question asks for a list of AWS services for the web tier and not the database tier. Also, when it comes to services providing scalability and elasticity for your web tier, Auto Scaling and Elastic Load Balancer should immediately come into mind. Therefore, Option 1 is the correct answer.</p> <p>To build an elastic and a highly-available web tier, you can use Amazon EC2, Auto Scaling, and Elastic Load Balancing. You can deploy your web servers on a fleet of EC2 instances to an Auto Scaling group, which will automatically monitor&nbsp;your applications and automatically adjust capacity to maintain steady, predictable performance at the lowest possible cost. Load balancing is an effective way to increase the availability of a system. Instances that fail can be replaced seamlessly behind the load balancer while other instances continue to operate. Elastic Load Balancing can be used to balance across instances in multiple availability zones of a region.</p> <p>Options 2, 3 and 4 are incorrect since they don\'t mention all of the required services&nbsp;in building a highly available and scalable web tier, such as EC2, Auto Scaling, and Elastic Load Balancer. Although&nbsp;Amazon RDS with Multi-AZ and DynamoDB are highly scalable databases, the scenario is more focused on building its web tier and not the database tier.</p> <p>&nbsp;</p> <p><strong>References:&nbsp;</strong></p> <p><a href="https://media.amazonwebservices.com/architecturecenter/AWS_ac_ra_ftha_04.pdf">https://media.amazonwebservices.com/architecturecenter/AWS_ac_ra_ftha_04.pdf</a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567474, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A large Philippine-based Business Process Outsourcing company is building a two-tier web application in their VPC to serve dynamic transaction-based content. The data tier is leveraging an Online Transactional Processing (OLTP) database but for the web tier, they are still deciding what service they will use. What AWS services should you leverage to build an elastic and scalable web tier?', 'id': 10079202, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EC2', 'prompt': {'relatedLectureIds': '', 'answers': ['When you need a higher packet per second (PPS) performance', 'When you need a low packet-per-second performance', 'When you need high latency networking', 'When you need a consistently lower inter-instance latencies', '<p>When you need a dedicated connection to your on-premises data center\xa0 </p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>You are a Solutions Architect in an intelligence agency that is currently hosting a learning and training portal in AWS. Your manager instructed you to launch a large EC2 instance with an attached EBS Volume and enable Enhanced Networking. What are the valid case scenarios in using Enhanced Networking? (Choose 2)\xa0 </p>', 'explanation': '<p>Enhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types. SR-IOV is a method of device virtualization that provides higher I/O performance and lower CPU utilization when compared to traditional virtualized network interfaces. Enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies. There is no additional charge for using enhanced networking.</p> <p>Option 2 is incorrect because you want to increase packet-per-second performance, and not lower it, when you enable enhanced networking.</p> <p>Option 3 is incorrect because higher latencies means slower network, which is the opposite of what you want to happen when you enable enhanced networking.</p> <p>Option 5 is incorrect because enabling enhanced networking does not provide a dedicated connection to your on-premises data center. Use AWS Direct Connect or enable VPN tunneling instead for this purpose.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elastic-compute-cloud-amazon-ec2/</span></a></p>'}, 'correct_response': ['a', 'd'], 'original_assessment_id': 2567450, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are a Solutions Architect in an intelligence agency that is currently hosting a learning and training portal in AWS. Your manager instructed you to launch a large EC2 instance with an attached EBS Volume and enable Enhanced Networking. What are the valid case scenarios in using Enhanced Networking? (Choose 2)', 'id': 10079182, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'VPC', 'prompt': {'relatedLectureIds': '', 'answers': ['Roughly around 5-8 minutes in order for the security rules to propagate.', 'Immediately after a reboot of the EC2 instances which belong to that security group.', 'Immediately.', 'It takes exactly one minute for the rules to apply to all availability zones within the AWS region.'], 'feedbacks': ['', '', '', ''], 'question': 'You have designed and built a new AWS architecture. After deploying your application to an On-demand EC2 instance, you found that there is an issue in your application when connecting to port 443. After troubleshooting the issue, you added port 443 to the security group of the instance.<br><br>How long will it take before the changes are applied to all of the resources in your VPC?', 'explanation': '<p>A&nbsp;<em>security group</em>&nbsp;acts as a virtual firewall for your instance to control inbound and outbound traffic. When you launch an instance in a VPC, you can assign up to five security groups to the instance. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC could be assigned to a different set of security groups. If you don\'t specify a particular group at launch time, the instance is automatically assigned to the default security group for the VPC.</p> <p>Option 3 is the correct answer. Changes made in a security group are immediately implemented. There is no need to wait for some amount of time for propagation nor reboot any instances for your changes to take effect.&nbsp;</p> <p>Options 1 and 4 are incorrect because the changes in your security group are implemented immediately and not after a minute or after a few minutes.</p> <p>Option 2 is incorrect because there is no need to reboot your EC2 instance before the security group changes are fully applied. The change takes effect immediately.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567476, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You have designed and built a new AWS architecture. After deploying your application to an On-demand EC2 instance, you found that there is an issue in your application when connecting to port 443. After troubleshooting the issue, you added port 443 to the security group of the instance.How long will it take before the changes are applied to all of the resources in your VPC?', 'id': 10079204, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'S3', 'prompt': {'relatedLectureIds': '', 'answers': ['HTTP 200 result code and MD5 checksum.', 'Amazon S3 has 99.999999999% durability hence, there is no need to confirm that data was inserted.', 'You will receive an SMS from Amazon SNS informing you that the object is successfully stored.', 'You will receive an email from Amazon SNS informing you that the object is successfully stored.'], 'feedbacks': ['', '', '', ''], 'question': '<p>A startup is building an AI-based face recognition application in AWS, where they store millions of images in an S3 bucket. As the Solutions Architect, you have to ensure that each and every image uploaded to their system is stored without any issues. </p><p>What is the correct indication that an object was successfully stored when you put objects in Amazon S3?</p>', 'explanation': '<p>If you triggered an S3 API call and got HTTP 200 result code and MD5 checksum, then it is considered as a successful upload. The S3 API will return an error code in case the upload is unsuccessful.</p> <p>Option 2 is incorrect because although S3 is durable, it is not an assurance that all objects uploaded using S3 API calls will be successful.</p> <p>Options 3 and 4 are incorrect because you don\'t receive an SMS nor an email notification by default, unless you added an event notification.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html">https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-s3/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567478, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A startup is building an AI-based face recognition application in AWS, where they store millions of images in an S3 bucket. As the Solutions Architect, you have to ensure that each and every image uploaded to their system is stored without any issues. What is the correct indication that an object was successfully stored when you put objects in Amazon S3?', 'id': 10079206, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'EBS', 'prompt': {'relatedLectureIds': '', 'answers': ['Create a new EBS volume in another Availability Zone and then specify the current EBS volume as the source.', 'Detach the EBS volume and attach it to an EC2 instance residing in another Availability Zone.', 'First, create a snapshot of the EBS volume. Afterwards, create a volume using the snapshot in the other Availability Zone.', 'First, create a new volume in the other Availability Zone. Next, perform a disk copy of the contents from the source volume to the new volume that you have created.'], 'feedbacks': ['', '', '', ''], 'question': '<p>A WordPress website hosted in an EC2 instance, which has an additional EBS volume attached, was mistakenly deployed in the <code>us-east-1a</code> Availability Zone due to a misconfiguration in your CloudFormation template. There is a requirement to quickly rectify the issue by moving and attaching the EBS volume to a new EC2 instance in the <code>us-east-1b</code> Availability Zone.\xa0 \xa0</p><p>As the Solutions Architect of the company, which of the following should you do to solve this issue?</p>', 'explanation': '<p>The first step is to create a snapshot of the EBS volume. Create a volume using this snapshot and then specify the new Availability Zone accordingly.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/snapshot_1b.png" alt="" width="750" height="579" /></p> <p>&nbsp;</p> <p>A point-in-time snapshot of an EBS volume, can be used as a baseline for new volumes or for data backup. If you make periodic snapshots of a volume, the snapshots are incremental&mdash;only the blocks on the device that have changed after your last snapshot are saved in the new snapshot. Even though snapshots are saved incrementally, the snapshot deletion process is designed so that you need to retain only the most recent snapshot in order to restore the entire volume.</p> <p>Snapshots occur asynchronously; the point-in-time snapshot is created immediately, but the status of the snapshot is&nbsp;<code class="code">pending</code>&nbsp;until the snapshot is complete (when all of the modified blocks have been transferred to Amazon S3), which can take several hours for large initial snapshots or subsequent snapshots where many blocks have changed. While it is completing, an in-progress snapshot is not affected by ongoing reads and writes to the volume.</p> <p>Option 1 is incorrect. There is no such action like this in AWS since EBS volumes do not require a source from other EBS volumes.</p> <p>Option 2 is incorrect because an EBS volume is only available in the Availability Zone it was created in and cannot be attached directly to other Availability Zones.</p> <p>Option 4 is incorrect because doing that is not the safest way to copy EBS contents. Create a snapshot instead for better reliability of the process.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html" target="_blank" rel="noopener">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html</a></p> <p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html</a></p> <p>&nbsp;&nbsp;&nbsp;</p> <p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p> <p><strong><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-ebs/</span></a></strong></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567480, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A WordPress website hosted in an EC2 instance, which has an additional EBS volume attached, was mistakenly deployed in the us-east-1a Availability Zone due to a misconfiguration in your CloudFormation template. There is a requirement to quickly rectify the issue by moving and attaching the EBS volume to a new EC2 instance in the us-east-1b Availability Zone.\xa0 \xa0As the Solutions Architect of the company, which of the following should you do to solve this issue?', 'id': 10079208, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudHSM', 'prompt': {'relatedLectureIds': '', 'answers': ['No major difference. They both do the same thing.', 'AWS CloudHSM does not support the processing, storage, and transmission of credit card data by a merchant or service provider, as it has not been validated as being compliant with Payment Card Industry (PCI) Data Security Standard (DSS); hence, you will need to use KMS.', 'You should consider using AWS CloudHSM over AWS KMS if you require your keys stored in dedicated, third-party validated hardware security modules under your exclusive control.', 'AWS CloudHSM should always be used for any payment transactions.'], 'feedbacks': ['', '', '', ''], 'question': 'There is a technical requirement by a financial firm that does online credit card processing to have a secure application environment on AWS. They are trying to decide on whether to use KMS or CloudHSM. <br><br>Which of the following statements is right when it comes to CloudHSM and KMS?', 'explanation': '<p>AWS Key Management Service (KMS) is a multi-tenant, managed service that allows you to use and manage encryption keys.&nbsp;AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. Both services offer a high level of security for your cryptographic keys. AWS CloudHSM provides a dedicated, FIPS 140-2 Level 3 HSM under your exclusive control, directly in your Amazon Virtual Private Cloud (VPC).</p><p>You should consider using AWS CloudHSM over AWS KMS if you require:</p><ul><li>Keys stored in dedicated, third-party validated hardware security modules under your exclusive control.</li><li>FIPS 140-2 compliance.</li><li>Integration with applications using PKCS#11, Java JCE, or Microsoft CNG interfaces.</li><li>High-performance in-VPC cryptographic acceleration (bulk crypto).</li></ul><p>&nbsp;</p><p>Resources:</p><p><a href="https://aws.amazon.com/cloudhsm/faqs/">https://aws.amazon.com/cloudhsm/faqs/</a></p><p><a href="https://aws.amazon.com/cloudhsm/">https://aws.amazon.com/cloudhsm/</a></p><p><a href="https://aws.amazon.com/kms/">https://aws.amazon.com/kms/</a></p><p>&nbsp;</p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567482, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'There is a technical requirement by a financial firm that does online credit card processing to have a secure application environment on AWS. They are trying to decide on whether to use KMS or CloudHSM. Which of the following statements is right when it comes to CloudHSM and KMS?', 'id': 10079210, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Do nothing as the IAM User is already capable of sending API calls to your AWS resources.\xa0 \xa0</p>', 'Enable Multi-Factor Authentication for the user.', '<p>Assign an IAM Policy to the user to allow it to send API calls.\xa0 </p>', '<p>Create a set of Access Keys for the user and attach the necessary permissions.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You recently created a brand new IAM User with a default setting using AWS CLI. This is intended to be used to send API requests to your S3, DynamoDB, Lambda, and other AWS resources of your cloud infrastructure.\xa0 \xa0</p><p>Which of the following must be done to allow the user to make API calls to your AWS resources?\xa0 </p>', 'explanation': '<p>You can choose the credentials that are right for your IAM user. When you use the AWS Management Console to create a user, you must choose to at least include a console password or access keys. By default, a brand new IAM user created using the AWS CLI or AWS API has no credentials of any kind. You must create the type of credentials for an IAM user based on the needs of your user.</p> <p>Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK). Users need their own access keys to make programmatic calls to AWS from the AWS Command Line Interface (AWS CLI), Tools for Windows PowerShell, the AWS SDKs, or direct HTTP calls using the APIs for individual AWS services.</p> <p>To fill this need, you can create, modify, view, or rotate access keys (access key IDs and secret access keys) for IAM users. When you create an access key, IAM returns the access key ID and secret access key. You should save these in a secure location and give them to the user.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/IAM/latest/UserGuide/images/iam-intro-federation.diagram.png" alt="" width="598" height="247" />&nbsp;</p> <p>&nbsp;</p> <p>Option 1 is incorrect because by default, a brand new IAM user created using the AWS CLI or AWS API has no credentials of any kind. Take note that in the scenario, you created the new IAM user using the AWS CLI and not via the AWS Management Console, where you must choose to at least include a console password or access keys when creating a new IAM user.</p> <p>Option 2 is incorrect because&nbsp;enabling Multi-Factor Authentication for the IAM user will still not provide the required Access Keys needed to send API calls to your AWS resources. You have to grant the IAM user with Access Keys to meet the requirement.</p> <p>Option 3 is incorrect because adding a new IAM policy to the new user will not grant the needed Access Keys needed to make API calls to the AWS resources.</p> <p>&nbsp;&nbsp;</p> <p><strong>References:</strong></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html</a></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html#id_users_creds">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html#id_users_creds</a>&nbsp;</p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567488, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You recently created a brand new IAM User with a default setting using AWS CLI. This is intended to be used to send API requests to your S3, DynamoDB, Lambda, and other AWS resources of your cloud infrastructure.\xa0 \xa0Which of the following must be done to allow the user to make API calls to your AWS resources?', 'id': 10079216, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'RDS', 'prompt': {'relatedLectureIds': '', 'answers': ['Consider using On-demand instances instead of Reserved EC2 instances', 'Consider not using a Multi-AZ RDS deployment for the development and test database', 'Consider using Spot instances instead of reserved EC2 instances', 'Consider removing the Elastic Load Balancer'], 'feedbacks': ['', '', '', ''], 'question': '<p>A tech company is running two production web servers hosted on Reserved EC2 instances with EBS-backed root volumes. These instances have a consistent CPU load of 90%. Traffic is being distributed to these instances by an Elastic Load Balancer. In addition, they also have Multi-AZ RDS MySQL databases for their production, test, and development environments.\xa0 </p><p>What recommendation would you make to reduce cost in this AWS environment without affecting availability and performance of mission-critical systems? Choose the best answer.</p>', 'explanation': '<p>One thing that you should notice here is that the company is using Multi-AZ databases in all of their environments, including their development and test environment. This is costly and unnecessary as these two environments are not critical. It is better to use&nbsp;Multi-AZ for production environments to reduce costs, which is why option 2 is the correct answer.&nbsp;</p> <p>Option 1 is incorrect because selecting Reserved instances is cheaper than On-demand instances for long term usage due to the discounts offered when purchasing reserved instances.</p> <p>Option 3 is incorrect because the web servers are running in a production environment. Never use Spot instances for production level web servers unless you are sure that they are not that critical in your system. This is because your spot instances can be terminated once the maximum price goes over the maximum amount that you specified.</p> <p>Option 4 is incorrect because the Elastic Load Balancer is crucial in maintaining the elasticity and reliability of your system.</p> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/rds/details/multi-az/">https://aws.amazon.com/rds/details/multi-az/</a></p> <p><a href="https://aws.amazon.com/pricing/cost-optimization/">https://aws.amazon.com/pricing/cost-optimization/</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567490, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A tech company is running two production web servers hosted on Reserved EC2 instances with EBS-backed root volumes. These instances have a consistent CPU load of 90%. Traffic is being distributed to these instances by an Elastic Load Balancer. In addition, they also have Multi-AZ RDS MySQL databases for their production, test, and development environments.\xa0 What recommendation would you make to reduce cost in this AWS environment without affecting availability and performance of mission-critical systems? Choose the best answer.', 'id': 10079218, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Networking', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>NAT Gateway</p>', '<p>NAT instances</p>', '<p>Egress-only Internet gateway</p>', '<p>Internet Gateway</p>'], 'feedbacks': ['', '', '', ''], 'question': "<p>You are working as a Solutions Architect in a well-funded financial startup. The CTO instructed you to launch a cryptocurrency mining server on a Reserved EC2 instance in us-east-1 region's private subnet which is using IPv6. Due to the financial data that the server contains, the system should be secured to avoid any unauthorized access and to meet the regulatory compliance requirements. </p><p>In this scenario, which VPC feature allows the EC2 instance to communicate to the Internet but prevents inbound traffic?</p>", 'explanation': '<p>An egress-only Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances.</p> <p>Take note that an egress-only Internet gateway is for use with IPv6 traffic only. To enable outbound-only Internet communication over IPv4, use a NAT gateway instead.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/vpc/latest/userguide/images/egress-only-igw-diagram.png" /></p> <p>&nbsp;</p> <p>Options 1 and 2 are incorrect because NAT gateways and NAT instances are only applicable for IPv4 and not IPv6. Even though these two components can enable the EC2 instance in a private subnet to communicate to the Internet and prevent inbound traffic, it is only limited with instances which are using IPv4 address and not IPv6. The most suitable VPC component to use is egress-only Internet gateway.</p> <p>Option 4 is incorrect because Internet gateways are primarily used to provide Internet access to your instances in the public subnet of your VPC, and not for private subnets. However, with an Internet gateway, traffic originating from the public Internet will also be able to reach your instances. The scenario is asking you to prevent inbound access, so this is not the correct answer.</p> <p>&nbsp;</p> <p><strong>Reference: </strong>&nbsp;</p> <p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567492, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': "You are working as a Solutions Architect in a well-funded financial startup. The CTO instructed you to launch a cryptocurrency mining server on a Reserved EC2 instance in us-east-1 region's private subnet which is using IPv6. Due to the financial data that the server contains, the system should be secured to avoid any unauthorized access and to meet the regulatory compliance requirements. In this scenario, which VPC feature allows the EC2 instance to communicate to the Internet but prevents inbound traffic?", 'id': 10079220, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'IAM', 'prompt': {'relatedLectureIds': '', 'answers': ['https://0499802888.signin.aws.amazon.com/console', '<p>https://signin.0499802888.aws.amazon.com/console</p>', 'https://signin.aws.amazon.com/console', 'https://aws.amazon.com/console'], 'feedbacks': ['', '', '', ''], 'question': 'An AWS account has an ID of 0499802888. Which of the following URLs would you provide to the IAM user to be able to access the AWS Console?', 'explanation': '<p>To use the AWS Management Console, IAM users must provide their account ID or account alias in addition to their username and password. When you, as an administrator, create an IAM user in the console, you must send the sign-in credentials to that user, including the username and the URL to the account sign-in page.</p> <p>Your unique account sign-in page URL is created automatically when you begin using IAM. You do not have to do anything to use this sign-in page.&nbsp;You can also customize the account sign-in URL for your account if you want the URL to contain your company name (or other friendly identifier) instead of your AWS account ID number.&nbsp;</p> <p>AWS sign-in page URL format:</p> <p><code>https://<span style="color: #0000ff;"><strong><em><code>My_AWS_Account_ID</code></em></strong></span>.signin.aws.amazon.com/console/<code></code></code></p> <p>Option 2 is incorrect because your account ID should come first before the word signin. Here is a technique to help you remember this format: <em>you should always come first since you own this account</em>.</p> <p>Option 3 is incorrect because this only redirects you to the sign in page where you will have to enter your account ID or alias manually. If the person you invited to the account is not aware of the account ID then he/she will not be able to log into the account.</p> <p>Option 4 is incorrect because this link redirects you to the details of what a console is.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/console.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/console.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS IAM Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-identity-and-access-management-iam/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 2567494, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'An AWS account has an ID of 0499802888. Which of the following URLs would you provide to the IAM user to be able to access the AWS Console?', 'id': 10079222, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'ElastiCache', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>ELB sticky sessions</p>', '<p>Multi-master DynamoDB</p>', '<p>Multi-AZ RDS</p>', '<p>ElastiCache in-memory caching</p>'], 'feedbacks': ['', '', '', ''], 'question': "<p>You have a fleet of running Spot EC2 instances behind an Application Load Balancer. The incoming traffic comes from various users across multiple AWS regions and you would like to have the user's session shared among your fleet of instances. You are required to set up a distributed session management layer that will provide a scalable and shared data storage for the user sessions. </p><p>Which of the following would be the best choice to meet the requirement while still providing sub-millisecond latency for your users?</p>", 'explanation': '<p>For sub-millisecond latency caching, ElastiCache is the best choice. In order to address scalability and to provide a shared data storage for sessions that can be accessed from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://d1.awsstatic.com/product-marketing/caching-session-management-diagram-v2.c6856e6de83c4222dbc4853d9ff873f5542a86d8.PNG" width="720" height="342" /></p> <p>&nbsp;</p> <p>Option 1 is incorrect because the scenario does not require you&nbsp;to route a user to the particular web server that is managing that individual user&rsquo;s session. Since the session state is shared among the instances, the use of the ELB sticky sessions feature is not recommended in this scenario.&nbsp;</p> <p>Options 2 and 3 are incorrect because although you can use DynamoDB and RDS for storing session state, these two are not the best choices in terms of cost-effectiveness and performance when compared to ElastiCache. There is a significant difference in terms of latency if you used DynamoDB and RDS when you store the session data.</p> <p><br /><strong>References:</strong><br /><a href="https://aws.amazon.com/caching/session-management/">https://aws.amazon.com/caching/session-management/</a></p> <p><a href="https://d0.awsstatic.com/whitepapers/performance-at-scale-with-amazon-elasticache.pdf">https://d0.awsstatic.com/whitepapers/performance-at-scale-with-amazon-elasticache.pdf</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Elasticache Cheat Sheet:</strong></p> <p><span style="font-weight: 400;"><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/">https://tutorialsdojo.com/aws-cheat-sheet-amazon-elasticache/</a></span></p> <p>&nbsp;</p> <p><strong>Redis (cluster mode enabled vs disabled) vs Memcached:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-redis-cluster-mode-enabled-vs-disabled-vs-memcached/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-redis-cluster-mode-enabled-vs-disabled-vs-memcached/</span></a></p> <p>&nbsp;</p>'}, 'correct_response': ['d'], 'original_assessment_id': 2567498, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': "You have a fleet of running Spot EC2 instances behind an Application Load Balancer. The incoming traffic comes from various users across multiple AWS regions and you would like to have the user's session shared among your fleet of instances. You are required to set up a distributed session management layer that will provide a scalable and shared data storage for the user sessions. Which of the following would be the best choice to meet the requirement while still providing sub-millisecond latency for your users?", 'id': 10079224, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Kinesis', 'prompt': {'relatedLectureIds': '', 'answers': ['Amazon S3 bucket has encountered a data loss.', 'Someone has manually deleted the record in Amazon S3.', 'By default, data records in Kinesis are only accessible for 24 hours from the time they are added to a stream.', 'The access of the Kinesis stream to the S3 bucket is insufficient.'], 'feedbacks': ['', '', '', ''], 'question': 'You are working for a startup that builds Internet of Things (IOT) devices and monitoring application. They are using IOT sensors to monitor all data by using Amazon Kinesis configured with default settings. You then send the data to an Amazon S3 bucket after 2 days. When you checked the data in S3, there are only data for the last day and nothing for the first day. <br><br>What is the root cause of this issue?', 'explanation': '<p>By default, records of a stream in Amazon Kinesis are accessible for up to 24 hours from the time they are added to the stream. You can raise this limit to up to 7 days by enabling extended data retention. Hence, Option 3 is correct.&nbsp;</p> <p>Option 1 is incorrect because Amazon S3 rarely experiences data loss. Amazon has an SLA for S3 that it commits to its customers. Amazon S3 Standard, S3 Standard&ndash;IA, S3 One Zone-IA, and S3 Glacier are all designed to provide 99.999999999% durability of objects over a given year. This durability level corresponds to an average annual expected loss of 0.000000001% of objects. Hence, Amazon S3 bucket data loss is highly unlikely.</p> <p>Option 2 is incorrect because if someone has deleted the data, this should have been visible in CloudTrail. Also, deleting that much data manually shouldn\'t have occurred in the first place if you have put in the appropriate security measures.</p> <p>Option 4 is incorrect because having insufficient access is highly unlikely since you are able to access the bucket and view the contents of the previous day\'s data collected by Kinesis.</p> <p>&nbsp;</p> <p><strong>Reference:&nbsp;</strong></p> <p><a href="https://aws.amazon.com/kinesis/data-streams/faqs/">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p> <p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html</a></p> <p>&nbsp;</p> <p><strong>Check out this&nbsp;Amazon Kinesis Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</span></a></p>'}, 'correct_response': ['c'], 'original_assessment_id': 2567500, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are working for a startup that builds Internet of Things (IOT) devices and monitoring application. They are using IOT sensors to monitor all data by using Amazon Kinesis configured with default settings. You then send the data to an Amazon S3 bucket after 2 days. When you checked the data in S3, there are only data for the last day and nothing for the first day. What is the root cause of this issue?', 'id': 10079226, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Route53', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Use a Network Load Balancer to distribute the load to the multiple EC2 instances across all AWS Regions.</p>', '<p>Use Route 53 to distribute the load to the multiple EC2 instances across all AWS Regions.</p>', '<p>Use an Application Load Balancer to distribute the load to the multiple EC2 instances across all AWS Regions.</p>', '<p>This is not possible in AWS. You can only set up a latency-based routing in one AWS region.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working as the Solutions Architect for a global technology consultancy firm which has an application that uses multiple EC2 instances located in various AWS regions such as US East (Ohio), US West (N. California), and EU (Ireland). Your manager instructed you to set up a latency-based routing to route incoming traffic for www.tutorialsdojo.com to all the EC2 instances across all AWS regions.\xa0 \xa0</p><p>Which of the following options can satisfy the given requirement?</p>', 'explanation': '<p>If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency.</p> <p>You can create latency records for your resources in multiple AWS Regions by using latency-based routing. In the event that Route 53 receives a DNS query for your domain or subdomain such as tutorialsdojo.com or portal.tutorialsdojo.com, it determines which AWS Regions you\'ve created latency records for, determines which region gives the user the lowest latency and then selects a latency record for that region. Route 53 responds with the value from the selected record which can be the IP address for a web server or the CNAME of your elastic load balancer. Hence, Option 2 is correct.</p> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://media.amazonwebservices.com/blog/weighted_then_geo_1.png" /></p> <p>&nbsp;</p> <p>Options 1 and 3 are incorrect because load balancers distribute traffic only within their respective regions and not to other AWS regions. It is best to use Route 53 instead to balance the incoming load to two or more AWS regions.</p> <p>Option 4 is incorrect as the requirement can be addressed with Route 53 latency-based routing.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency</a></p> <p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/TutorialAddingLBRRegion.html"> https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/TutorialAddingLBRRegion.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 2567502, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are working as the Solutions Architect for a global technology consultancy firm which has an application that uses multiple EC2 instances located in various AWS regions such as US East (Ohio), US West (N. California), and EU (Ireland). Your manager instructed you to set up a latency-based routing to route incoming traffic for www.tutorialsdojo.com to all the EC2 instances across all AWS regions.\xa0 \xa0Which of the following options can satisfy the given requirement?', 'id': 10079228, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'Route53', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>DNSSEC (Domain Name System Security Extensions)</p>', '<p>PTR (pointer record)</p>', '<p>SPF (sender policy framework)</p>', '<p>SRV (service locator)</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>You are working for a startup which develops an AI-based traffic monitoring service. You need to register a new domain called <code>www.tutorialsdojo-ai.com</code> and set up other DNS entries for the other components of your system in AWS. Which of the following is not supported by Amazon Route 53?</p>', 'explanation': '<p>Amazon Route 53&rsquo;s DNS services does not support DNSSEC at this time. However, their domain name registration service supports configuration of signed DNSSEC keys for domains when DNS service is configured at another provider. More information on configuring DNSSEC for your domain name registration can be found&nbsp;<a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html">here</a>.</p> <p>Amazon Route 53 currently supports the following DNS record types:</p> <ul> <li>-A (address record)</li> <li>-AAAA (IPv6 address record)</li> <li>-CNAME (canonical name record)</li> <li>-CAA (certification authority authorization)</li> <li>-MX (mail exchange record)</li> <li>-NAPTR (name authority pointer record)</li> <li>-NS (name server record)</li> <li>-PTR (pointer record)</li> <li>-SOA (start of authority record)</li> <li>-SPF (sender policy framework)</li> <li>-SRV (service locator)</li> <li>-TXT (text record)</li> </ul> <p>&nbsp;</p> <p><strong>Reference:</strong></p> <p><a href="https://aws.amazon.com/route53/faqs/">https://aws.amazon.com/route53/faqs/</a></p> <p><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html</a></p> <p>&nbsp;</p> <p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-route-53/</span></a></p>'}, 'correct_response': ['a'], 'original_assessment_id': 6874728, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'You are working for a startup which develops an AI-based traffic monitoring service. You need to register a new domain called www.tutorialsdojo-ai.com and set up other DNS entries for the other components of your system in AWS. Which of the following is not supported by Amazon Route 53?', 'id': 10079232, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudFormation', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>The value of the <code>AWSTemplateFormatVersion</code> is incorrect. It should be 2017-06-06.</p>', '<p>The <code>Resources</code> section is missing.</p>', '<p>An invalid section named <code>Parameters</code> is present. This will cause the CloudFormation stack to fail.</p>', '<p>The <code>Conditions</code> section is missing.</p>'], 'feedbacks': ['', '', '', ''], 'question': '<p>A new DevOps engineer has created a CloudFormation template for a web application and she raised a pull-request in GIT for you to check and review. After checking the template, you immediately told her that the template will not work. </p><p>Which of the following is the reason why this CloudFormation template will fail to deploy the stack? </p><pre class="prettyprint linenums">{ "AWSTemplateFormatVersion":"2010-09-09", \n  "Parameters":{ \n     "VPCId":{ \n       "Type":"String", \n       "Description":"tutorialsdojo" \n     }, \n     "SubnetId":{ \n       "Type":"String", \n       "Description":"subnet-b46032ec" \n     } \n   }, \n   "Outputs":{ \n     "InstanceId":{ \n       "Value":{ \n         "Ref":"TutorialsDojoInstance" \n       }, "Description":"Instance Id" \n      } \n     } \n    } </pre>', 'explanation': '<p>In CloudFormation, a template is a JSON or a YAML-formatted text file that describes your AWS infrastructure. Templates include several major sections. The Resources section is the only required section. Some sections in a template can be in any order. However, as you build your template, it might be helpful to use the logical ordering of the following list, as values in one section might refer to values from a previous section. Take note that all of the sections here are optional, except for Resources, which is the only one required.</p> <ul> <li>-Format Version</li> <li>-Description</li> <li>-Metadata</li> <li>-Parameters</li> <li>-Mappings</li> <li>-Conditions</li> <li>-Transform</li> <li>-Resources (required)</li> <li>-Outputs</li> </ul> <p>&nbsp;</p> <p><strong>Reference: </strong></p> <p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html</a></p> <p>&nbsp;</p> <p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-aws-cloudformation/</span></a></p>'}, 'correct_response': ['b'], 'original_assessment_id': 7428768, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A new DevOps engineer has created a CloudFormation template for a web application and she raised a pull-request in GIT for you to check and review. After checking the template, you immediately told her that the template will not work. Which of the following is the reason why this CloudFormation template will fail to deploy the stack? { "AWSTemplateFormatVersion":"2010-09-09", \n  "Parameters":{ \n     "VPCId":{ \n       "Type":"String", \n       "Description":"tutorialsdojo" \n     }, \n     "SubnetId":{ \n       "Type":"String", \n       "Description":"subnet-b46032ec" \n     } \n   }, \n   "Outputs":{ \n     "InstanceId":{ \n       "Value":{ \n         "Ref":"TutorialsDojoInstance" \n       }, "Description":"Instance Id" \n      } \n     } \n    }', 'id': 10079234, 'related_lectures': [], 'assessment_type': 'multiple-choice'}, {'section': 'CloudWatch', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>SWF</p>', '<p>CloudWatch</p>', '<p>Amazon Simple Queue Service</p>', '<p>Route 53</p>', '<p>Amazon Simple Notification Service</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>A construction company has an online system that tracks all of the status and progress of their projects. The system is hosted in AWS and there is a requirement to monitor the read and write IOPs metrics for their MySQL RDS instance and send real-time alerts to their DevOps team. </p><p>Which of the following services in AWS can you use to meet the requirements? (Choose 2)</p>', 'explanation': '<p>In this scenario, you can use CloudWatch to monitor your AWS resources and SNS to provide notification. Hence, the correct answers are Options 2 and 5.</p> <p>Amazon Simple Notification Service (SNS) is a flexible, fully managed pub/sub messaging and mobile notifications service for coordinating the delivery of messages to subscribing endpoints and clients.</p> <p>Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources.</p> <p>Option 1 is incorrect because SWF is mainly used for managing workflows and not for monitoring and notifications.</p> <p>Option 3 is incorrect because SQS is a messaging queue service and not suitable for this kind of scenario.</p> <p>Option 4 is incorrect because Route 53 is primarily used for routing and domain name registration and management.</p> <p>&nbsp;</p> <p><strong>References: </strong></p> <p><a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CW_Support_For_AWS.html ">http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CW_Support_For_AWS.html </a></p> <p><a href="https://aws.amazon.com/sns/">https://aws.amazon.com/sns/</a></p> <p>&nbsp;&nbsp;</p> <p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-cloudwatch/</span></a></p>'}, 'correct_response': ['b', 'e'], 'original_assessment_id': 7428772, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'A construction company has an online system that tracks all of the status and progress of their projects. The system is hosted in AWS and there is a requirement to monitor the read and write IOPs metrics for their MySQL RDS instance and send real-time alerts to their DevOps team. Which of the following services in AWS can you use to meet the requirements? (Choose 2)', 'id': 10079236, 'related_lectures': [], 'assessment_type': 'multi-select'}, {'section': 'RDS', 'prompt': {'relatedLectureIds': '', 'answers': ['<p>Force all connections to your DB instance to use SSL by setting the <code>rds.force_ssl</code> parameter to true. Once done, reboot your DB instance.</p>', '<p>Download the Amazon RDS Root CA certificate. Import the certificate to your servers and configure your application to use SSL to encrypt the connection to RDS.</p>', '<p>Specify the TDE option in an RDS option group that is associated with that DB instance to enable transparent data encryption (TDE).</p>', '<p>Enable the IAM DB authentication in RDS using the AWS Management Console.</p>', '<p>Configure the security groups of your EC2 instances and RDS to only allow traffic to and from port 443.</p>'], 'feedbacks': ['', '', '', '', ''], 'question': '<p>An application is hosted in an Auto Scaling group of EC2 instances and a Microsoft SQL Server on Amazon RDS. There is a requirement that all in-flight data between your web servers and RDS should be secured. Which of the following options is the most suitable solution that you should implement? (Choose 2)</p>', 'explanation': '<p>You can use Secure Sockets Layer (SSL) to encrypt connections between your client applications and your Amazon RDS DB instances running Microsoft SQL Server. SSL support is available in all AWS regions for all supported SQL Server editions.</p> <p>When you create a SQL Server DB instance, Amazon RDS creates an SSL certificate for it. The SSL certificate includes the DB instance endpoint as the Common Name (CN) for the SSL certificate to guard against spoofing attacks.</p> <p>There are 2 ways to use SSL to connect to your SQL Server DB instance:</p> <div class="itemizedlist"> <ul class="itemizedlist" type="disc"> <li class="listitem"> <p>-Force SSL for all connections &mdash; this happens transparently to the client, and the client doesn\'t have to do any work to use SSL.</p> </li> <li class="listitem"> <p>-Encrypt specific connections &mdash; this sets up an SSL connection from a specific client computer, and you must do work on the client to encrypt connections.</p> </li> </ul> </div> <p>&nbsp;</p> <p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/images/rds_sql_ssl_cert.png" alt="" width="680" height="244" /></p> <p>&nbsp;</p> <p>You can force all connections to your DB instance to use SSL, or you can encrypt connections from specific client computers only. To use SSL from a specific client, you must obtain certificates for the client computer, import certificates on the client computer, and then encrypt the connections from the client computer.</p> <p>If you want to force SSL, use the&nbsp;<code class="code">rds.force_ssl</code>&nbsp;parameter. By default, the&nbsp;<code class="code">rds.force_ssl</code>&nbsp;parameter is set to&nbsp;<code class="code">false</code>. Set the&nbsp;<code class="code">rds.force_ssl</code>&nbsp;parameter to&nbsp;<code class="code">true</code>&nbsp;to force connections to use SSL. The&nbsp;<code class="code">rds.force_ssl</code>&nbsp;parameter is static, so after you change the value, you must reboot your DB instance for the change to take effect.</p> <p>Hence, the correct answers for this scenario are Options 1 and 2.</p> <p>Option 3 is incorrect because the transparent data encryption (TDE) is primarily used to encrypt stored data on your DB instances running Microsoft SQL Server, and not the data that is in-transit.</p> <p>Option 4 is incorrect because the IAM database authentication is not suitable for this scenario since it is just an authentication method and not used for encryption. With&nbsp;IAM database authentication, you don\'t need to use a password when you connect to a DB instance but instead, you use an authentication token.</p> <p>Option 5 is incorrect because it is not enough to configure the security groups of your EC2 instances and RDS to only allow traffic to and from port 443. You need to either force all connections to your DB instance to use SSL, or you can encrypt connections from specific client computers, just as mentioned above.</p> <p>&nbsp;</p> <p><strong>References:</strong></p> <p class="p1"><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Concepts.General.SSL.Using.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/SQLServer.Concepts.General.SSL.Using.html</a></p> <p class="p1"><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.Options.TDE.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.Options.TDE.html</a></p> <p class="p2"><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p> <p class="p2">&nbsp;</p> <p class="p2"><strong>Check out this Amazon RDS Cheat Sheet:</strong></p> <p><a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/"><span style="font-weight: 400;">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</span></a></p> <p>&nbsp;</p> <p>&nbsp;</p>'}, 'correct_response': ['a', 'b'], 'original_assessment_id': 9887790, '_class': 'assessment', 'updated': '2019-06-02T00:09:39Z', 'created': '2019-06-02T00:09:39Z', 'question_plain': 'An application is hosted in an Auto Scaling group of EC2 instances and a Microsoft SQL Server on Amazon RDS. There is a requirement that all in-flight data between your web servers and RDS should be secured. Which of the following options is the most suitable solution that you should implement? (Choose 2)', 'id': 10079238, 'related_lectures': [], 'assessment_type': 'multi-select'}]}, 'type': 'practice-test', 'title': 'AWS Certified Solutions Architect Associate Practice Test 6'}], 'order': '0', 'chapter': 'Exercises'}]
				// console.log(json)
				for (var i = 0; i < json_data.length; i++) {
					$('.list-group-flush').append(
						`<li class="list-group-item chapter-${json_data[i]['order']}" style="background-color:#ffffff00" ><a href='#' style="color:#ec6e6e">${json_data[i]['order']}&ensp;${json_data[i]['chapter']}</a><ul></ul></li>`
					)
					for (var j = 0; j < json_data[i]['quizzes'].length; j++) {
						$(`.chapter-${json_data[i]['order']} > ul`).append(
							`<li class="d-${i}-${j}"><a href="#" style="color:#ec6e6e">${json_data[i]['quizzes'][j]['title']}</a></li>`
						)
					}
				}


			$('.list-group-flush').on('click', 'li', function (event) {
				$('.container').empty()
				$(this).addClass('active')
				event.preventDefault();
				event.stopPropagation()
				var idRe = /d-(\d{1,3})-(\d{1,3})/
				id = idRe.exec($(this).attr("class"))
				quizzes_data = json_data[Number(id[1])]['quizzes'][Number(id[2])]


				if (quizzes_data['type'] === 'simple-quiz' || quizzes_data['type'] === 'practice-test') {
					

					for (var i = 0; i < quizzes_data['quiz_data']['count']; i++) {
						$('.container').append(
							`<div class="question-list-${i} mt-4"><span><b>Question&ensp;${i}</b></span><div>${quizzes_data['quiz_data']['results'][i]['prompt']['question']}</div><div class="funkyradio"></div><div class="text-center"><button class="btn btn-primary btn-sm" id="answerBtn-${i}">Show Answer</button></div></div>`
						)
						for (var j = 0; j < quizzes_data['quiz_data']['results'][i]['prompt']['answers'].length; j++) {
							$(`.question-list-${i} .funkyradio`).append(
								`<div class="funkyradio-primary"><input type="radio" name="radio" id="radio${i}${j}"/><label for="radio${i}${j}" class="px-5 mx-1">${quizzes_data['quiz_data']['results'][i]['prompt']['answers'][j]}</label></div>`
							)
						}
					}

					$('.text-center').on('click', 'button', function (event) {
						event.preventDefault();
						var idRe2 = /answerBtn-(\d+)/
						id2 = idRe2.exec($(this).attr('id'))[1]
						$(this).html(
							`Right answer is <span style="color:red;font-size:20px" class="px-1">&ensp;${quizzes_data['quiz_data']['results'][Number(id2)]['correct_response']}</span>`
						)
						$(this).attr('class', 'btn btn-success btn-sm disabled')
try {
							for (var k = 0; k < quizzes_data['quiz_data']['results'][Number(id2)]['prompt']['feedbacks'].length; k++) {
							$(`label[for="radio${Number(id2)}${k}"]`).append(`<p class="mb-0" style="color:teal">${quizzes_data['quiz_data']['results'][Number(id2)]['prompt']['feedbacks'][k]}</p>`)
							}
}
catch(err) {
console.log("This is no feedbacks in this question")
}
							if (quizzes_data['quiz_data']['results'][Number(id2)]['prompt']['explanation'] != undefined){

							$(this).parent().parent().append(`<div class='bg-light text-info mt-3'><p>Explanation</p>${quizzes_data['quiz_data']['results'][Number(id2)]['prompt']['explanation']}</div>`)
							}

						
					 });



				}  
				
				else if (quizzes_data['type'] === 'practice') {
          for (var i = 0; i < quizzes_data['practice_data']['results'].length; i++) {
            let key = i
            $('.container').append(`<div class="questionArea-${i}"><p>${quizzes_data['practice_data']['results'][i]['body']}</p><button class="btn btn-primary btn-sm answer-${i}">Show  Answer</button></div>`)

            $(`.questionArea-${i}`).on('click', `.answer-${i}`, function(event) {
              console.log('Not bad')
              event.preventDefault();
              $(this).parent().append(`<div class="mt-5 bg-light">${quizzes_data['practice_data']['results'][key]['answer']}</div>`)
            })


            
          }
        }

				else {
				for (var i = 0; i < quizzes_data['quiz_data']['results'].length; i++) {
            $('.container').append(`<div class="codingArea-${i}"><p>${quizzes_data['quiz_data']['results'][i]['prompt']['instructions']}</p><div class="preContent bg-light"><p>提示内容</p></div><button class="btn btn-primary answer-${i}">Show  Answer</button></div>`)
            for (var j = 0; j < quizzes_data['quiz_data']['results'][i]['prompt']["initial_files"].length; j++) {
              $(".preContent").append(`<p>文件名：${quizzes_data['quiz_data']['results'][i]['prompt']["initial_files"][j]['file_name']}</p><pre>${quizzes_data['quiz_data']['results'][i]['prompt']["initial_files"][j]["content"]}</pre>`)
            }

            $(`.codingArea-${i}`).on('click', `.answer-${i}`, function(event) {
              event.preventDefault();
              $(this).parent().append(`<div class="mt-5"><pre class="bg-light">${quizzes_data['quiz_data']['results'][0]['prompt']['solution_files'][0]['content']}</pre></div>`)

            });

          }


				}


			});
		});
	</script>
	<style type="text/css">
		.sidebar-sticky {
			height: 100%
		}

		.funkyradio div {
			clear: both;
			overflow: hidden;
		}

		.funkyradio label {
			width: 100%;
			border-radius: 3px;
			border: 1px solid #D1D3D4;
			font-weight: normal;
		}

		.funkyradio input[type="radio"]:empty,
		.funkyradio input[type="checkbox"]:empty {
			display: none;
		}

		.funkyradio input[type="radio"]:empty~label,
		.funkyradio input[type="checkbox"]:empty~label {
			position: relative;
			line-height: 2.5em;
			text-indent: 3.25em;
			margin-top: 1em;
			cursor: pointer;
			-webkit-user-select: none;
			-moz-user-select: none;
			-ms-user-select: none;
			user-select: none;
		}

		.funkyradio input[type="radio"]:empty~label:before,
		.funkyradio input[type="checkbox"]:empty~label:before {
			position: absolute;
			display: block;
			top: 0;
			bottom: 0;
			left: 0;
			content: '';
			width: 2.5em;
			background: #D1D3D4;
			border-radius: 3px 0 0 3px;
		}

		.funkyradio input[type="radio"]:hover:not(:checked)~label,
		.funkyradio input[type="checkbox"]:hover:not(:checked)~label {
			color: #888;
		}

		.funkyradio input[type="radio"]:hover:not(:checked)~label:before,
		.funkyradio input[type="checkbox"]:hover:not(:checked)~label:before {
			content: '\25B6';
			text-indent: .9em;
			color: #C2C2C2;
		}

		.funkyradio input[type="radio"]:checked~label,
		.funkyradio input[type="checkbox"]:checked~label {
			color: #777;
		}

		.funkyradio input[type="radio"]:checked~label:before,
		.funkyradio input[type="checkbox"]:checked~label:before {
			content: '\25B6';
			text-indent: .9em;
			color: #333;
			background-color: #ccc;
		}

		.funkyradio input[type="radio"]:focus~label:before,
		.funkyradio input[type="checkbox"]:focus~label:before {
			box-shadow: 0 0 0 3px #999;
		}

		.funkyradio-primary input[type="radio"]:checked~label:before,
		.funkyradio-primary input[type="checkbox"]:checked~label:before {
			color: #fff;
			background-color: #337ab7;
		}

		.funkyradio-success input[type="radio"]:checked~label:before,
		.funkyradio-success input[type="checkbox"]:checked~label:before {
			color: #fff;
			background-color: #5cb85c;
		}

		.funkyradio-danger input[type="radio"]:checked~label:before,
		.funkyradio-danger input[type="checkbox"]:checked~label:before {
			color: #fff;
			background-color: #d9534f;
		}
	</style>
</body>

</html>
